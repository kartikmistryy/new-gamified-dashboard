{
  "Infrastructure": {
    "description": "",
    "functions": {
      "Build & Configuration": {
        "files": [
          {
            "path": ".circleci/TROUBLESHOOT.md",
            "description": "This document provides troubleshooting information and solutions for various issues encountered on CircleCI, including links to relevant GitHub issues.",
            "spof": true
          },
          {
            "path": ".circleci/create_circleci_config.py",
            "description": "This script defines Python classes to programmatically generate CircleCI job configurations, primarily for running tests (e.g., pytest) with specific environment variables, docker images, installation steps, and handling for parallel and flaky tests.",
            "spof": false
          },
          {
            "path": "docker/README.md",
            "description": "This README describes the various Dockerfiles for the `transformers` library, categorizing them into 'fast' CPU-only CIs and 'slow' GPU-compatible CIs, and explains their usage and purpose.",
            "spof": false
          },
          {
            "path": "docker/transformers-doc-builder",
            "description": "This directory contains the necessary configurations and resources for building the documentation of the `transformers` library. It likely leverages Docker to provide a consistent environment for the documentation build process.",
            "spof": false
          },
          {
            "path": "docker/transformers-pytorch-tpu",
            "description": "This directory is intended to contain Docker-related files for running Hugging Face Transformers with PyTorch on Google Cloud TPUs. It likely provides the necessary environment and dependencies within a container for these specific hardware and software configurations. Despite being empty, its name suggests a dedicated purpose for Dockerized TPU-accelerated PyTorch Transformers.",
            "spof": false
          },
          {
            "path": "docker/transformers-pytorch-deepspeed-nightly-gpu",
            "description": "This directory is intended to house Docker-related configurations and files for a nightly GPU build environment for the Transformers library, specifically utilizing PyTorch and DeepSpeed. It likely defines the setup for a containerized development or testing environment tailored for high-performance deep learning tasks with these frameworks.",
            "spof": false
          },
          {
            "path": "docker/transformers-gpu",
            "description": "This directory is designated for Docker-related configurations and files tailored for GPU-accelerated environments to run Hugging Face Transformers. It would contain the necessary Dockerfiles or scripts to containerize a deep learning setup optimized for GPU usage.",
            "spof": false
          },
          {
            "path": "docker/transformers-pytorch-xpu",
            "description": "This directory is designated for Docker-related configurations or build contexts specifically tailored for the Transformers library utilizing PyTorch and XPU. It likely supports containerization efforts for benchmarking or deploying Transformers models on XPU hardware within a Docker environment.",
            "spof": false
          },
          {
            "path": "docker/transformers-intel-cpu",
            "description": "This directory is designated for Docker-related files and configurations specifically tailored for benchmarking the Hugging Face Transformers library on Intel CPUs. It acts as a dedicated space for Docker assets to facilitate performance analysis within the 2023 active benchmark set.",
            "spof": false
          },
          {
            "path": "docker/transformers-pytorch-amd-gpu",
            "description": "This directory is intended to contain Docker-related files and configurations specifically for building a Docker image to run Hugging Face Transformers with PyTorch on AMD GPUs. It serves as the dedicated location for defining such a specialized compute environment.",
            "spof": false
          },
          {
            "path": "docker/transformers-all-latest-gpu",
            "description": "This directory is intended to contain Docker-related configurations and files for benchmarking the Hugging Face Transformers library. Specifically, it targets the latest versions of Transformers running on GPU environments. Its role is to define the containerized setup for performance evaluations.",
            "spof": false
          },
          {
            "path": "docker/transformers-pytorch-gpu",
            "description": "This directory is designated for Docker configurations to build a containerized environment. It is intended for running Hugging Face Transformers models with PyTorch on a GPU, likely for benchmarking purposes. Its current emptiness suggests a planned or incomplete setup.",
            "spof": false
          },
          {
            "path": "docker/transformers-pytorch-deepspeed-amd-gpu",
            "description": "This directory is designated for Docker-related configurations concerning the Hugging Face Transformers library. It specifically targets environments using PyTorch and DeepSpeed, optimized for AMD GPUs. Its role is to define the containerization setup for benchmarking or running such deep learning workloads.",
            "spof": false
          },
          {
            "path": "utils/aggregate_failure_reports.py",
            "description": "This script aggregates multiple JSON failure report files from a specified directory into a single JSON array, which is then written to an output file.",
            "spof": true
          },
          {
            "path": "utils/check_doc_toc.py",
            "description": "This script ensures that all model documentation files are correctly referenced in the `_toctree.yml` file and cleans the model documentation section by removing duplicates and sorting entries alphabetically. It can also auto-fix these inconsistencies.",
            "spof": true
          },
          {
            "path": "utils/scan_skipped_tests.py",
            "description": "This script scans model test files to identify common tests and determine which models skip or run specific tests, then summarizes this information into a JSON report. It can operate in a single-test mode or summarize all discovered common tests.",
            "spof": true
          },
          {
            "path": "utils/check_modeling_structure.py",
            "description": "This utility script checks modeling and modular files in the Transformers library to ensure they adhere to specific coding conventions, such as proper weight initialization in `_init_weights` and correct `self.post_init()` calls in `__init__` methods.",
            "spof": true
          },
          {
            "path": "utils/get_github_job_time.py",
            "description": "This script retrieves and analyzes the execution times of all jobs within a specified GitHub Actions workflow run, outputting each job's duration.",
            "spof": false
          },
          {
            "path": "utils/check_repo.py",
            "description": "This utility script performs various consistency checks on the repository, primarily focusing on model definitions, testing, documentation, and auto-mapping within the `transformers` library to ensure adherence to project standards.",
            "spof": false
          },
          {
            "path": "utils/check_self_hosted_runner.py",
            "description": "This script checks the status of specified self-hosted GitHub Actions runners using the GitHub API. It reports if any of the target runners are offline, raising an error if issues are found.",
            "spof": true
          },
          {
            "path": "utils/get_test_reports.py",
            "description": "This utility script provides a way to manually run tests of the Transformers repository, simulating how they would be executed by continuous integration. It supports features like running specific test suites, parallel execution, GPU/CPU filtering, temporary cache management, and resuming interrupted runs.",
            "spof": true
          },
          {
            "path": "utils/compare_test_runs.py",
            "description": "This script provides utilities to parse and compare test summary files from different test runs. It identifies and reports tests that have been added or removed between two sets of job test results.",
            "spof": true
          },
          {
            "path": "utils/check_doctest_list.py",
            "description": "This script checks and optionally sorts lists of file paths for doctests, ensuring that all listed files exist and the lists are in alphabetical order. It is used to maintain the integrity and order of doctest configuration files.",
            "spof": false
          },
          {
            "path": "utils/check_copies.py",
            "description": "This utility script checks for consistency between copied code sections and their original sources, verifies that model lists in READMEs are complete and synchronized, and can automatically fix detected inconsistencies within the HuggingFace Transformers library.",
            "spof": false
          },
          {
            "path": "utils/split_doctest_jobs.py",
            "description": "This script identifies and groups doctest files within the repository, structuring them for efficient parallel execution in CI/CD pipelines like GitHub Actions, especially for running doctests against documentation files.",
            "spof": true
          },
          {
            "path": "utils/format_extras_slack_message.py",
            "description": "This script formats the results of 'extras smoke tests' into a Slack message. It reads test failure reports from a JSON file and outputs the formatted message as environment variables for GitHub Actions.",
            "spof": true
          },
          {
            "path": "utils/custom_init_isort.py",
            "description": "This script is a utility for the Transformers library that sorts imports within the `_import_structure` dictionary found in custom `__init__.py` files, which standard import sorters cannot handle. It helps maintain order in delayed import mechanisms used to optimize package loading.",
            "spof": false
          },
          {
            "path": "utils/important_files.py",
            "description": "This file defines a list of transformer models that are considered important and should always be included in tests.",
            "spof": true
          },
          {
            "path": "utils/check_bad_commit.py",
            "description": "This script identifies the first 'bad' commit that introduced a test failure using `git bisect` and retrieves related GitHub commit and PR information.",
            "spof": true
          },
          {
            "path": "utils/check_dummies.py",
            "description": "This script verifies and updates dummy object files in the `transformers` library. These dummy files ensure that all objects can be imported even if their dependencies are not installed, providing helpful error messages upon access.",
            "spof": false
          },
          {
            "path": "utils/pr_slow_ci_models.py",
            "description": "This script identifies models that require 'slow CI' testing, either due to being newly added in a pull request or explicitly specified in a PR comment. It outputs a JSON list of these models.",
            "spof": true
          },
          {
            "path": "utils/extract_pr_number_from_circleci.py",
            "description": "This script extracts the pull request number from CircleCI environment variables (either `CIRCLE_PULL_REQUEST` or `CIRCLE_BRANCH`) and prints it. It is used by the `trigger_circleci.yml` workflow.",
            "spof": true
          },
          {
            "path": "utils/create_dependency_mapping.py",
            "description": "This script provides utilities for analyzing dependencies between modular model files in a source-code repository, extracting model-specific imports, and performing a topological sort to determine the correct processing order based on these dependencies.",
            "spof": false
          },
          {
            "path": "utils/check_docstrings.py",
            "description": "This utility script checks and optionally fixes docstrings of public objects within the Transformers library, ensuring that argument sections match their function/class signatures.",
            "spof": false
          },
          {
            "path": "utils/get_modified_files.py",
            "description": "This script identifies Python files modified since a common Git ancestor (merge-base main HEAD) within specified top-level subdirectories passed as arguments. It uses `git diff` to find changes and filters for Python files matching the given directories.",
            "spof": true
          },
          {
            "path": "utils/check_inits.py",
            "description": "This script is a utility for the Transformers library that checks the consistency between `_import_structure` and `TYPE_CHECKING` sections in `__init__.py` files, ensuring that objects defined for delayed imports and for type checkers are identical. It also verifies that all submodules are correctly referenced.",
            "spof": false
          },
          {
            "path": "utils/tests_fetcher.py",
            "description": "This utility script identifies and fetches relevant tests to run based on file modifications in a Git repository, optimizing CI/CD by only executing tests impacted by recent changes.",
            "spof": false
          },
          {
            "path": "utils/set_cuda_devices_for_ci.py",
            "description": "This script dynamically sets and prints the `CUDA_VISIBLE_DEVICES` environment variable, primarily for use in GitHub Actions CI workflows to manage GPU allocation based on the test folder or existing environment settings.",
            "spof": true
          },
          {
            "path": "utils/add_pipeline_model_mapping_to_test.py",
            "description": "This script automates the process of adding or updating the `pipeline_model_mapping` attribute in model test files within the `transformers` library. It is designed for CI jobs or internal `transformers` members to maintain consistency in test configurations.",
            "spof": true
          },
          {
            "path": "utils/update_metadata.py",
            "description": "This script is a utility for updating the metadata of the Hugging Face Transformers library, primarily by generating and uploading information about supported models, frameworks, and pipeline configurations to a dedicated metadata repository. It also includes a check mode to verify proper pipeline definitions.",
            "spof": false
          },
          {
            "path": "utils/patch_helper.py",
            "description": "This script automates the process of cherry-picking merged pull requests (identified by a GitHub label) onto a specified release branch, ordering them chronologically to minimize merge conflicts. It uses Git and GitHub CLI to extract, sort, and apply commits to prepare a patch.",
            "spof": true
          },
          {
            "path": "utils/check_config_docstrings.py",
            "description": "This script checks that all configuration classes in the `transformers` library have a valid Hugging Face model checkpoint link included in their docstrings. It raises an error if any config class is missing this required documentation.",
            "spof": false
          },
          {
            "path": "utils/notification_service_doc_tests.py",
            "description": "This file processes documentation test results, extracts failure details, and formats them into Slack messages for reporting within a CI/CD pipeline. It posts test summaries and detailed failure reports to a Slack channel.",
            "spof": false
          },
          {
            "path": "utils/deprecate_models.py",
            "description": "This script automates the process of deprecating specified models within the Transformers library. It moves model files, updates references, modifies documentation, and cleans up associated configurations and tests.",
            "spof": true
          },
          {
            "path": "utils/split_model_tests.py",
            "description": "This script lists test model directories and other subdirectories under `tests`, then splits them into multiple sublists. It is primarily used in GitHub Actions to distribute test jobs across multiple parallel runs, bypassing job limits.",
            "spof": true
          },
          {
            "path": "utils/notification_service.py",
            "description": "This file processes and aggregates CI test results for the `transformers` library, generating structured messages for reporting build status, failures, and warnings, likely for notification platforms like Slack.",
            "spof": true
          },
          {
            "path": "utils/add_dates.py",
            "description": "This script automates the management and validation of release dates and paper links within Hugging Face Transformers model documentation. It interacts with Git history, GitHub, and external paper information services to ensure data accuracy.",
            "spof": false
          },
          {
            "path": "utils/check_config_attributes.py",
            "description": "This script verifies that attributes defined in a `PreTrainedConfig` class's `__init__` method are actually utilized within the associated model's source code, accounting for specific exceptions.",
            "spof": true
          },
          {
            "path": "utils/get_ci_error_statistics.py",
            "description": "This script analyzes GitHub Actions workflow run artifacts to extract and categorize test errors. It fetches workflow data, downloads test result artifacts, and then processes them to generate statistics on test failures, both by error type and by affected model.",
            "spof": false
          },
          {
            "path": "utils/extract_metadata.py",
            "description": "This script extracts metadata such as 'extras' and supported Python versions from the project's `setup.py` file. It is primarily used for CI testing and can be invoked from the command line to output this information.",
            "spof": true
          },
          {
            "path": "utils/update_tiny_models.py",
            "description": "This script updates and uploads tiny (dummy) model versions to the Hugging Face Hub for CI testing. It also generates an updated summary file of these tiny models for the main `transformers` repository.",
            "spof": false
          },
          {
            "path": "utils/process_bad_commit_report.py",
            "description": "This script processes a JSON report of new failed tests, enriching it with job links and grouping failures by author. It then uploads the processed data to the Hugging Face Hub and generates a formatted Slack report for CI monitoring.",
            "spof": true
          },
          {
            "path": "utils/process_circleci_workflow_test_reports.py",
            "description": "This script processes CircleCI workflow test reports, fetching job artifacts to generate summaries of test statuses and detailed analyses of test failures, organized by test and by model, then saves them to JSON files.",
            "spof": false
          },
          {
            "path": "utils/collated_reports.py",
            "description": "This script processes individual model test reports, collates their results into a single JSON file, and optionally uploads this collated report to a Hugging Face dataset repository for benchmarking and CI/CD purposes.",
            "spof": true
          },
          {
            "path": "utils/check_model_tester.py",
            "description": "This script checks model tester classes to ensure that their configuration parameters, such as `vocab_size`, `hidden_size`, and `num_layers`, do not exceed predefined thresholds to prevent tests from becoming too large or resource-intensive. It raises an exception if any configuration values are found to be excessively large for testing.",
            "spof": false
          },
          {
            "path": "utils/extract_warnings.py",
            "description": "This script downloads GitHub Actions workflow artifacts, extracts specific types of warnings (e.g., DeprecationWarning, UserWarning) from `warnings.txt` files found within them, and saves the extracted warnings to a JSON file.",
            "spof": true
          },
          {
            "path": "utils/get_previous_daily_ci.py",
            "description": "This file provides utilities to query the GitHub Actions API for scheduled daily CI runs, retrieve their details, and download associated artifacts or reports from the Hugging Face Transformers repository.",
            "spof": false
          },
          {
            "path": "utils/get_pr_run_slow_jobs.py",
            "description": "This script identifies and suggests slow CI jobs (tests for specific models or quantizers) to run for a GitHub pull request. It can either parse explicit job names from a PR comment or infer relevant jobs based on changed files within the PR.",
            "spof": true
          },
          {
            "path": "utils/modular_model_converter.py",
            "description": "This file provides utilities for converting and refactoring Python source code within the Hugging Face Transformers library. It includes tools for renaming model-related entities while preserving case and refactoring parent class calls to `super()` calls, likely for modularization purposes.",
            "spof": true
          },
          {
            "path": "utils/check_modular_conversion.py",
            "description": "This script compares modular Python files with their generated modeling counterparts in the `transformers` library, detecting differences, optionally applying fixes, and ensuring consistency and proper formatting using a linter (ruff).",
            "spof": false
          },
          {
            "path": "examples/modular-transformers/modular_test_detr.py",
            "description": "This file defines `TestDetrModel`, a subclass of `DeformableDetrModel`, to specifically test naming conventions and suffix handling for models containing \"detr\" in their name.",
            "spof": true
          },
          {
            "path": "examples/pytorch/conftest.py",
            "description": "This `conftest.py` file configures pytest for the `transformers` library, setting up the Python path to enable source code imports for tests and integrating shared testing utilities.",
            "spof": false
          },
          {
            "path": "docs/source/_config.py",
            "description": "This file contains configuration settings for the `transformers` library documentation, including installation commands for notebooks and patterns to avoid during code formatting.",
            "spof": false
          },
          {
            "path": "docs/source/es/pr_checks.md",
            "description": "This document describes the automated checks performed on pull requests for the Hugging Face Transformers library, explaining their purpose and how to debug them locally. It covers tests, documentation generation, code style, and repository consistency checks.",
            "spof": true
          },
          {
            "path": "docs/source/it/pr_checks.md",
            "description": "This document explains the various CI/CD checks performed on Pull Requests submitted to the HuggingFace Transformers library, covering tests, documentation builds, code style, and repository consistency. It also provides guidance on how to debug these checks locally if they fail.",
            "spof": true
          },
          {
            "path": "docs/source/ja/pr_checks.md",
            "description": "This document describes the various checks performed on Pull Requests in the HuggingFace Transformers repository, including tests, documentation builds, code style, and repository consistency. It also explains how to debug these checks locally.",
            "spof": false
          },
          {
            "path": "tests/cli/test_system.py",
            "description": "This file contains system tests for the `transformers` library's command-line interface (CLI), specifically verifying the functionality of the `env` and `version` commands.",
            "spof": true
          },
          {
            "path": "tests/sagemaker/conftest.py",
            "description": "This file defines pytest fixtures and a SageMaker test environment configuration, including AWS credentials and hyperparameters, for testing SageMaker integrations within the transformers library.",
            "spof": true
          },
          {
            "path": "tests/repo_utils/test_get_test_info.py",
            "description": "This file contains unit tests for the `get_test_info` utility functions, which are responsible for extracting mappings between test classes, model classes, and tester classes from model test files. It verifies that these utilities correctly identify and map tests and testers for specific models like BERT and BLIP.",
            "spof": true
          },
          {
            "path": "tests/repo_utils/test_check_copies.py",
            "description": "This file contains unit tests for the `check_copies` utility, which is responsible for verifying the consistency of copied code blocks within the Hugging Face Transformers library. It includes tests for finding specific code snippets and ensuring copies are up-to-date.",
            "spof": false
          },
          {
            "path": "tests/repo_utils/test_tests_fetcher.py",
            "description": "This file contains unit tests for the `tests_fetcher` utility module, which provides functionalities for analyzing code, identifying dependencies, and determining relevant tests to run within the Transformers repository. It simulates repository structures and changes to verify the utility's functions like checking out commits, cleaning code, and identifying tests.",
            "spof": false
          },
          {
            "path": "tests/utils/test_expectations.py",
            "description": "This file contains unit tests for the `Expectations` class, verifying its ability to retrieve specific 'expectations' (unique IDs) based on device properties like type and version, and to raise an error when no match is found.",
            "spof": false
          },
          {
            "path": "tests/utils/test_add_new_model_like.py",
            "description": "This file contains unit tests for the `add_new_model_like` utility, which automates the process of creating new model files and configurations based on existing models within the Transformers library.",
            "spof": true
          },
          {
            "path": "tests/utils/test_hub_utils.py",
            "description": "This file contains unit tests for utility functions in the `transformers` library related to caching, downloading, and checking the existence of files from the Hugging Face Hub, including scenarios for errors, local files, gated repositories, and offline mode.",
            "spof": false
          },
          {
            "path": "tests/utils/test_file_utils.py",
            "description": "This file contains unit tests for various utility functions and import mechanisms within the `transformers` library. It specifically tests the `ContextManagers` and `find_labels` utilities, as well as the availability of the `transformers` module spec.",
            "spof": true
          },
          {
            "path": "scripts/stale.py",
            "description": "This script automates the process of identifying and managing stale issues in the HuggingFace Transformers GitHub repository. It closes issues that have been inactive for an extended period or adds a 'stale' comment to prompt activity.",
            "spof": false
          },
          {
            "path": "src/transformers/dependency_versions_check.py",
            "description": "This file checks and enforces the required versions for a set of core dependencies of the Transformers library at runtime. It ensures that installed packages meet the minimum or specific version constraints defined in `dependency_versions_table.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/dependency_versions_table.py",
            "description": "This file contains an auto-generated Python dictionary that defines the required versions and constraints for various software dependencies used by the project. It serves as a centralized list of package versions.",
            "spof": false
          },
          {
            "path": "src/transformers/configuration_utils.py",
            "description": "This file defines the `PreTrainedConfig` base class, which provides common configuration attributes and utilities for loading, saving, and managing model configurations within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/testing_utils.py",
            "description": "This file provides utility functions and decorators for testing within the Transformers library, including logic to skip tests based on environment variables and to categorize tests (e.g., slow, staging, pipeline).",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 27
          },
          {
            "name": "Yih-Dar",
            "percent": 18
          },
          {
            "name": "Yoni Gozlan",
            "percent": 11
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 80,
      "spofCount": 36
    },
    "busFactor": 2,
    "authorCount": 26
  },
  "Performance Optimization Toolkit": {
    "description": "A set of advanced tools and integrations for making models faster and more memory-efficient. Includes features like quantization, optimized attention mechanisms, and support for distributed training frameworks.",
    "functions": {
      "Performance Benchmarking and Analysis Framework": {
        "files": [
          {
            "path": "docker/transformers-pytorch-deepspeed-latest-gpu",
            "description": "This directory is intended to house the Docker configuration and scripts for a GPU-optimized environment. It facilitates benchmarking of Hugging Face Transformers models using PyTorch and Deepspeed. Its primary role is to provide a containerized setup for performance analysis.",
            "spof": false
          },
          {
            "path": "docker/transformers-quantization-latest-gpu",
            "description": "This directory is intended to contain Docker configurations and related assets for creating containerized environments. Its primary role is to facilitate benchmarking of quantized Hugging Face Transformers models. These environments are specifically tailored to utilize the latest GPU hardware.",
            "spof": false
          },
          {
            "path": "examples/pytorch/continuous_batching.py",
            "description": "This script benchmarks and compares continuous batching generation against standard generation methods for Hugging Face Transformers models, including performance metrics and optional profiling.",
            "spof": true
          },
          {
            "path": "tests/sagemaker/test_single_node_gpu.py",
            "description": "This file contains unit tests for training Hugging Face models on a single-node GPU instance using AWS SageMaker. It verifies the training process and evaluates key performance indicators like accuracy, loss, and training runtime for a specific model and script.",
            "spof": true
          },
          {
            "path": "tests/optimization/test_optimization.py",
            "description": "This file contains unit tests for various optimization components within the Hugging Face Transformers library, including optimizers like AdamW and Adafactor, and a suite of learning rate schedulers. It verifies their functional correctness and state persistence.",
            "spof": false
          },
          {
            "path": "benchmark/optimum_benchmark_wrapper.py",
            "description": "This script serves as a wrapper to execute the `optimum-benchmark` command-line tool, passing configuration directory, configuration name, and additional arguments, while also disabling Hydra logging.",
            "spof": true
          },
          {
            "path": "benchmark/README.md",
            "description": "This file provides documentation and guidelines for adding new benchmarks to the repository, including the required Python function signature and how to record metrics to a database using `MetricsRecorder`.",
            "spof": true
          },
          {
            "path": "benchmark/benchmarks_entrypoint.py",
            "description": "This file defines a `MetricsRecorder` class responsible for collecting and storing benchmark measurements, including device metrics and model performance, either in a PostgreSQL database or by exporting them to CSV files using pandas DataFrames.",
            "spof": false
          },
          {
            "path": "benchmark/benchmark.py",
            "description": "This script uses the `optimum-benchmark` library to run performance benchmarks for the Transformers library, allowing comparison across different git commits and custom metric reporting. It generates and aggregates benchmark summaries.",
            "spof": true
          },
          {
            "path": "benchmark/utils",
            "description": "This directory is designated to store utility functions and helper scripts specifically for benchmarking tasks within the `transformers` project. Its purpose is to provide reusable code for setting up, executing, or analyzing performance benchmarks.",
            "spof": false
          },
          {
            "path": "benchmark/config",
            "description": "This directory is designated for storing configuration files specifically tailored for benchmarking the `transformers` library. It centralizes various settings and parameters necessary for defining and executing benchmark runs.",
            "spof": false
          },
          {
            "path": "benchmark/benches/llama.py",
            "description": "This file implements a benchmark for the Llama-2-7b-hf model using the Hugging Face Transformers library, measuring model loading time, eager forward pass and generation performance, and token generation latency while collecting system metrics.",
            "spof": true
          },
          {
            "path": "benchmark_v2/README.md",
            "description": "This file provides documentation for Benchmarking v2, a comprehensive framework for benchmarking transformer models. It details how to run benchmarks, upload results, interpret output, and contribute new benchmarks.",
            "spof": true
          },
          {
            "path": "benchmark_v2/run_benchmarks.py",
            "description": "This script is the main entry point for running a suite of benchmarks, automatically discovering them, configuring their execution based on command-line arguments or config files, and organizing their outputs. It supports various configurations, including model-specific runs, and can optionally push results to a dataset.",
            "spof": false
          },
          {
            "path": "benchmark_v2/benchmark_scripts/continuous_batching_overall.py",
            "description": "This script benchmarks the performance of a continuous batching example by running it with various configurations and parsing its output to report throughput, duration, and generated tokens in a tabulated format.",
            "spof": true
          },
          {
            "path": "benchmark_v2/framework/benchmark_config.py",
            "description": "This file defines the `BenchmarkConfig` class for configuring benchmark scenarios in the `transformers` library, including parameters like attention implementation, batch size, and compilation settings. It also provides utilities for validating and adapting these configurations for performance testing.",
            "spof": true
          },
          {
            "path": "benchmark_v2/framework/hardware_metrics.py",
            "description": "This file provides utilities for collecting system hardware information, including CPU, memory, and software versions, with a primary focus on monitoring GPU utilization and memory usage across different GPU vendors (NVIDIA, AMD, Intel XPU) in a separate process for benchmarking purposes.",
            "spof": false
          },
          {
            "path": "benchmark_v2/framework/data_classes.py",
            "description": "This file defines data structures (`BenchmarkMetadata`, `BenchmarkResult`) and utility functions for collecting, processing, and pretty-printing benchmark results and associated metadata.",
            "spof": true
          },
          {
            "path": "scripts/check_tokenizers.py",
            "description": "This script compares the tokenization output of slow (Python-based) and fast (Rust-based) tokenizers from the Transformers library. It checks for discrepancies and reports the accuracy of the fast tokenizers against their slow counterparts across various models and a specified dataset.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/metrics.py",
            "description": "This file provides utilities for OpenTelemetry integration, including decorators for tracing function calls and a class for collecting performance metrics related to a continuous batch processor in the Transformers library.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "R√©mi Ouazan",
            "percent": 47
          },
          {
            "name": "√Åkos Hadnagy",
            "percent": 21
          },
          {
            "name": "Yuanyuan Chen",
            "percent": 10
          }
        ]
      },
      "Distributed Computing and Parallelism": {
        "files": [
          {
            "path": "examples/3D_parallel.py",
            "description": "This script tests training a model using various distributed parallelism strategies, including Tensor Parallelism (TP), Data Parallelism (DP), and Context Parallelism (CP), often referred to as 3D parallelism.",
            "spof": false
          },
          {
            "path": "examples/pytorch/3d_parallel_checks.py",
            "description": "This script tests training a HuggingFace model using a combination of Tensor Parallelism (TP), Data Parallelism (DP), and Context Parallelism (CP) in a distributed PyTorch environment, including sanity checks for parameter synchronization.",
            "spof": false
          },
          {
            "path": "examples/pytorch/context_parallel.py",
            "description": "This file demonstrates how to use PyTorch's experimental `context_parallel` feature for distributed training of a Hugging Face Causal Language Model, integrating it with `DistributedDataParallel` and optimized attention kernels.",
            "spof": false
          },
          {
            "path": "examples/pytorch/xla_spawn.py",
            "description": "This script is a launcher utility for distributed training on TPUs using PyTorch XLA, spawning multiple processes for a given training script.",
            "spof": true
          },
          {
            "path": "examples/pytorch/test_accelerate_examples.py",
            "description": "This file contains unit tests for various Hugging Face Transformers examples, ensuring they function correctly when executed with the Accelerate library without using the Trainer API. It covers tasks like GLUE, NER, SQUAD, and summarization, verifying output metrics and checkpoint creation.",
            "spof": false
          },
          {
            "path": "examples/pytorch/old_test_xla_examples.py",
            "description": "This file contains old unit tests for verifying PyTorch XLA examples and the Trainer functionality on TPUs. It tests scripts like `run_glue.py` and `test_trainer_tpu.py` using `xla_spawn`.",
            "spof": false
          },
          {
            "path": "docs/source/de/accelerate.md",
            "description": "This document provides a tutorial on how to use the Hugging Face Accelerate library for distributed training with PyTorch models. It guides users through adapting their native PyTorch training loop for distributed environments, including setup, preparation, and execution.",
            "spof": false
          },
          {
            "path": "docs/source/ar/accelerate.md",
            "description": "This is an Arabic documentation page explaining how to use the Hugging Face Accelerate library for distributed training of Transformers models. It covers setup, preparation of training components, handling the backward pass, and execution in scripts or notebooks.",
            "spof": true
          },
          {
            "path": "docs/source/en/accelerate.md",
            "description": "This document explains how to use the Accelerate library for distributed training with PyTorch, specifically within the Transformers library. It covers integration with both the `Trainer` API and native PyTorch training loops, using FSDP as an example backend.",
            "spof": true
          },
          {
            "path": "docs/source/en/experts_interface.md",
            "description": "This document explains the various experts backends available in the Hugging Face Transformers library for Mixture-of-Experts (MoE) models, detailing their characteristics, usage, and compatibility with `torch.compile`.",
            "spof": true
          },
          {
            "path": "docs/source/en/accelerator_selection.md",
            "description": "This document provides a guide on selecting the number and order of accelerators (e.g., CUDA, XPU) for distributed deep learning training. It covers methods for configuring accelerator usage with tools like torchrun, Accelerate, and DeepSpeed, and explains how to use environment variables for device ordering.",
            "spof": true
          },
          {
            "path": "docs/source/en/deepspeed.md",
            "description": "This document provides a guide on integrating and configuring DeepSpeed with Hugging Face Transformers' Trainer, covering installation, ZeRO stages, and memory optimization for distributed training of large models.",
            "spof": false
          },
          {
            "path": "docs/source/en/perf_infer_gpu_multi.md",
            "description": "This file documents how to enable and use tensor parallelism in the Hugging Face Transformers library for multi-GPU inference. It covers automatic and manual partitioning plans, various partitioning strategies, and instructions for creating custom strategies.",
            "spof": false
          },
          {
            "path": "docs/source/en/fsdp.md",
            "description": "This document provides a guide on how to configure and use FullyShardedDataParallel (FSDP) with Hugging Face Accelerate for efficient distributed training. It covers various FSDP options, including sharding strategies, CPU offload, wrapping policies, checkpointing, and TPU integration.",
            "spof": true
          },
          {
            "path": "docs/source/en/expert_parallelism.md",
            "description": "This document explains expert parallelism for Mixture-of-Experts (MoE) models within the Hugging Face Transformers library. It details how to enable and configure expert parallelism using `DistributedConfig` for large-scale model inference.",
            "spof": true
          },
          {
            "path": "docs/source/en/perf_train_cpu_many.md",
            "description": "This documentation page explains how to perform distributed training with CPUs using the Hugging Face Transformers library, detailing configurations for both bare-metal setups with the Trainer API and deployment on a Kubernetes cluster with PyTorchJob.",
            "spof": false
          },
          {
            "path": "docs/source/en/perf_train_gpu_many.md",
            "description": "This document explains various GPU parallelism methods (data, model, pipeline, tensor) for training large machine learning models, including their combinations and strategies for optimal setup. It guides users on accelerating training and fitting large models into memory across multi-GPU setups.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/nanotron.md",
            "description": "This documentation file describes Nanotron, a distributed training framework, and its integration with Hugging Face Transformers models. It covers model conversion, weight mapping, and provides resources for using Nanotron for large-scale training.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/torchtitan.md",
            "description": "This document describes how to integrate Hugging Face Transformers models with PyTorch's torchtitan distributed training framework. It provides code examples and explains the integration process for using Transformers models within torchtitan's infrastructure.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/deepspeed.md",
            "description": "This documentation file describes DeepSpeed integration with Hugging Face Transformers, explaining its benefits and how to use it with or without the `Trainer` class, particularly highlighting the `HfDeepSpeedConfig`.",
            "spof": true
          },
          {
            "path": "docs/source/es/accelerate.md",
            "description": "This document provides a tutorial on how to use the Hugging Face Accelerate library for distributed training of ü§ó Transformers models. It covers configuring Accelerate, preparing training components, and adapting the training loop for distributed environments.",
            "spof": false
          },
          {
            "path": "docs/source/hi/accelerate.md",
            "description": "This documentation file, written in Hindi, provides a tutorial on using Hugging Face Accelerate for distributed training with ü§ó Transformers models. It covers the setup, preparation of training objects, replacing the backward pass, and launching the training process.",
            "spof": true
          },
          {
            "path": "docs/source/it/accelerate.md",
            "description": "This document is an Italian-language guide explaining how to use the Hugging Face Accelerate library for distributed training of Transformers models. It covers basic setup, integrating Accelerate into a training loop, and launching training from scripts or notebooks.",
            "spof": false
          },
          {
            "path": "docs/source/it/perf_train_tpu.md",
            "description": "This document provides information, currently incomplete, on how to train models using TPUs. It references general training strategies also applicable to GPU training.",
            "spof": false
          },
          {
            "path": "docs/source/it/perf_infer_gpu_many.md",
            "description": "This document provides guidance on performing efficient inference using multiple GPUs. It details techniques and considerations for optimizing inference performance in such environments.",
            "spof": true
          },
          {
            "path": "docs/source/it/perf_train_cpu_many.md",
            "description": "Questa guida spiega come configurare e utilizzare l'addestramento distribuito efficiente su pi√π CPU con PyTorch DDP, Intel oneCCL e Intel MPI nel contesto dei modelli Transformers.",
            "spof": true
          },
          {
            "path": "docs/source/pt/accelerate.md",
            "description": "This document is a tutorial explaining how to use the Hugging Face Accelerate library for distributed training of PyTorch models, demonstrating how to adapt an existing training loop for multi-GPU or distributed setups with minimal code changes.",
            "spof": false
          },
          {
            "path": "docs/source/zh/big_models.md",
            "description": "This document explains how to instantiate and manage large models in Hugging Face Transformers, focusing on memory efficiency. It covers techniques like sharded checkpoints and low-memory loading with Accelerate to overcome memory limitations.",
            "spof": false
          },
          {
            "path": "docs/source/zh/perf_infer_gpu_multi.md",
            "description": "This document explains how to perform multi-GPU inference using PyTorch's tensor parallelism within the Hugging Face Transformers library, providing code examples and discussing expected performance benefits for supported models like Llama.",
            "spof": true
          },
          {
            "path": "docs/source/zh/accelerate.md",
            "description": "This document provides a guide on how to use the Hugging Face `accelerate` library to enable distributed training for models, specifically with ü§ó Transformers. It details the setup, preparation of training objects, and the use of `accelerator.backward` to adapt a native PyTorch training loop for distributed environments.",
            "spof": true
          },
          {
            "path": "docs/source/zh/fsdp.md",
            "description": "This document introduces and guides the configuration and usage of Fully Sharded Data Parallel (FSDP) for large model training within the Hugging Face `transformers` library, utilizing the `accelerate` framework. It covers FSDP settings, such as sharding strategies, CPU offload, wrapping policies, checkpointing, and TPU support.",
            "spof": true
          },
          {
            "path": "docs/source/zh/main_classes/deepspeed.md",
            "description": "This document provides a comprehensive guide on integrating and utilizing DeepSpeed within the Hugging Face Transformers library, covering installation, various ZeRO stages, and configuration for single-GPU, multi-GPU, and multi-node training and inference scenarios.",
            "spof": false
          },
          {
            "path": "docs/source/ja/perf_train_cpu_many.md",
            "description": "This document provides instructions in Japanese for efficient distributed training on multiple CPUs using Intel¬Æ oneCCL and PyTorch DDP within the Hugging Face Transformers library. It covers installation, environment setup, and example commands for single-node and multi-node CPU training.",
            "spof": true
          },
          {
            "path": "docs/source/ja/accelerate.md",
            "description": "This documentation page, written in Japanese, explains how to use Hugging Face Accelerate for distributed training with Hugging Face Transformers models. It covers installation, basic setup, modification of the training loop, and how to launch training with Accelerate.",
            "spof": false
          },
          {
            "path": "docs/source/ja/perf_infer_gpu_many.md",
            "description": "This document provides information on how to perform efficient inference on multiple GPUs, including considerations for Flash Attention 2. It also notes that many strategies from single-GPU setups are applicable.",
            "spof": true
          },
          {
            "path": "docs/source/ja/perf_train_gpu_many.md",
            "description": "This document explains various parallelism techniques (DataParallel, TensorParallel, PipelineParallel, ZeRO) for efficient multi-GPU training of large models, particularly within a PyTorch and Hugging Face context. It details different strategies for scaling model training across single and multi-node setups, including their benefits and drawbacks.",
            "spof": false
          },
          {
            "path": "docs/source/ja/perf_train_tpu.md",
            "description": "This document provides (or will provide) instructions and guidance on training models using TPUs within the Transformers library, primarily in Japanese. It currently serves as a placeholder while referencing other GPU training documentation.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/deepspeed.md",
            "description": "This document provides a guide in Japanese for integrating DeepSpeed with Hugging Face Transformers, covering installation, deployment configurations (multi-GPU, single-GPU, multi-node), and examples for training and inference.",
            "spof": false
          },
          {
            "path": "docs/source/ko/accelerate.md",
            "description": "This file is a Korean-language tutorial explaining how to use Hugging Face Accelerate for distributed training of models, particularly within the Transformers library. It details the setup, integration, and launch of training with Accelerate.",
            "spof": false
          },
          {
            "path": "docs/source/ko/accelerator_selection.md",
            "description": "This document provides instructions in Korean on how to select the number and order of GPUs for distributed training, using tools like `torchrun`, `accelerate launch`, `deepspeed`, and environment variables like `CUDA_VISIBLE_DEVICES` and `CUDA_DEVICE_ORDER`.",
            "spof": false
          },
          {
            "path": "docs/source/ko/deepspeed.md",
            "description": "This document provides a guide on how to use DeepSpeed with the Hugging Face Transformers library, covering installation, memory requirements, and detailed configuration for different ZeRO stages.",
            "spof": true
          },
          {
            "path": "docs/source/ko/fsdp.md",
            "description": "This document provides a guide in Korean on using Fully Sharded Data Parallel (FSDP) for training large models with Hugging Face Transformers and Accelerate. It covers FSDP concepts, configuration options like sharding strategies and CPU offload, and how to launch FSDP-enabled training.",
            "spof": true
          },
          {
            "path": "docs/source/ko/perf_train_cpu_many.md",
            "description": "This document provides instructions for setting up and running efficient distributed training on multiple CPUs using Intel's oneCCL and MPI libraries with PyTorch, specifically within the Hugging Face Transformers Trainer. It covers installation, environment setup, and example commands for single-node and multi-node CPU training.",
            "spof": true
          },
          {
            "path": "docs/source/ko/perf_infer_gpu_multi.md",
            "description": "This document provides a guide on how to implement and configure tensor parallelism for distributed inference on multiple GPUs using the Transformers library, detailing various partitioning strategies and custom implementation methods.",
            "spof": true
          },
          {
            "path": "docs/source/ko/perf_train_gpu_many.md",
            "description": "This document, written in Korean, provides a comprehensive guide to efficient training on multiple GPUs for transformer models, detailing various parallelism techniques like DataParallel, TensorParallel, PipelineParallel, and ZeRO, along with their concepts and scalability strategies.",
            "spof": false
          },
          {
            "path": "tests/deepspeed/test_deepspeed.py",
            "description": "This file contains unit and integration tests for the DeepSpeed integration within the Hugging Face Transformers library, specifically focusing on functionalities like ZeRO-3 optimization and initialization.",
            "spof": false
          },
          {
            "path": "tests/deepspeed/test_alst_ulysses_sp.py",
            "description": "This file contains a test for the Hugging Face `Trainer` to verify that enabling ALST/Ulysses sequence parallelism with DeepSpeed and Accelerate produces equivalent training losses compared to disabling it. It runs a training script both with and without sequence parallelism and asserts the numerical closeness of the resulting losses.",
            "spof": true
          },
          {
            "path": "tests/deepspeed/test_model_zoo.py",
            "description": "This file defines and configures integration tests for various Hugging Face models across different tasks using DeepSpeed. It prepares command-line arguments and specifies 'tiny' model versions to be tested with DeepSpeed's distributed training capabilities.",
            "spof": false
          },
          {
            "path": "tests/sagemaker/test_multi_node_data_parallel.py",
            "description": "This file contains integration tests for running Hugging Face models with multi-node data parallelism on Amazon SageMaker. It defines tests that launch SageMaker training jobs and verify their training runtime and evaluation metrics against predefined thresholds.",
            "spof": true
          },
          {
            "path": "tests/sagemaker/test_multi_node_model_parallel.py",
            "description": "This file contains parameterized tests for verifying multi-node, model-parallel training of HuggingFace models on Amazon SageMaker. It uses `smdistributed` for model parallelism and asserts key performance indicators from training jobs.",
            "spof": true
          },
          {
            "path": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "description": "This script is designed for fine-tuning Transformer models for sequence classification tasks, primarily on the GLUE benchmark. It handles argument parsing for model, data, and training configurations, and sets up data loading, logging, and training procedures.",
            "spof": false
          },
          {
            "path": "tests/sagemaker/scripts/pytorch/run_ddp.py",
            "description": "This script serves as an entry point for running PyTorch distributed training on SageMaker, configuring `torch.distributed.launch` with parameters derived from SageMaker environment variables to execute `run_glue.py`.",
            "spof": true
          },
          {
            "path": "tests/fsdp/test_context_parallel.py",
            "description": "This file implements a test for the `Trainer`'s context parallelism (CP) feature within FSDP. It verifies that training losses are equivalent when CP is enabled compared to when it's disabled, and also serves as the training script executed by the test.",
            "spof": true
          },
          {
            "path": "tests/fsdp/test_fsdp.py",
            "description": "This file contains unit and integration tests for the Fully Sharded Data Parallel (FSDP) functionality within the Hugging Face Transformers library. It specifically tests FSDP's configuration, usage with the Trainer API, and integration with the Accelerate library for distributed training.",
            "spof": false
          },
          {
            "path": "tests/extended/test_trainer_ext.py",
            "description": "This file contains extended unit tests for the `Trainer` class in the Hugging Face Transformers library, specifically focusing on sequence-to-sequence tasks, distributed training configurations, logging, and memory optimization with `bitsandbytes`.",
            "spof": false
          },
          {
            "path": "tests/tensor_parallel/test_tensor_parallel.py",
            "description": "This file contains unit tests for the tensor parallelism utilities and features within the Hugging Face Transformers library. It specifically verifies the correctness of packed weight conversions and the `tp_plan` property of models.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/accelerate.py",
            "description": "This file provides utility functions for integrating with the Hugging Face Accelerate library, focusing on model memory management, device mapping, and module size computations for efficient model loading and execution.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/tpu.py",
            "description": "This file provides utility functions for integrating Hugging Face Transformers with TPUs. Specifically, it defines a function to modify a `DataLoader` for PyTorch/XLA FSDP with SPMD (Single Program, Multiple Data) by adding a sharding specification.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/deepspeed.py",
            "description": "This file provides classes and utilities for integrating DeepSpeed into the HuggingFace Transformers library, particularly for managing and synchronizing DeepSpeed configurations with `TrainingArguments` within the `Trainer` class.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/tensor_parallel.py",
            "description": "This file provides utilities for setting up and managing tensor parallelism in models, including initialization of device meshes and sharding of model weights for distributed execution.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/fsdp.py",
            "description": "This file provides utility functions for detecting and managing PyTorch's Fully Sharded Data Parallel (FSDP) functionalities. It includes checks for whether a module is FSDP-managed and if FSDP is enabled via environment variables.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Ferdinand Mom",
            "percent": 22
          },
          {
            "name": "Arthur",
            "percent": 14
          },
          {
            "name": "Steven Liu",
            "percent": 12
          }
        ]
      },
      "Optimized Attention and Computation Kernels": {
        "files": [
          {
            "path": "examples/modular-transformers/modular_global_indexing.py",
            "description": "This file defines a custom attention function (`custom_flex`) and registers it with the `transformers` library's `AttentionInterface`. It also introduces `GlobalIndexingAttention` as a subclass of `LlamaAttention`.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_add_function.py",
            "description": "This file implements a modified multi-headed attention mechanism, including functions for Rotary Position Embedding, and is automatically generated from a modular component, likely for a modular transformer architecture.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_switch_function.py",
            "description": "This file implements the `SwitchFunctionAttention` mechanism, including rotary position embeddings and helper functions for attention computations, as part of a modular transformer model within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_global_indexing.py",
            "description": "This file defines a modular multi-headed attention mechanism, `GlobalIndexingAttention`, for transformer models. It incorporates Rotary Position Embeddings, Grouped Query Attention, and leverages kernelized functions for efficient computation.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_add_function.py",
            "description": "This file demonstrates how to integrate and use the `apply_rotary_pos_emb` function (from Llama models) within a class derived from ZambaAttention, highlighting that this function is not natively present in Zamba.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_switch_function.py",
            "description": "This file defines a specialized attention class, `SwitchFunctionAttention`, that inherits from `LlamaAttention` but explicitly uses the `rotate_half` function from the Cohere model's implementation instead of Llama's.",
            "spof": true
          },
          {
            "path": "examples/pytorch/continuous_batching_simple.py",
            "description": "This file demonstrates continuous batching for text generation using Hugging Face Transformers. It loads a pre-trained model, processes a dataset, and performs batched generation to measure and display performance metrics like tokens per second.",
            "spof": true
          },
          {
            "path": "docs/source/ar/attention.md",
            "description": "This document, written in Arabic, explains various attention mechanisms used in Transformer models, focusing on efficient approaches for long sequences like LSH attention (Reformer) and local attention (Longformer), as well as related techniques such as axial positional encodings.",
            "spof": true
          },
          {
            "path": "docs/source/en/attention_interface.md",
            "description": "This document explains how to use, configure, and customize different attention backend implementations within the Transformers library. It details how to set attention functions for models, manage attention masks, and register new custom attention functions.",
            "spof": false
          },
          {
            "path": "docs/source/en/continuous_batching.md",
            "description": "This document explains continuous batching in the Hugging Face Transformers library, detailing how it optimizes GPU utilization for LLM inference by dynamically managing requests and showcasing its API usage.",
            "spof": false
          },
          {
            "path": "docs/source/en/main_classes/kernels.md",
            "description": "This file documents the kernel configuration utilities for the Hugging Face Transformers library, specifically focusing on the `KernelConfig` class.",
            "spof": true
          },
          {
            "path": "docs/source/en/kernel_doc/overview.md",
            "description": "This document provides an overview of Kernels, explaining their role in optimizing PyTorch operations, how Hugging Face distributes them, and their integration and usage within the Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/kernel_doc/loading_kernels.md",
            "description": "This document explains how to load and configure optimized kernels in Hugging Face Transformers to accelerate model inference and training, including installation, various loading methods, and troubleshooting.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/yoso.md",
            "description": "This file documents the YOSO (You Only Sample (Almost) Once) model, which approximates self-attention using LSH-based Bernoulli sampling for efficiency. It provides an overview, usage tips, and details on its implementation within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/es/attention.md",
            "description": "This document describes efficient attention mechanisms and related techniques, such as LSH attention, local attention, and axial positional encoding, used in Transformer models like Longformer and Reformer to handle long sequences.",
            "spof": true
          },
          {
            "path": "docs/source/zh/attention.md",
            "description": "This document explains efficient attention mechanisms for Transformer models, focusing on techniques like LSH attention, local attention, and axial positional encoding used in models such as Reformer and Longformer to handle long input sequences.",
            "spof": true
          },
          {
            "path": "docs/source/ja/attention.md",
            "description": "This file is a Japanese documentation explaining various attention mechanisms (like LSH attention and local attention) and other techniques (like axial positional encodings) used in Transformer models to efficiently handle long sequences, featuring models like Reformer and Longformer.",
            "spof": true
          },
          {
            "path": "tests/test_modeling_common.py",
            "description": "This file contains common utilities and test parameterizations for evaluating the behavior of Transformer models. It specifically focuses on comparing the eager and SDPA (Scaled Dot-Product Attention) implementations across various data types, padding strategies, and attention mask configurations.",
            "spof": false
          },
          {
            "path": "tests/kernels/test_kernels.py",
            "description": "This file contains unit tests for the kernel integration features in the Hugging Face Transformers library. It verifies the functionality of kernelized models, kernel configuration, and environment variable handling related to kernels.",
            "spof": true
          },
          {
            "path": "tests/utils/test_masking_utils.py",
            "description": "This file contains unit tests for the masking utility functions used in the Hugging Face Transformers library, covering causal, chunked, and packed sequence masks across different attention implementations.",
            "spof": false
          },
          {
            "path": "src/transformers/modeling_rope_utils.py",
            "description": "This file provides utility functions and a decorator for managing and dynamically adjusting Rotary Positional Embeddings (RoPE) frequencies and scaling during model inference, supporting different scaling methods like linear and NTK scaling based on sequence length.",
            "spof": true
          },
          {
            "path": "src/transformers/modeling_attn_mask_utils.py",
            "description": "This file contains utilities for creating and converting attention masks, including causal and sliding window masks, for use in transformer models. It is marked as deprecated, with new code advised to use `masking_utils.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/masking_utils.py",
            "description": "This module provides utility functions for creating, combining, and manipulating various attention masks (e.g., causal, sliding window, padding) for transformer models, including optimizations for different hardware backends like XPU and SDPA.",
            "spof": false
          },
          {
            "path": "src/transformers/modeling_flash_attention_utils.py",
            "description": "This file provides utility functions for integrating and managing different Flash Attention implementations (like Flash Attention 2, Flash Attention 3, and NPU/XPU variants) within the Transformers library. It handles lazy loading of these implementations, adapts their APIs, and includes helper functions for input padding and unpadding necessary for efficient use.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/flash_attention.py",
            "description": "This file provides a wrapper function (`flash_attention_forward`) to integrate Flash Attention into the Transformers library. It handles data type conversion, input preprocessing (like transposing tensors), and argument mapping for efficient attention computation, leveraging an underlying Flash Attention implementation.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/npu_flash_attention.py",
            "description": "This file provides NPU (Neural Processing Unit) optimized Flash Attention functions for the Transformers library, leveraging Huawei Ascend NPU's fusion attention capabilities with support for different causal mask alignments.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/flex_attention.py",
            "description": "This file provides utility functions and a wrapper for integrating PyTorch's `flex_attention` into the Transformers library, including dynamic compilation management and block mask generation for efficient attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/moe.py",
            "description": "This file provides optimized implementations for the forward pass of Mixture-of-Experts (MoE) layers, utilizing batched and grouped matrix multiplication techniques for efficient computation.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/flash_paged.py",
            "description": "This file implements a paged attention forward pass, designed for efficient attention computation using a paged key-value cache. It integrates a FlashAttention-like kernel to handle variable sequence lengths and causal attention in models like large language models.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/eager_paged.py",
            "description": "This file implements an eager, non-compiled version of paged attention, designed to integrate with a PagedAttentionCache for efficient key-value management in transformer models.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/sdpa_paged.py",
            "description": "This file implements a paged attention forward pass using `torch.nn.functional.scaled_dot_product_attention`, incorporating a PagedAttentionCache and handling grouped query attention. It is designed for efficient attention computations, likely within a continuous batching setup.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/sdpa_attention.py",
            "description": "This file implements a wrapper around `torch.nn.functional.scaled_dot_product_attention` (SDPA) for `transformers` models, providing logic for Grouped Query Attention (GQA) and handling hardware-specific (NPU, XPU) considerations and `torch` version compatibility.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/hub_kernels.py",
            "description": "This file manages the integration of optimized hardware kernels from the Hugging Face `kernels` hub into the `transformers` library. It defines mappings for various model layers and functions to their respective kernel implementations, enabling performance enhancements across different devices.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/kernel_config.py",
            "description": "This file defines the `KernelConfig` class and related utilities for managing and validating custom kernel mappings for deep learning models, enabling the specification of optimized kernels for specific layers and devices.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 22
          },
          {
            "name": "Raushan Turganbay",
            "percent": 12
          },
          {
            "name": "Anton Vlasjuk",
            "percent": 10
          }
        ]
      },
      "Model Quantization Framework": {
        "files": [
          {
            "path": "examples/quantization/custom_quantization.py",
            "description": "This file demonstrates how to implement and apply a custom 8-bit quantization method within the Hugging Face Transformers library, using a pre-trained `facebook/opt-350m` model for text generation.",
            "spof": true
          },
          {
            "path": "examples/quantization/custom_quantization_int8_example.py",
            "description": "This file implements and demonstrates a custom INT8 symmetric quantization method for Hugging Face Transformers models, including defining a specialized linear layer, quantization configuration, and a quantizer to apply it.",
            "spof": false
          },
          {
            "path": "docs/source/en/main_classes/quantization.md",
            "description": "This file documents the quantization techniques and configuration options available in the Hugging Face Transformers library. It explains how quantization reduces model size and speeds up inference, detailing various `Config` classes for different quantization algorithms like AWQ, GPTQ, and BitsAndBytes.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/auto_round.md",
            "description": "This document provides a comprehensive guide to AutoRound, an advanced quantization algorithm for large language models, covering installation, supported configurations, quantization procedures, and inference methods.",
            "spof": true
          },
          {
            "path": "docs/source/en/quantization/aqlm.md",
            "description": "This document provides an overview of Additive Quantization of Language Models (AQLM) within the Hugging Face Transformers library. It explains how to install, load, and configure AQLM-quantized models, detailing various setups and their performance characteristics.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/contribute.md",
            "description": "This document guides developers on how to integrate new quantization methods into the Hugging Face Transformers library using the `HfQuantizer` class. It outlines requirements, file structure, and implementation steps for contributing quantization support.",
            "spof": true
          },
          {
            "path": "docs/source/en/quantization/eetq.md",
            "description": "This document describes the Easy & Efficient Quantization for Transformers (EETQ) library, detailing its features, installation, and how to use it for quantizing Hugging Face Transformer models.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/higgs.md",
            "description": "This file provides documentation for HIGGS, a zero-shot quantization algorithm. It explains how to install and use HIGGS for model quantization with the Transformers library, including its compatibility with `torch.compile`.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/bitnet.md",
            "description": "This document describes BitNet, a quantization-aware training technique that uses BitLinear layers with ternary weights and 8-bit activations. It explains the quantization process, how to load BitNet models, and relevant resources for training and fine-tuning.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/fbgemm_fp8.md",
            "description": "This document provides instructions and examples for quantizing Hugging Face Transformers models to FP8 using the FBGEMM library, including installation, configuration, and model saving/loading.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/fp_quant.md",
            "description": "This documentation file introduces FP-Quant, a set of quantization algorithms designed for LLMs on NVIDIA Blackwell GPUs, detailing its integration with the Hugging Face Transformers library and usage examples for post-training and quantization-aware training.",
            "spof": true
          },
          {
            "path": "docs/source/en/quantization/awq.md",
            "description": "This documentation provides a guide on using Activation-aware Weight Quantization (AWQ) with the Hugging Face Transformers library, detailing how to load AWQ-quantized models, utilize fused modules for performance, and leverage ExLlamaV2 kernels.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/quark.md",
            "description": "This document introduces Quark, a deep learning quantization toolkit, outlining its features, supported quantization methods, and data types. It also provides instructions and examples on how to integrate and use Quark-quantized models within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/quantization/optimum.md",
            "description": "This documentation file introduces Hugging Face Optimum, an optimization library that supports quantization for various hardware and model accelerators like Intel CPUs, AMD GPUs, Furiousa NPUs, and ONNX Runtime. It aims to explain how Optimum enhances performance through quantization.",
            "spof": true
          },
          {
            "path": "docs/source/en/quantization/overview.md",
            "description": "This document provides an overview of quantization methods supported by the Transformers library, comparing their features, hardware compatibility, and bit precision. It also includes resources and tools for users interested in model quantization.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/bitsandbytes.md",
            "description": "This document provides comprehensive guidance on using the `bitsandbytes` library for quantizing Large Language Models (LLMs) within the Hugging Face Transformers ecosystem. It covers installation, hardware compatibility, 8-bit and 4-bit quantization methods (LLM.int8 and QLoRA), and practical examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/finegrained_fp8.md",
            "description": "This documentation file explains fine-grained FP8 quantization in the Transformers library, detailing its application to weights and activations, supported models, hardware requirements, and provides code examples for implementation and usage.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/selecting.md",
            "description": "This guide helps users select the most suitable quantization methods for different use cases (inference, fine-tuning, research) in Transformers, detailing their advantages, disadvantages, and providing benchmark comparisons.",
            "spof": true
          },
          {
            "path": "docs/source/en/quantization/quanto.md",
            "description": "This documentation file explains how to use the Quanto library, a PyTorch quantization backend, with Hugging Face Optimum and Transformers models. It covers installation, model weight quantization with `QuantoConfig`, and compatibility with `torch.compile`.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/spqr.md",
            "description": "This document describes the SpQR quantization algorithm, explaining its structure and providing instructions and code examples for loading SpQR-quantized models within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/gptq.md",
            "description": "This documentation file explains how to use the GPTQ (GPT-QModel) post-training quantization technique within the Hugging Face Transformers library, covering installation, model quantization, saving/loading, and the use of optimized kernels like Marlin.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/compressed_tensors.md",
            "description": "This document introduces `compressed-tensors`, an extension to `safetensors` for handling quantized and sparse tensor data. It explains its features, installation, and how to use and inspect compressed models within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/hqq.md",
            "description": "This document provides an overview and usage guide for Half-Quadratic Quantization (HQQ) within the Hugging Face Transformers library. It details installation, model quantization methods, and supported backends for HQQ.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/mxfp4.md",
            "description": "This document describes MXFP4 quantization, a 4-bit floating-point format used in the Hugging Face Transformers library to reduce memory requirements for large models. It details its benefits, hardware requirements, and how to verify and use MXFP4 with compatible models.",
            "spof": true
          },
          {
            "path": "docs/source/en/quantization/torchao.md",
            "description": "This document provides a guide on integrating and utilizing the `torchao` library for model quantization within the Hugging Face Transformers framework. It covers `torchao` features, supported quantization techniques, hardware compatibility, installation, and practical examples for different GPU architectures and CPU.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/vptq.md",
            "description": "This document introduces Vector Post-Training Quantization (VPTQ), a method for quantizing large language models to low bit-widths. It explains how to install and use VPTQ, including loading quantized models, understanding bit-width calculations, and provides performance benchmarks and resources.",
            "spof": false
          },
          {
            "path": "docs/source/en/quantization/concept_guide.md",
            "description": "This document explains the core concepts, methods, and benefits of quantization in machine learning models, detailing various schemes (int4, FP8, affine), granularity, and techniques (PTQ, QAT) for reducing model size and improving inference speed. It also outlines how quantization is integrated and utilized within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bitnet.md",
            "description": "This document provides a comprehensive overview and usage guide for the BitNet model within the Hugging Face Transformers library, detailing its architecture, quantization, training, and available variants, alongside an important note on achieving its computational efficiency.",
            "spof": false
          },
          {
            "path": "docs/source/it/perf_infer_gpu_one.md",
            "description": "This document provides guidance on efficient inference on a single GPU, focusing on the integration of `bitsandbytes` for Int8 mixed-precision matrix decomposition. It includes instructions and examples for setting up 8-bit quantization for both single and multi-GPU configurations.",
            "spof": false
          },
          {
            "path": "docs/source/zh/main_classes/quantization.md",
            "description": "This document provides a comprehensive guide to quantizing Hugging Face Transformers models using various methods, including AWQ, GPTQ, and bitsandbytes. It covers how to load, quantify, and fine-tune models, along with performance benchmarks and usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/ja/main_classes/quantization.md",
            "description": "This document provides a guide in Japanese on how to quantize Hugging Face Transformers models using techniques like GPTQ and bitsandbytes. It covers loading, quantizing, saving, and fine-tuning models to optimize memory usage and inference speed.",
            "spof": false
          },
          {
            "path": "docs/source/ko/perf_infer_gpu_one.md",
            "description": "This document provides a guide in Korean for efficient inference on a single GPU, focusing on integrating `bitsandbytes` for FP4 and Int8 mixed-precision quantization to reduce model size and memory footprint.",
            "spof": false
          },
          {
            "path": "docs/source/ko/main_classes/quantization.md",
            "description": "This document provides Korean-language documentation on quantization techniques within the Hugging Face Transformers library, explaining how they reduce memory and computation costs, and detailing various supported quantization configurations like AWQ, GPTQ, and BitsAndBytes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/quantization/gptq.md",
            "description": "This documentation page in Korean provides a guide on how to use GPTQ quantization with the Hugging Face Transformers library. It covers the steps for installing necessary libraries, configuring GPTQ, quantizing a model, and then saving and loading the quantized model.",
            "spof": false
          },
          {
            "path": "docs/source/ko/quantization/awq.md",
            "description": "This document provides a guide in Korean on using Activation-aware Weight Quantization (AWQ) with Hugging Face Transformers, covering how to load AWQ-quantized models, utilize fused modules for performance, and leverage ExLlama-v2 support.",
            "spof": true
          },
          {
            "path": "docs/source/ko/quantization/eetq.md",
            "description": "This document is a Korean-language guide to the EETQ quantization library, detailing its features, installation process, and how to use it for quantizing and loading models within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/quantization/bitsandbytes.md",
            "description": "This document provides a guide to using the `bitsandbytes` library for 8-bit and 4-bit model quantization within the Hugging Face Transformers library, covering installation, configuration, and specific features for memory optimization and fine-tuning. It includes examples for both 8-bit (LLM.int8()) and 4-bit (QLoRA) quantization methods, as well as instructions for dequantization.",
            "spof": true
          },
          {
            "path": "docs/source/ko/quantization/quanto.md",
            "description": "This file is a Korean-language documentation page explaining the Quanto library for model quantization and its integration with Hugging Face Transformers. It covers Quanto's features, installation, and basic usage for weight quantization in models.",
            "spof": true
          },
          {
            "path": "docs/source/ko/quantization/quark.md",
            "description": "This file is a Korean documentation page introducing Quark, a deep learning quantization toolkit, and its integration with Hugging Face Transformers. It details Quark's features, supported quantization types, and provides examples for using Quark-quantized models within the Transformers library.",
            "spof": true
          },
          {
            "path": "tests/quantization/higgs/test_higgs.py",
            "description": "This file contains unit tests for the Higgs quantization method within the Hugging Face Transformers library. It verifies the functionality of HiggsConfig, quantized model conversion, generation, saving/loading, and multi-GPU usage for Higgs-quantized models.",
            "spof": true
          },
          {
            "path": "tests/quantization/autoround/test_auto_round.py",
            "description": "This file contains unit tests for the AutoRound quantization feature within the Hugging Face Transformers library. It verifies the functionality of AutoRound for various model configurations, data types, device setups, and model loading/saving scenarios.",
            "spof": false
          },
          {
            "path": "tests/quantization/mxfp4/test_mxfp4.py",
            "description": "This file contains unit and integration tests for the MXFP4 quantization functionality within the Transformers library, including tests for `Mxfp4Config` and `Mxfp4HfQuantizer`.",
            "spof": false
          },
          {
            "path": "tests/quantization/ggml/test_ggml.py",
            "description": "This file contains unit tests for the integration and functionality of GGUF (GGML Universal File Format) quantized models within the Hugging Face Transformers library, covering various quantization types, tokenization, and model serialization.",
            "spof": false
          },
          {
            "path": "tests/quantization/autoawq/test_awq.py",
            "description": "This file contains unit tests for the AWQ (Activation-aware Weight Quantization) feature in the Hugging Face Transformers library. It validates the `AwqConfig` class and the functionality of AWQ-quantized models, including model conversion, inference, and handling of different backends and data types.",
            "spof": false
          },
          {
            "path": "tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py",
            "description": "This file contains unit tests for the FBGEMM FP8 quantization feature in the HuggingFace Transformers library. It verifies the functionality of FbgemmFp8Config and the behavior of quantized models, including conversion, inference, saving/loading, and multi-GPU usage.",
            "spof": false
          },
          {
            "path": "tests/quantization/bnb/test_4bit.py",
            "description": "This file contains unit tests for the 4-bit quantization feature using BitsAndBytes within the Hugging Face Transformers library. It verifies aspects like memory footprint, parameter counts, module types, and generation quality of 4-bit quantized models.",
            "spof": false
          },
          {
            "path": "tests/quantization/bnb/test_mixed_int8.py",
            "description": "This file contains unit tests for the 8-bit mixed precision quantization functionality within the Hugging Face Transformers library, focusing on `bitsandbytes` integration. It verifies quantization correctness, memory footprint, skipped modules, and generation quality of quantized models.",
            "spof": false
          },
          {
            "path": "tests/quantization/bnb/README.md",
            "description": "This README provides a guide for testing and troubleshooting `bitsandbytes` integration for mixed int8 quantization with Hugging Face `transformers`. It covers library and hardware requirements, virtual environment setup, and solutions for common CUDA and `bitsandbytes`-related errors.",
            "spof": false
          },
          {
            "path": "tests/quantization/gptq/test_gptq.py",
            "description": "This file contains unit tests and integration tests for the GPTQ quantization functionality in the Hugging Face Transformers library, covering configuration, model loading, memory footprint, generation quality, and serialization of quantized models.",
            "spof": false
          },
          {
            "path": "tests/quantization/aqlm_integration/test_aqlm.py",
            "description": "This file contains integration tests for AQLM (Adaptive Quantization for Language Models) quantization within the Hugging Face Transformers library. It verifies the functionality of AQLM configuration, model conversion, quantized model loading, inference, saving, and multi-GPU usage.",
            "spof": false
          },
          {
            "path": "tests/quantization/compressed_tensors_integration/test_compressed_tensors.py",
            "description": "This file contains integration tests for the `CompressedTensorsConfig` and various quantized models (e.g., TinyLlama with different quantization schemes) within the Hugging Face Transformers library. It verifies correct configuration handling, model loading, and perplexity of quantized models.",
            "spof": false
          },
          {
            "path": "tests/quantization/compressed_tensors_integration/test_compressed_models.py",
            "description": "This file contains integration tests for `transformers` models using `compressed_tensors`. It verifies that compressed models maintain weight and output consistency compared to their uncompressed counterparts, and ensures proper handling of `run_compressed` configurations during model loading.",
            "spof": false
          },
          {
            "path": "tests/quantization/bitnet_integration/test_bitnet.py",
            "description": "This file contains integration tests for the BitNet quantization features within the Hugging Face Transformers library. It verifies the functionality of BitNet configuration, model quantization, weight handling, and model serialization/deserialization.",
            "spof": false
          },
          {
            "path": "tests/quantization/quanto_integration/test_quanto.py",
            "description": "This file contains integration tests for the `quanto` quantization library within the Hugging Face Transformers library, verifying model quantization, layer conversion, inference quality, and serialization behavior.",
            "spof": true
          },
          {
            "path": "tests/quantization/fp_quant_integration/test_fp_quant.py",
            "description": "This file contains integration tests for floating-point quantization (FPQuant) functionalities within the Hugging Face Transformers library. It verifies the behavior of FPQuantConfig and tests various FPQuant configurations on a causal language model, including model loading, generation, saving, and multi-accelerator support.",
            "spof": true
          },
          {
            "path": "tests/quantization/vptq_integration/test_vptq.py",
            "description": "This file contains integration tests for the Vector Quantized Training (VPTQ) quantization feature within the Hugging Face Transformers library. It verifies the functionality, loading, saving, and multi-GPU usage of VPTQ-quantized models, as well as the correct application of VPTQ layers during conversion.",
            "spof": true
          },
          {
            "path": "tests/quantization/finegrained_fp8/test_fp8.py",
            "description": "This file contains unit tests for the fine-grained FP8 quantization feature within the Hugging Face Transformers library. It verifies the correct functionality of FP8 configurations, model conversion, quantized model inference, and persistence, including edge cases like dequantization and environments without accelerators.",
            "spof": false
          },
          {
            "path": "tests/quantization/hqq/test_hqq.py",
            "description": "This file contains unit tests for the HQQ (Half-Quadratic Quantization) integration within the Hugging Face Transformers library, verifying its functionality, configuration, multi-GPU support, and serialization with various models and settings.",
            "spof": false
          },
          {
            "path": "tests/quantization/torchao_integration/test_torchao.py",
            "description": "This file contains unit tests for the integration of `torchao` quantization with Hugging Face `transformers` models. It verifies various quantization configurations, model loading, and text generation capabilities.",
            "spof": false
          },
          {
            "path": "tests/quantization/quark_integration/test_quark.py",
            "description": "This file contains unit tests for the integration of AMD's Quark quantization library within the Hugging Face Transformers library. It verifies the functionality, memory footprint, layer type conversion, and generation quality of Quark-quantized large language models on GPUs.",
            "spof": false
          },
          {
            "path": "tests/quantization/spqr_integration/test_spqr.py",
            "description": "This file contains integration tests for the SPQR quantization method within the Hugging Face Transformers library. It verifies the functionality of `SpQRConfig` and `AutoModelForCausalLM` with SPQR quantized models, including model conversion, text generation, multi-GPU support, and `torch.compile` integration.",
            "spof": true
          },
          {
            "path": "tests/quantization/eetq_integration/test_eetq.py",
            "description": "This file contains integration tests for the Eetq (Efficient Transformer Quantization) feature in the Hugging Face Transformers library. It verifies the functionality of Eetq configuration, quantized model conversion, inference, saving/loading, and multi-GPU support.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/aqlm.py",
            "description": "This file provides integration for AQLM (Additive Quantization of Language Model) within the Hugging Face Transformers library. It includes a function to replace standard `nn.Linear` layers in a model with AQLM quantized linear layers.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/awq.py",
            "description": "This file provides integration for AWQ (Activation aware Weight Quantization) within the Hugging Face Transformers library. It defines methods to replace linear layers with AWQ quantized versions and handle activation scaling for various model architectures.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/spqr.py",
            "description": "This file provides integration for Sparse-Quantized Representation (SpQR) quantization within the Hugging Face Transformers library. It includes functionality to replace standard linear layers with SPQR quantized layers.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/eetq.py",
            "description": "This file implements classes and functions for integrating EETQ (Efficient and Effective Quantization) into Hugging Face Transformers models. It provides mechanisms to quantize `torch.nn.Linear` layers to INT8 for more efficient inference.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/bitnet.py",
            "description": "This file provides utilities and modules for implementing BitNet-style quantization, including functions for packing/unpacking 2-bit ternary weights and classes for quantized linear layers and activation/weight quantization with Straight-Through Estimator (STE).",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/higgs.py",
            "description": "This file provides the integration of the HIGGS quantization method within the Transformers library, utilizing the FLUTE (Flexible Lookup Table Engine) for LUT-quantized Large Language Models. It includes utilities for data preparation and handling Hadamard transforms.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/vptq.py",
            "description": "This file provides an integration for Vector Post-Training Quantization (VPTQ) into the Hugging Face Transformers library. It defines a function to replace standard `nn.Linear` layers with VPTQ-quantized `VQuantLinear` layers within a given model.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/hqq.py",
            "description": "This file provides utility functions for integrating Half-Quadratic Quantization (HQQ) into Hugging Face Transformers models, specifically preparing `nn.Linear` layers for HQQ quantization.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/quanto.py",
            "description": "This file provides utilities for integrating `optimum.quanto` quantization into Hugging Face Transformers models, including functions to replace standard layers with Quanto quantized layers and manage the quantization process during model loading.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/bitsandbytes.py",
            "description": "This file provides utilities for integrating BitsAndBytes (bnb) quantization (4-bit and 8-bit) into Hugging Face Transformers models, including classes for quantization/deserialization and a function to replace linear layers with bnb-specific modules.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/fp_quant.py",
            "description": "This file provides integration for FP-Quant (Floating Point Quantization) within the Hugging Face Transformers library. It defines classes for quantizing and deserializing model weights using the FP-Quant scheme, and a utility function to adapt quantization configurations.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/finegrained_fp8.py",
            "description": "This file implements fine-grained FP8 (8-bit floating point) quantization and matrix multiplication operations, utilizing both CUTLASS kernels (if available) and Triton-based kernels for optimized model inference.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/fbgemm_fp8.py",
            "description": "This file implements custom modules and operations for integrating FBGEMM FP8 quantization into Hugging Face Transformers models, providing efficient 8-bit floating-point linear layers and Mixture of Experts (MoE) functionality.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/quark.py",
            "description": "This file defines a `QuarkDeserialize` class for converting quantization parameter keys to a format compatible with the Quark integration within the Hugging Face Transformers library, facilitating the deserialization of quantized model states.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/torchao.py",
            "description": "This file implements the integration of the torchao library into Hugging Face Transformers, providing functionalities for quantizing model weights and deserializing torchao-quantized tensors during model loading.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/mxfp4.py",
            "description": "This file implements classes and utilities for quantizing, dequantizing, and managing model weights in the MXFP4 (mixed-precision FP4) format, integrating with Triton kernels for efficient low-precision operations within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_auto_round.py",
            "description": "This file defines the `AutoRoundQuantizer` class, which implements the AutoRound quantization method for Hugging Face models. It handles the pre-quantization processing and validation required for models that have been quantized using AutoRound.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_fp_quant.py",
            "description": "This file defines the `FPQuantHfQuantizer` class, which implements the FP-Quant method for quantizing models within the Hugging Face Transformers library. It supports loading pre-quantized models and in-flight quantization of full-precision models, primarily for GPU and Intel XPU devices.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_bitnet.py",
            "description": "This file defines the `BitNetHfQuantizer` class, which implements 1.58-bit quantization for models using the BitNet method, including converting linear layers to BitLinear and validating the environment for BitNet quantized models.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "description": "This file implements the `Bnb4BitHfQuantizer` class, which enables 4-bit quantization for Hugging Face models using the `bitsandbytes` library. It handles environment validation, module conversion, memory management, and dequantization for 4-bit specific model loading and processing.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_awq.py",
            "description": "This file implements the `AwqQuantizer` class, enabling 4-bit Activation-aware Weight Quantization (AWQ) for models within the Hugging Face Transformers library. It handles AWQ-specific environment validation, model processing, and configuration.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_higgs.py",
            "description": "This file defines the `HiggsHfQuantizer` class, which implements the HIGGS quantization method for Hugging Face models, handling environment validation, model preparation, and weight repacking for HIGGS-specific operations.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "description": "This file defines the `Bnb8BitHfQuantizer` class, which implements 8-bit quantization using the `bitsandbytes` library for Hugging Face Transformers models. It handles environment validation, model preparation, and serialization for 8-bit quantized models.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/auto.py",
            "description": "This file provides automatic selection and instantiation of quantization configurations and their corresponding quantizers for various quantization methods (e.g., AWQ, GPTQ, BitsAndBytes). It serves as a central dispatcher for quantization-related functionalities within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_spqr.py",
            "description": "This file defines the `SpQRHfQuantizer` class, which implements the SpQR quantization method for Hugging Face Transformers models. It handles the preparation and loading of models quantized with SpQR, ensuring proper environment setup and module replacement.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/base.py",
            "description": "This file defines the abstract base class `HfQuantizer` and related utilities for handling quantization of HuggingFace Transformers models, providing methods for model preparation, parameter handling, and dequantization.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "description": "This file implements the `FineGrainedFP8HfQuantizer` class, which provides a Hugging Face quantizer for fine-grained FP8 quantization of models. It includes logic for environment validation, model processing, and weight conversions specific to FP8.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_aqlm.py",
            "description": "This file implements the AQLM (Adaptive Quantization for Language Models) method within the Hugging Face Transformers library. It provides the `AqlmHfQuantizer` class for loading and preparing models with AQLM quantization, including environment validation and training support checks.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_eetq.py",
            "description": "This file defines the `EetqHfQuantizer` class, which implements 8-bit quantization using the EETQ method for models within the Hugging Face Transformers library. It handles environment validation, module conversion, and quantization operations specific to EETQ.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_gptq.py",
            "description": "This file implements the Hugging Face `HfQuantizer` for the GPTQ quantization method, allowing models to be quantized using the GPTQ technique, leveraging the `optimum` and `gptqmodel` libraries. It handles the integration and processing of models for GPTQ quantization within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_vptq.py",
            "description": "This file implements the Hugging Face quantizer for the VPTQ (Very Post Training Quantization) method, enabling the loading and handling of models pre-quantized using VPTQ. It integrates VPTQ into the Transformers library by replacing linear layers with VPTQ-compatible ones.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_mxfp4.py",
            "description": "This file defines the `Mxfp4HfQuantizer` class, which implements mixed-precision FP4 quantization for models within the Hugging Face Transformers library. It handles the quantization process, including environmental validation, kernel integration, and state dict manipulation for MXFP4-quantized models.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_torchao.py",
            "description": "This file implements a Hugging Face quantizer using the `torchao` library, enabling quantization functionalities for models within the Hugging Face ecosystem. It handles environment validation, state dict processing, memory management, and serialization specific to `torchao` quantization.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_hqq.py",
            "description": "This file implements the Hugging Face quantizer for Half-Quadratic Quantization (HQQ), enabling HQQ integration with `transformers` models. It handles HQQ-specific setup, compatibility, and quantization of linear layers.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_quanto.py",
            "description": "This file defines `QuantoHfQuantizer`, a class that integrates the `quanto` library for model quantization within the Hugging Face Transformers framework. It handles environment validation, applies quanto-specific quantization logic, and processes models for quantization.",
            "spof": true
          },
          {
            "path": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "description": "This file implements a Hugging Face quantizer (`CompressedTensorsHfQuantizer`) for models, leveraging the `compressed_tensors` library. It manages the quantization, compression, and decompression of models, and supports Quantization-Aware Training (QAT).",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizers_utils.py",
            "description": "This file provides utility functions for navigating module hierarchies and determining whether specific modules or tensors should be included or excluded from a quantization process based on name patterns.",
            "spof": true
          },
          {
            "path": "src/transformers/quantizers/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `transformers.quantizers` package, exposing core classes like `AutoHfQuantizer` and `HfQuantizer`, along with utility functions for automatic quantization configuration and registration.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_quark.py",
            "description": "This file implements the `QuarkHfQuantizer` class, integrating AMD's Quark quantization library into the Hugging Face Transformers framework. It manages the quantization of models using Quark and the deserialization of Quark-quantized weights.",
            "spof": false
          },
          {
            "path": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "description": "This file implements the Hugging Face quantizer for FP8 quantization using FBGEMM kernels. It includes environment validation, model transformation for FP8 compatibility, and specific tensor parallelism configurations for certain models like Llama4.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/quantization_config.py",
            "description": "This file defines common data structures and utility classes for managing quantization configurations within the Transformers library. It includes an enumeration of supported quantization methods and a base mixin class for serialization and instantiation of quantization-specific settings.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Marc Sun",
            "percent": 28
          },
          {
            "name": "Mohamed Mekkouri",
            "percent": 15
          },
          {
            "name": "Cyril Vallez",
            "percent": 8
          }
        ]
      },
      "PEFT and Fine-Tuning Optimization": {
        "files": [
          {
            "path": "docs/source/de/peft.md",
            "description": "This German-language documentation page explains how to load, use, and train Parameter-Efficient Fine Tuning (PEFT) adapters within the Hugging Face Transformers library. It covers topics like supported PEFT methods, 8-bit/4-bit loading, adding multiple adapters, and training with the `Trainer` class.",
            "spof": true
          },
          {
            "path": "docs/source/ar/peft.md",
            "description": "This document provides a guide to using Parameter-Efficient Fine-tuning (PEFT) with ü§ó Transformers, explaining how to load, manage, and train PEFT adapters. It covers installation, supported methods, quantization loading, and adapter manipulation within the Transformers framework.",
            "spof": true
          },
          {
            "path": "docs/source/en/peft.md",
            "description": "This document guides users on how to apply Parameter-Efficient Fine-Tuning (PEFT) methods, particularly LoRA, within the Hugging Face Transformers framework. It covers installation, configuration, training, saving, loading, and managing adapters, including advanced features like hotswapping.",
            "spof": false
          },
          {
            "path": "docs/source/en/community_integrations/unsloth.md",
            "description": "This file documents Unsloth, a framework for faster and more memory-efficient fine-tuning of large language models, explaining its features, showing a usage example with the Hugging Face Transformers library, and detailing its integration points.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/peft.md",
            "description": "This documentation file describes the `PeftAdapterMixin` in the `transformers` library, detailing its functions for integrating and managing PEFT (Parameter-Efficient Fine-Tuning) adapters like LoRA, IA3, and AdaLora.",
            "spof": false
          },
          {
            "path": "docs/source/zh/peft.md",
            "description": "This document provides a guide on how to load, utilize, and train Parameter-Efficient Fine-Tuning (PEFT) adapters within the Hugging Face Transformers library. It covers various aspects including supported PEFT methods, quantization, and adapter management.",
            "spof": true
          },
          {
            "path": "docs/source/ja/peft.md",
            "description": "This document provides a comprehensive guide in Japanese on how to use Parameter-Efficient Fine-Tuning (PEFT) adapters with Hugging Face Transformers, covering setup, loading, managing, and training of adapters.",
            "spof": true
          },
          {
            "path": "docs/source/ko/how_to_hack_models.md",
            "description": "This document, written in Korean, provides a guide on customizing Transformer model components, specifically demonstrating how to modify the attention mechanism to apply LoRA (Low-Rank Adaptation) to query (q) and value (v) projections.",
            "spof": true
          },
          {
            "path": "docs/source/ko/peft.md",
            "description": "This document provides a guide in Korean on how to load, use, and train PEFT (Parameter-Efficient Fine Tuning) adapters within the Hugging Face Transformers library. It covers setup, supported PEFT models, loading adapters (including 8-bit/4-bit quantization), adding/enabling/disabling adapters, and training them.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/peft.md",
            "description": "This document describes the `PeftAdapterMixin` in the Transformers library, which provides functionalities from the PEFT library for managing adapters like LoRA, IA3, and AdaLora. It details its capabilities and lists supported methods for adapter management.",
            "spof": true
          },
          {
            "path": "tests/peft_integration/test_peft_integration.py",
            "description": "This file contains integration tests to ensure the correct functionality and integration of the PEFT (Parameter-Efficient Fine-Tuning) library within the Hugging Face Transformers library, covering aspects like model loading, saving, and adapter management.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/neftune.py",
            "description": "This file implements the NEFTune (Noisy Embeddings for Fine-Tuning) method. It provides functions to activate and deactivate noise addition to a model's embedding layers during training to improve fine-tuning performance.",
            "spof": true
          },
          {
            "path": "src/transformers/integrations/peft.py",
            "description": "This file implements conversion operations and utilities for integrating PEFT (Parameter-Efficient Fine-Tuning) models with Hugging Face Transformers, specifically handling the transformation and merging of LoRA weights.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/peft_utils.py",
            "description": "This file provides utility functions for detecting and managing PEFT (Parameter-Efficient Fine-Tuning) adapter configurations and ensuring compatible PEFT library versions within the Transformers library.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Arthur",
            "percent": 34
          },
          {
            "name": "Benjamin Bossan",
            "percent": 32
          },
          {
            "name": "Steven Liu",
            "percent": 6
          }
        ]
      },
      "Single-Device and Hardware Optimization Guides": {
        "files": [
          {
            "path": "docs/source/ar/model_memory_anatomy.md",
            "description": "This document explains the memory anatomy and GPU usage during the training of Transformer models, detailing how model weights, optimizer states, gradients, and activations contribute to memory consumption. It also provides practical examples and tips for optimizing memory and training speed on a single GPU.",
            "spof": true
          },
          {
            "path": "docs/source/ar/llm_tutorial_optimization.md",
            "description": "This document is a tutorial on optimizing Large Language Models (LLMs) for improved speed and memory efficiency during deployment. It covers techniques such as lower precision (quantization), Flash Attention, and architectural innovations to address the challenges of high memory consumption and long input sequences.",
            "spof": false
          },
          {
            "path": "docs/source/en/assisted_decoding.md",
            "description": "This documentation file explains various assisted decoding methods available in the Hugging Face Transformers library, such as speculative decoding, prompt lookup decoding, self-speculative decoding, and universal assisted decoding, along with code examples for their implementation.",
            "spof": true
          },
          {
            "path": "docs/source/en/perf_hardware.md",
            "description": "This file provides practical tips and considerations for setting up a deep learning machine, focusing on GPU selection, power supply, cooling, and multi-GPU connectivity.",
            "spof": true
          },
          {
            "path": "docs/source/en/optimization_overview.md",
            "description": "This file provides an overview and practical guide to various inference optimization techniques available in the Transformers library, covering methods like compilation, quantization, attention backends, and parallelism to improve model speed and memory usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/perf_train_gaudi.md",
            "description": "This file provides documentation on how to use Intel Gaudi AI accelerators with Hugging Face Transformers for training and inference, including configuration details and performance tips.",
            "spof": false
          },
          {
            "path": "docs/source/en/llm_tutorial_optimization.md",
            "description": "This file is a tutorial explaining various techniques to optimize Large Language Models (LLMs) for improved speed and reduced memory consumption during inference. It covers topics such as lower precision, Flash Attention, and architectural innovations.",
            "spof": false
          },
          {
            "path": "docs/source/en/perf_train_special.md",
            "description": "This document provides instructions and information on how to leverage Apple Silicon (M series) for accelerating PyTorch model training, specifically using the Metal Performance Shaders (MPS) backend. It covers setup, considerations, and limitations for training on Apple devices.",
            "spof": true
          },
          {
            "path": "docs/source/en/perf_train_cpu.md",
            "description": "This documentation file provides instructions and examples for training large models efficiently on CPUs, particularly Intel CPUs, using mixed precision with PyTorch and Hugging Face Transformers' Trainer.",
            "spof": false
          },
          {
            "path": "docs/source/en/perf_train_gpu_one.md",
            "description": "This document provides a guide on efficiently training deep learning models on GPUs using various optimization features available in the Transformers library and PyTorch. It covers techniques such as batch size, gradient accumulation, gradient checkpointing, mixed precision, and different optimizers.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_memory_anatomy.md",
            "description": "This document explains the anatomy of model training and memory utilization on a GPU for Transformer models. It details how different components like model weights, optimizer states, and activations consume GPU memory during training, accompanied by practical code examples.",
            "spof": true
          },
          {
            "path": "docs/source/en/perf_torch_compile.md",
            "description": "This document explains how to use `torch.compile` to optimize Hugging Face Transformers models, detailing its parameters like modes and fullgraph, and provides performance benchmarks.",
            "spof": true
          },
          {
            "path": "docs/source/en/serve-cli/serving_optims.md",
            "description": "This file documents various server optimizations available for `transformers serve`, including continuous batching, quantization, attention backend selection, and data type configuration, to improve throughput and reduce memory usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/internal/modeling_utils.md",
            "description": "This document lists and describes the custom layers, utility functions, and classes used internally by the Transformers library for modeling, primarily serving as a reference for developers studying the model code. It covers components like `WeightConverter`, custom attention functions, and PyTorch helper utilities.",
            "spof": false
          },
          {
            "path": "docs/source/es/model_memory_anatomy.md",
            "description": "This document, in Spanish, explains the anatomy of model training, focusing on GPU memory usage and optimization techniques. It details how memory is consumed by different model components and the computational intensity of various operations during transformer training.",
            "spof": true
          },
          {
            "path": "docs/source/es/performance.md",
            "description": "This document provides guidance and resources on optimizing the performance and scalability of large transformer models. It covers strategies for both training and inference across various hardware configurations, including single/multi-GPU, CPU, and TPU setups.",
            "spof": true
          },
          {
            "path": "docs/source/it/big_models.md",
            "description": "This document, written in Italian, explains techniques for instantiating and loading very large pre-trained models, focusing on minimizing RAM usage through shared checkpoints and low-memory loading methods in the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/it/perf_train_special.md",
            "description": "This document is an incomplete Italian guide on training models using specialized hardware, referencing other guides for general training strategies.",
            "spof": true
          },
          {
            "path": "docs/source/it/perf_infer_special.md",
            "description": "This file is a placeholder Italian documentation page for inference on specialized hardware. It indicates that the full content will be added soon and currently links to a guide for CPU inference.",
            "spof": false
          },
          {
            "path": "docs/source/it/perf_train_cpu.md",
            "description": "This Italian documentation page provides guidance on efficient CPU training, focusing on the use of Intel Extension for PyTorch (IPEX) for mixed precision (BF16) training.",
            "spof": true
          },
          {
            "path": "docs/source/it/perf_hardware.md",
            "description": "This document provides practical advice and benchmarks on optimizing hardware, specifically GPUs, for deep learning model training. It covers topics like power, cooling, and multi-GPU connectivity (e.g., NVLink) to enhance performance.",
            "spof": false
          },
          {
            "path": "docs/source/it/perf_infer_cpu.md",
            "description": "This document provides a guide on efficient inference for large models on CPU, focusing on PyTorch's JIT-mode (TorchScript) and Intel¬Æ Extension for PyTorch (IPEX) graph optimizations, especially for Transformer models.",
            "spof": true
          },
          {
            "path": "docs/source/zh/perf_train_special.md",
            "description": "This document provides instructions and explanations for accelerating PyTorch training on Apple Silicon Macs using the Metal Performance Shaders (MPS) backend. It covers setup, benefits, and usage examples for configuring training on `mps` devices.",
            "spof": true
          },
          {
            "path": "docs/source/zh/perf_hardware.md",
            "description": "This document provides guidance on hardware considerations, particularly GPUs, for optimizing deep learning model training and inference. It covers aspects like power supply, cooling, and the impact of multi-GPU connectivity methods like NVLink on training performance.",
            "spof": true
          },
          {
            "path": "docs/source/zh/perf_train_cpu.md",
            "description": "This document provides a guide on performing efficient, mixed-precision training of large models on CPUs using the Intel PyTorch Extension (IPEX). It covers installation, configuration with the Hugging Face Trainer, and practical examples.",
            "spof": true
          },
          {
            "path": "docs/source/zh/perf_torch_compile.md",
            "description": "This document benchmarks the performance improvements achieved by using `torch.compile()` for inference with various computer vision models within the Hugging Face Transformers library. It provides benchmark code examples, results across different hardware and batch sizes, and visualizations.",
            "spof": true
          },
          {
            "path": "docs/source/zh/performance.md",
            "description": "This document provides guidance and solutions for optimizing the performance and scalability of large transformer models. It covers strategies for both training and inference across various hardware configurations, including single/multi-GPU, CPU, and TPU setups.",
            "spof": true
          },
          {
            "path": "docs/source/zh/main_classes/optimizer_schedules.md",
            "description": "This file documents the optimization utilities available in the Transformers library, with a particular focus on various learning rate schedules and their usage, along with information on the AdaFactor optimizer.",
            "spof": false
          },
          {
            "path": "docs/source/ja/big_models.md",
            "description": "This document explains how to efficiently load and manage very large pre-trained models, minimizing RAM usage during instantiation and loading. It introduces sharded checkpoints and refers to the Accelerate library for low-memory loading strategies.",
            "spof": false
          },
          {
            "path": "docs/source/ja/perf_infer_cpu.md",
            "description": "This document provides guidance on achieving efficient inference for large models on CPU, focusing on PyTorch JIT mode (TorchScript) and optimizations offered by Intel¬Æ Extension for PyTorch (IPEX).",
            "spof": true
          },
          {
            "path": "docs/source/ja/perf_torch_compile.md",
            "description": "This documentation file, written in Japanese, provides a guide and benchmarks on optimizing inference speed for Hugging Face Transformers computer vision models using PyTorch's `torch.compile()` feature. It includes code examples and performance comparisons across various models and hardware configurations.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_memory_anatomy.md",
            "description": "This document explains the anatomy of model training, focusing on GPU memory usage and operational breakdown during training. It provides insights into how GPU resources are utilized and how memory is consumed by different components to understand performance optimization techniques.",
            "spof": true
          },
          {
            "path": "docs/source/ja/perf_train_special.md",
            "description": "This document provides information on training models using specialized hardware. It is currently under development, with content to be added soon.",
            "spof": true
          },
          {
            "path": "docs/source/ja/perf_train_cpu.md",
            "description": "This document provides a guide on efficient training of large models on CPUs, focusing on the use of Intel Extension for PyTorch (IPEX) with mixed precision (BFloat16) for performance optimization. It includes installation instructions and examples for using IPEX with the Hugging Face Trainer.",
            "spof": true
          },
          {
            "path": "docs/source/ja/perf_infer_special.md",
            "description": "This document, written in Japanese, is intended to provide information on performing inference on specialized hardware. It notes that the full guide is still forthcoming.",
            "spof": true
          },
          {
            "path": "docs/source/ja/performance.md",
            "description": "This document provides guidance on optimizing the performance and scalability of large transformer models. It covers strategies and tools for efficient training and inference across various hardware configurations, including single/multi-GPU, CPU, and TPU.",
            "spof": true
          },
          {
            "path": "docs/source/ja/perf_hardware.md",
            "description": "This document provides guidance on selecting and configuring hardware, particularly GPUs and their interconnections (like NVLink), to optimize performance for training and inference of machine learning models. It includes practical advice on power, cooling, and benchmarks demonstrating the impact of multi-GPU connectivity.",
            "spof": true
          },
          {
            "path": "docs/source/ja/perf_train_gpu_one.md",
            "description": "This document, written in Japanese, outlines methods and tools for optimizing memory utilization and accelerating model training on a single GPU. It covers practical techniques such as batch size selection, gradient accumulation, mixed precision training, and optimizer choices to improve training efficiency, especially for large models.",
            "spof": false
          },
          {
            "path": "docs/source/ja/perf_infer_gpu_one.md",
            "description": "This document provides a guide in Japanese on optimizing transformer-based models for efficient inference on a single GPU. It details the use of Flash Attention 2 and `bitsandbytes` integration for FP4 and Int8 mixed-precision quantization.",
            "spof": false
          },
          {
            "path": "docs/source/ko/perf_hardware.md",
            "description": "This document provides guidance on selecting and configuring hardware, particularly GPUs, for optimizing model training and inference performance. It covers aspects like power supply, cooling, and the impact of multi-GPU connectivity (e.g., NVLink) on training speed.",
            "spof": true
          },
          {
            "path": "docs/source/ko/perf_train_special.md",
            "description": "This document provides guidance on how to leverage Apple Silicon's Metal Performance Shaders (MPS) for accelerated PyTorch training on Mac devices, including considerations for use with Hugging Face Transformers models.",
            "spof": true
          },
          {
            "path": "docs/source/ko/perf_train_cpu.md",
            "description": "This document provides a guide on efficient training of large models on CPU, focusing on using Intel's IPEX and mixed-precision (BFloat16) within the Hugging Face Transformers `Trainer`.",
            "spof": true
          },
          {
            "path": "docs/source/ko/llm_optims.md",
            "description": "This is a Korean documentation file for the Hugging Face Transformers library, detailing various optimization techniques for Large Language Model (LLM) inference, including static KV-cache and `torch.compile`.",
            "spof": true
          },
          {
            "path": "docs/source/ko/optimizers.md",
            "description": "This document provides a guide in Korean on how to use various optimizers with the HuggingFace Transformers library's `Trainer` and `TrainingArguments`. It covers installation and usage examples for optimizers like APOLLO, GrokAdamW, LOMO, Schedule Free, and StableAdamW.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_memory_anatomy.md",
            "description": "This document, written in Korean, explains the anatomy of model training, focusing on GPU utilization, computational operations, and memory structure. It details various components contributing to GPU memory consumption during the training of Transformer models to help users understand performance optimization.",
            "spof": true
          },
          {
            "path": "docs/source/ko/llm_tutorial_optimization.md",
            "description": "This document provides a tutorial on optimizing Large Language Models (LLMs) for improved speed and reduced memory consumption. It covers various techniques, including lower precision (quantization), flash attention, and specialized architectural innovations.",
            "spof": false
          },
          {
            "path": "docs/source/ko/perf_infer_cpu.md",
            "description": "This document provides a guide on achieving efficient inference for large models on CPUs. It specifically details how to leverage Intel¬Æ Extension for PyTorch (IPEX) for graph optimization in JIT mode to improve performance.",
            "spof": true
          },
          {
            "path": "docs/source/ko/perf_train_gpu_one.md",
            "description": "This document provides a guide in Korean on optimizing GPU training performance for deep learning models using the Hugging Face Transformers library and PyTorch, covering techniques like batch sizing, gradient accumulation, mixed precision, and more.",
            "spof": true
          },
          {
            "path": "src/transformers/convert_slow_tokenizer.py",
            "description": "This file provides utilities for converting 'slow' tokenizers into their 'fast' counterparts, consolidating SentencePiece dependencies to make them optional for the core fast tokenizers.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/__init__.py",
            "description": "This file serves as the `__init__.py` for the `integrations` package in the Transformers library. It defines a lazy-loading mechanism for various deep learning framework integrations and utilities, conditionally importing them based on dependency availability and Python version.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/chunk_utils.py",
            "description": "This file provides utility functions for chunking tensor operations, particularly to process neural network layers in a memory-efficient manner by dividing inputs into smaller sub-batches.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Steven Liu",
            "percent": 24
          },
          {
            "name": "Cyril Vallez",
            "percent": 15
          },
          {
            "name": "Arthur",
            "percent": 11
          }
        ]
      },
      "Model Export and Inference Engine Integration": {
        "files": [
          {
            "path": "docs/source/ar/gguf.md",
            "description": "This document explains the GGUF file format, its interaction with the Hugging Face `transformers` library, and provides guidance on loading and processing GGUF models within `transformers`. It also lists supported quantization types and model architectures.",
            "spof": true
          },
          {
            "path": "docs/source/ar/serialization.md",
            "description": "This document provides a guide on how to export Hugging Face Transformers models to the ONNX format using the Hugging Face Optimum library. It details both command-line interface and programmatic approaches for exporting models.",
            "spof": false
          },
          {
            "path": "docs/source/en/serialization.md",
            "description": "This document describes how to export Hugging Face Transformers models to various production formats like ExecuTorch and ONNX for optimized deployment on different devices and cloud platforms. It provides installation instructions and usage examples for the `optimum-cli` and programmatic export.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/sglang.md",
            "description": "This document explains how to use Hugging Face Transformers models as a backend for SGLang, a low-latency inference engine. It covers setup, code examples, and technical details of the integration.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/llama_cpp.md",
            "description": "This document provides a guide for integrating and deploying Hugging Face Transformers models using `llama.cpp`, a C/C++ inference engine. It covers converting models to the GGUF format, local deployment, and details the Transformers integration process.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/tensorrt-llm.md",
            "description": "This document describes how to use NVIDIA TensorRT-LLM, specifically its AutoDeploy feature, to optimize the inference of Hugging Face Transformers models on NVIDIA GPUs. It provides usage examples and explains the integration process.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/vllm.md",
            "description": "This document explains how to use vLLM, a high-throughput inference engine for LLMs, with Hugging Face Transformers models. It details the integration process, including how vLLM loads and optimizes Transformers models for efficient inference.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/executorch.md",
            "description": "This document explains how to export and run Hugging Face Transformers models using ExecuTorch, a lightweight runtime for edge devices. It provides instructions and code examples for integrating ExecuTorch with Transformers, including CLI, Python, and C++ usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/transformers_as_backend.md",
            "description": "This document provides a guide on implementing Transformers models to serve as compatible backends for inference engines like vLLM and SGLang. It details the necessary steps for model implementation, including custom attention functions, tensor/pipeline parallelism, and considerations for multimodal models.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/executorch.md",
            "description": "This documentation page introduces ExecuTorch, a PyTorch solution for on-device inference on edge devices, and details its integration with Hugging Face Transformers. It explains how to export Transformer models using `torch.export` for efficient deployment with ExecuTorch.",
            "spof": true
          },
          {
            "path": "docs/source/zh/serialization.md",
            "description": "This document, written in Chinese, provides a guide on how to export Hugging Face Transformers models to the ONNX format using the `ü§ó Optimum` library, covering both command-line interface (CLI) and programmatic methods.",
            "spof": false
          },
          {
            "path": "docs/source/ja/serialization.md",
            "description": "This document explains how to export Hugging Face Transformers models to the ONNX format for deployment, using the ü§ó Optimum library. It covers both command-line interface (CLI) and programmatic methods for exporting models.",
            "spof": true
          },
          {
            "path": "docs/source/ko/gguf.md",
            "description": "This document explains the interaction between the GGUF file format and Hugging Face Transformers. It details how to load GGUF models into Transformers for further processing and lists supported quantization types and model architectures.",
            "spof": true
          },
          {
            "path": "docs/source/ko/executorch.md",
            "description": "This file provides a Korean-language explanation of ExecuTorch, a solution for on-device inference on mobile and edge devices, and details its integration with Hugging Face Transformers for optimized model deployment.",
            "spof": false
          },
          {
            "path": "docs/source/ko/serialization.md",
            "description": "This document explains how to export Hugging Face Transformers models to the ONNX format using the ü§ó Optimum library. It details both command-line interface and programmatic methods for exporting models and highlights the benefits of using ONNX for deployment and optimization.",
            "spof": true
          },
          {
            "path": "tests/test_executorch.py",
            "description": "This file contains unit tests for the Hugging Face Transformers integration with Executorch. It verifies the functionality and export capabilities of transformer models for Executorch, including different caching strategies (static and hybrid).",
            "spof": true
          },
          {
            "path": "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py",
            "description": "This script converts 'slow' tokenizer checkpoints from the Hugging Face Transformers library into their 'fast' (Rust-based `tokenizers` library) serialization format. It can process specified tokenizers and checkpoints or convert all available ones from AWS.",
            "spof": false
          },
          {
            "path": "src/transformers/safetensors_conversion.py",
            "description": "This file provides utilities for automatically converting models on the Hugging Face Hub to the `safetensors` format, including checking for existing conversions and initiating new ones via pull requests.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/tiktoken.py",
            "description": "This file provides a utility function to convert a `tiktoken` tokenizer encoding into a `PretrainedTokenizerFast` format, saving its configuration to disk. It handles the process of extracting `tiktoken`'s BPE data and converting it for use with Hugging Face's fast tokenizers.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/mistral.py",
            "description": "This file provides utilities for converting a 'tekken' tokenizer, specifically for Mistral models, into a Hugging Face `PreTrainedTokenizerFast` object using the `tokenizers` library.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/executorch.py",
            "description": "This file provides wrapper classes and utilities for exporting Hugging Face Transformers models, including Vision-Language Models and decoder-only Large Language Models with caching, to the ExecuTorch format for deployment.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/ggml.py",
            "description": "This file provides an integration with GGML/GGUF, adapting code from pygguf to map Hugging Face Transformers model configurations to GGUF parameters for various architectures.",
            "spof": false
          },
          {
            "path": "src/transformers/integrations/integration_utils.py",
            "description": "This file provides utility functions for integrating with various external machine learning and hyperparameter optimization libraries, including checking their availability and facilitating their use within the Transformers framework.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Steven Liu",
            "percent": 20
          },
          {
            "name": "Guang Yang",
            "percent": 10
          },
          {
            "name": "Anton Vlasjuk",
            "percent": 9
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 306,
      "spofCount": 148
    },
    "busFactor": 14,
    "authorCount": 104
  },
  "Developer Enablement Resources": {
    "description": "A comprehensive set of resources including extensive documentation, tutorials, and ready-to-use example scripts that guide users in applying and customizing models for a wide variety of real-world use cases.",
    "functions": {
      "Documentation Localization and Translation": {
        "files": [
          {
            "path": "i18n/README_hd.md",
            "description": "This file is the Hindi (hd) translation of the Hugging Face Transformers library's README. It includes guidelines for English-Hindi translation specific to Hugging Face documentation and introduces the library's features in Hindi.",
            "spof": false
          },
          {
            "path": "i18n/README_es.md",
            "description": "This file is the Spanish-language README for the Hugging Face Transformers library, introducing its features, capabilities, and supported models for various machine learning tasks across different modalities.",
            "spof": false
          },
          {
            "path": "i18n/README_de.md",
            "description": "This file is the German translation of the README for the Hugging Face Transformers library. It introduces the library's capabilities for state-of-the-art machine learning models across text, image, and audio modalities, and highlights its integration with JAX, PyTorch, and TensorFlow.",
            "spof": false
          },
          {
            "path": "i18n/README_zh-hant.md",
            "description": "This file is the Traditional Chinese (ÁπÅÈ´î‰∏≠Êñá) README for the Hugging Face Transformers library. It provides an overview, installation instructions, quick-start guides, and reasons to use or not use the library.",
            "spof": true
          },
          {
            "path": "i18n/README_ur.md",
            "description": "This file is the Urdu translation of the Hugging Face Transformers library's main README. It provides an overview of the library, its capabilities across different modalities (text, vision, audio), supported deep learning frameworks, and links to online demos.",
            "spof": true
          },
          {
            "path": "i18n/README_pt-br.md",
            "description": "This file is the Brazilian Portuguese (pt-br) localized README for the Hugging Face Transformers library, providing an overview of its capabilities, supported modalities (text, vision, audio), and online demonstrations.",
            "spof": false
          },
          {
            "path": "i18n/README_vi.md",
            "description": "This file is the Vietnamese translation of the Hugging Face Transformers library README, providing an overview of its capabilities, models, and supported modalities.",
            "spof": false
          },
          {
            "path": "i18n/README_te.md",
            "description": "This file is the Telugu translation of the README for the Hugging Face Transformers library. It introduces the library, its capabilities in various modalities (text, vision, audio), and provides information on its API and available models.",
            "spof": false
          },
          {
            "path": "i18n/README_ar.md",
            "description": "This file is the Arabic translation of the README for the HuggingFace Transformers library. It provides an overview of the library's features, supported models, and online demos in Arabic.",
            "spof": true
          },
          {
            "path": "i18n/README_bn.md",
            "description": "This file is the Bengali (bn) translation of the README for the Hugging Face Transformers library. It provides an overview, installation instructions, and quickstart examples for using the library's `pipeline` API for various NLP, vision, and audio tasks.",
            "spof": true
          },
          {
            "path": "i18n/README_ru.md",
            "description": "This file is the Russian translation of the README for the Hugging Face Transformers library, providing an overview of its capabilities and features for modern machine learning.",
            "spof": false
          },
          {
            "path": "i18n/README_ko.md",
            "description": "This file is the Korean translation of the Hugging Face Transformers library's README. It provides an overview of the library's capabilities, supported models for various AI tasks (text, vision, audio, multimodal), and online demos.",
            "spof": true
          },
          {
            "path": "i18n/README_zh-hans.md",
            "description": "This file is the Simplified Chinese translation of the main README for the Hugging Face Transformers library. It provides an overview of the library, installation instructions, and a quick-start guide, along with translation guidelines.",
            "spof": true
          },
          {
            "path": "i18n/README_fr.md",
            "description": "This file is the French localization of the README for the Hugging Face Transformers library. It provides an overview of the library's capabilities for various AI tasks across text, vision, and audio modalities.",
            "spof": false
          },
          {
            "path": "i18n/README_it.md",
            "description": "This file is the Italian language README for the Hugging Face Transformers library, providing an overview, installation instructions, and quickstart examples for using pre-trained models.",
            "spof": true
          },
          {
            "path": "i18n/README_ja.md",
            "description": "This file is the Japanese translation of the main README for the Hugging Face Transformers library. It provides an introduction, installation instructions, and quick-start examples for the library in Japanese.",
            "spof": true
          },
          {
            "path": "docs/TRANSLATING.md",
            "description": "This document provides instructions for translating the Transformers library documentation into other languages, covering steps from opening an issue to forking the repository, copying files, and beginning the translation process.",
            "spof": true
          },
          {
            "path": "docs/source/de/add_new_model.md",
            "description": "This document provides a comprehensive guide in German on how to add a new PyTorch model to the Hugging Face Transformers library, detailing design principles, code style, and a step-by-step process for contributors.",
            "spof": false
          },
          {
            "path": "docs/source/de/installation.md",
            "description": "This file provides comprehensive instructions for installing and setting up the Hugging Face Transformers library in German. It covers various installation methods (pip, editable, conda), cache configuration, and details on using the library in offline mode.",
            "spof": true
          },
          {
            "path": "docs/source/de/preprocessing.md",
            "description": "This document, written in German, provides a guide on data preprocessing for Hugging Face Transformers models. It covers tokenization, padding, truncation, and creating tensors for text data, and briefly introduces feature extraction for audio and image data.",
            "spof": true
          },
          {
            "path": "docs/source/de/contributing.md",
            "description": "This document is a German-language guide for contributing to the Hugging Face Transformers library. It outlines various ways to contribute, from fixing bugs and adding new features to improving documentation, and provides instructions for creating issues and pull requests.",
            "spof": false
          },
          {
            "path": "docs/source/de/run_scripts.md",
            "description": "This document, written in German, provides instructions and guidance on how to run example training scripts for Hugging Face Transformers models, covering setup, execution, distributed training, mixed precision, and TPU usage.",
            "spof": false
          },
          {
            "path": "docs/source/de/training.md",
            "description": "This document is a German-language tutorial explaining how to fine-tune pre-trained Transformers models using Hugging Face's Trainer, TensorFlow/Keras, and native PyTorch.",
            "spof": true
          },
          {
            "path": "docs/source/de/quicktour.md",
            "description": "This file is a German-language quickstart guide for the Hugging Face Transformers library. It introduces users to basic functionalities like `pipeline` for inference and `AutoClass` for loading models and tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/de/model_sharing.md",
            "description": "This document provides a German-language tutorial on how to share trained or fine-tuned models on the Hugging Face Model Hub, covering programmatic uploads, web interface uploads, model conversion for different frameworks, and adding model cards.",
            "spof": true
          },
          {
            "path": "docs/source/de/index.md",
            "description": "This file serves as the main index page for the German documentation of the Hugging Face Transformers library. It introduces the library's capabilities, supported models, and outlines the structure of the documentation.",
            "spof": true
          },
          {
            "path": "docs/source/de/autoclass_tutorial.md",
            "description": "This tutorial explains how to use Hugging Face's `AutoClass` methods (e.g., AutoTokenizer, AutoModel, AutoProcessor) to automatically load pre-trained models and preprocessing tools from various checkpoints. It demonstrates loading components for NLP, audio, and image processing tasks.",
            "spof": true
          },
          {
            "path": "docs/source/de/testing.md",
            "description": "This document, written in German, explains how to test Hugging Face Transformers models, including how to run existing tests, select specific tests, and write new ones. It covers various pytest functionalities for test execution and management.",
            "spof": true
          },
          {
            "path": "docs/source/ar/glossary.md",
            "description": "This file is an Arabic glossary that defines common machine learning terms and terms specific to the Hugging Face Transformers library to help users better understand the documentation.",
            "spof": true
          },
          {
            "path": "docs/source/ar/how_to_hack_models.md",
            "description": "This document provides a guide in Arabic on how to modify Hugging Face Transformers models, demonstrating how to customize the attention mechanism of the Segment Anything (SAM) model to apply LoRA effectively.",
            "spof": true
          },
          {
            "path": "docs/source/ar/community.md",
            "description": "This file provides an Arabic-language list of community resources and notebooks for ü§ó Transformers, detailing various fine-tuning and training tasks for different Transformer models.",
            "spof": true
          },
          {
            "path": "docs/source/ar/notebooks.md",
            "description": "This document provides an Arabic-language list of official Hugging Face Transformers notebooks, including documentation and various PyTorch and TensorFlow examples for different tasks. It also invites community contributions to the list.",
            "spof": true
          },
          {
            "path": "docs/source/ar/fast_tokenizers.md",
            "description": "This document, written in Arabic, provides instructions on how to use tokenizers created with the ü§ó Tokenizers library within the ü§ó Transformers framework. It demonstrates how to load fast tokenizers directly from a tokenizer object or from a saved JSON file into `PreTrainedTokenizerFast`.",
            "spof": true
          },
          {
            "path": "docs/source/ar/create_a_model.md",
            "description": "This Arabic documentation file explains how to create custom model components, including configurations, models, tokenizers, and image processors, within the Hugging Face Transformers library, offering more detailed control than the `AutoClass`.",
            "spof": false
          },
          {
            "path": "docs/source/ar/model_summary.md",
            "description": "This document provides an overview of various Transformer models, categorizing them by their architecture (encoder, decoder, encoder-decoder) and application domains such as Computer Vision, Natural Language Processing, and Audio.",
            "spof": true
          },
          {
            "path": "docs/source/ar/model_sharing.md",
            "description": "This document provides instructions in Arabic on how to share trained or fine-tuned models on the Hugging Face Model Hub, covering both programmatic and web interface methods, along with essential setup and best practices like adding a model card.",
            "spof": false
          },
          {
            "path": "docs/source/ar/autoclass_tutorial.md",
            "description": "This file is an Arabic tutorial explaining how to use the `AutoClass` functionality in the Hugging Face Transformers library. It demonstrates how to automatically load pre-trained models, tokenizers, image processors, feature extractors, and processors for various NLP, vision, and audio tasks.",
            "spof": false
          },
          {
            "path": "docs/source/ar/sagemaker.md",
            "description": "This file, written in Arabic, announces the migration of SageMaker-related documentation to a new location on hf.co/docs/sagemaker and provides links to specific topics like training and inference.",
            "spof": true
          },
          {
            "path": "docs/source/ar/_config.py",
            "description": "This configuration file defines installation instructions for Transformers and related libraries, intended for use in Jupyter notebooks within the Arabic documentation. It also specifies patterns to be avoided by the code formatter Black.",
            "spof": true
          },
          {
            "path": "docs/source/ar/index.md",
            "description": "This file serves as the main overview and index page for the Arabic documentation of the Hugging Face Transformers library. It introduces the library's capabilities, supported tasks, framework compatibility, and outlines the documentation structure.",
            "spof": true
          },
          {
            "path": "docs/source/ar/philosophy.md",
            "description": "This document outlines the philosophy and core design principles of the Hugging Face Transformers library, detailing its target audience, key objectives, and fundamental architectural concepts in Arabic.",
            "spof": true
          },
          {
            "path": "docs/source/ar/quicktour.md",
            "description": "This file provides a quick tour of the Hugging Face Transformers library in Arabic, demonstrating how to use pipelines for inference and AutoClasses for loading models and tokenizers.",
            "spof": false
          },
          {
            "path": "docs/source/ar/installation.md",
            "description": "This document provides comprehensive instructions in Arabic for installing the Hugging Face Transformers library using various methods (pip, source, editable, conda) and configuring its cache. It also details how to use the library in an offline environment.",
            "spof": true
          },
          {
            "path": "docs/source/ar/custom_models.md",
            "description": "This documentation file, written in Arabic, provides a tutorial on how to build and integrate custom models and configurations into the Hugging Face Transformers library. It demonstrates the process using a ResNet model as an example.",
            "spof": true
          },
          {
            "path": "docs/source/ar/task_summary.md",
            "description": "This Arabic markdown file provides an overview of various machine learning tasks (audio, computer vision, and natural language processing) that can be accomplished using the Hugging Face Transformers library. It includes explanations and code examples for each task.",
            "spof": false
          },
          {
            "path": "docs/source/ar/tasks_explained.md",
            "description": "This document explains how various ü§ó Transformers models tackle different tasks in natural language processing, speech/audio, and computer vision. It delves into the internal workings and architectures of specific models like Wav2Vec2, ViT, and ConvNeXT to clarify their problem-solving mechanisms.",
            "spof": true
          },
          {
            "path": "docs/source/ar/llm_tutorial.md",
            "description": "This file is an Arabic tutorial that explains how to perform text generation using Large Language Models (LLMs) with Hugging Face Transformers. It covers basic usage of the `generate` function, common pitfalls like output length and generation strategies, and how to address them.",
            "spof": true
          },
          {
            "path": "docs/source/ar/preprocessing.md",
            "description": "This document provides a tutorial on data preprocessing for Hugging Face Transformers models, covering text tokenization, audio feature extraction, and image processing. It explains concepts like padding, truncation, and converting inputs into tensors for models.",
            "spof": true
          },
          {
            "path": "docs/source/ar/troubleshooting.md",
            "description": "This document is a troubleshooting guide for the Hugging Face Transformers library, providing solutions to common issues encountered by users. It covers problems such as connection errors, CUDA out-of-memory, TensorFlow model loading, ImportErrors, and incorrect outputs related to padding tokens.",
            "spof": true
          },
          {
            "path": "docs/source/es/glossary.md",
            "description": "This file provides a glossary of machine learning and Hugging Face Transformers-related terms in Spanish, intended to help users understand the documentation better.",
            "spof": true
          },
          {
            "path": "docs/source/es/model_sharing.md",
            "description": "This document, written in Spanish, provides a tutorial on how to share trained or fine-tuned models on the Hugging Face Model Hub. It covers methods such as using code with `Trainer` or `PushToHubCallback`, converting models between PyTorch, TensorFlow, and Flax, and utilizing the web interface.",
            "spof": false
          },
          {
            "path": "docs/source/es/community.md",
            "description": "This file is a Spanish-language community page for the Hugging Face Transformers library. It lists various community-contributed resources and notebooks related to the library.",
            "spof": false
          },
          {
            "path": "docs/source/es/_config.py",
            "description": "This file defines configuration variables for documentation and notebooks, including installation instructions for the Transformers library and placeholders for code formatting.",
            "spof": false
          },
          {
            "path": "docs/source/es/sagemaker.md",
            "description": "This document, written in Spanish, provides information about running training on Amazon SageMaker, noting that the content has been moved to a new location and will be removed in a future version.",
            "spof": true
          },
          {
            "path": "docs/source/es/custom_models.md",
            "description": "This documentation file in Spanish explains how to create, configure, and share custom models within the Hugging Face Transformers library. It provides a step-by-step guide on writing custom configurations and models, using ResNet as an example, and how to push them to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "docs/source/es/autoclass_tutorial.md",
            "description": "This tutorial explains how to use the Hugging Face Transformers `AutoClass` to automatically load pre-trained models, tokenizers, feature extractors, and processors. It demonstrates how `AutoClass` simplifies the process of inferring and loading the correct architecture from a given checkpoint.",
            "spof": true
          },
          {
            "path": "docs/source/es/installation.md",
            "description": "This document provides a comprehensive guide in Spanish for installing Hugging Face Transformers, including pip, source, editable, and conda installations. It also covers configuring the cache directory and enabling offline mode for the library.",
            "spof": true
          },
          {
            "path": "docs/source/es/create_a_model.md",
            "description": "This documentation file, written in Spanish, provides a guide on how to create custom model architectures, configurations, and tokenizers within the Hugging Face Transformers library. It explains how to customize various aspects of models, offering more granular control than the default AutoClass methods.",
            "spof": true
          },
          {
            "path": "docs/source/es/index.md",
            "description": "This file is the Spanish-language index page for the Hugging Face Transformers documentation, introducing the library's capabilities for machine learning with PyTorch, TensorFlow, and JAX, and listing supported models and documentation sections.",
            "spof": true
          },
          {
            "path": "docs/source/es/quicktour.md",
            "description": "This file is a quick tour or getting started guide for the Hugging Face Transformers library in Spanish. It introduces core features like the `pipeline` for easy model inference and `AutoClass` for loading models and tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/es/bertology.md",
            "description": "This document, titled 'BERTolog√≠a' (BERTology), introduces the field of studying the internal workings of large transformers like BERT. It references key research papers and describes features added to BERT/GPT/GPT-2 models within the Hugging Face Transformers library to facilitate this research, such as accessing hidden states and attention weights.",
            "spof": true
          },
          {
            "path": "docs/source/es/run_scripts.md",
            "description": "This document is a Spanish guide detailing how to run and configure example training scripts for Hugging Face Transformers models. It covers setup, execution for tasks like summarization, distributed training, mixed-precision training, and usage with TPUs and Hugging Face Accelerate.",
            "spof": false
          },
          {
            "path": "docs/source/es/task_summary.md",
            "description": "This file provides an overview of various audio, computer vision, and natural language processing tasks that can be solved using the Hugging Face Transformers library, including explanations and code examples. It serves as an introductory guide to the library's capabilities in Spanish.",
            "spof": false
          },
          {
            "path": "docs/source/es/tokenizer_summary.md",
            "description": "This document provides an overview of tokenization, explaining various subword tokenization algorithms like BPE, WordPiece, and SentencePiece used in Hugging Face Transformers. It details how these methods address the challenges of word and character-level tokenization for natural language processing models.",
            "spof": true
          },
          {
            "path": "docs/source/es/tasks_explained.md",
            "description": "This document explains in Spanish how various Hugging Face Transformers models solve tasks in natural language processing, speech, and computer vision. It details the internal workings of specific models like Wav2Vec2, ViT, and BERT for different applications.",
            "spof": true
          },
          {
            "path": "docs/source/es/tasks/multiple_choice.md",
            "description": "This document provides a guide in Spanish on how to fine-tune a BERT model for multiple-choice tasks using the SWAG dataset, covering data loading, preprocessing, and training steps.",
            "spof": false
          },
          {
            "path": "docs/source/fr/autoclass_tutorial.md",
            "description": "This tutorial in French explains how to use `AutoClass` and `from_pretrained()` functions in the Hugging Face Transformers library to automatically load pre-trained tokenizers, image processors, feature extractors, processors, and models for different tasks.",
            "spof": false
          },
          {
            "path": "docs/source/fr/run_scripts_fr.md",
            "description": "This document provides instructions in French on how to run training scripts using Hugging Face Transformers, covering setup, execution, and advanced topics like distributed training, mixed precision, and TPU usage.",
            "spof": false
          },
          {
            "path": "docs/source/fr/installation.md",
            "description": "This document provides comprehensive installation instructions for the Hugging Face Transformers library in French, covering various methods (pip, source, conda), cache configuration, and offline usage.",
            "spof": false
          },
          {
            "path": "docs/source/fr/quicktour.md",
            "description": "This file is a French-language quick tour guide for the Hugging Face Transformers library, demonstrating how to use the pipeline for inference, load pre-trained models, and prepare for training.",
            "spof": true
          },
          {
            "path": "docs/source/fr/_config.py",
            "description": "This file configures installation instructions for French-language documentation notebooks in the Transformers library. It also defines patterns to be avoided by code formatting tools.",
            "spof": false
          },
          {
            "path": "docs/source/fr/index.md",
            "description": "This file serves as the main French index page for the Hugging Face Transformers documentation, providing an overview of the library, its capabilities, and links to various documentation sections and supported models.",
            "spof": true
          },
          {
            "path": "docs/source/fr/task_summary.md",
            "description": "This document, written in French, provides an overview of the tasks that the Hugging Face Transformers library can perform across audio, computer vision, and natural language processing modalities. It details various applications like audio classification, automatic speech recognition, image classification, object detection, and image segmentation, often with code examples.",
            "spof": true
          },
          {
            "path": "docs/source/fr/tasks_explained.md",
            "description": "This document explains how various models within the Hugging Face Transformers library address specific tasks in natural language processing, audio, and computer vision, detailing their architectures and underlying mechanisms. It serves as a guide to understanding the technical approaches behind these models.",
            "spof": true
          },
          {
            "path": "docs/source/fr/in_translation.md",
            "description": "This file serves as a placeholder indicating that a French documentation page is currently under translation. It informs readers that the content is not yet finalized.",
            "spof": true
          },
          {
            "path": "docs/source/it/custom_models.md",
            "description": "This Italian documentation page explains how to create and share custom models and configurations within the Hugging Face Transformers library, using a ResNet example to demonstrate the process.",
            "spof": true
          },
          {
            "path": "docs/source/it/autoclass_tutorial.md",
            "description": "This Italian tutorial explains how to use `AutoClass` in the Hugging Face Transformers library to automatically infer and load pre-trained tokenizers, feature extractors, processors, and models for various tasks.",
            "spof": true
          },
          {
            "path": "docs/source/it/pipeline_tutorial.md",
            "description": "This file is an Italian tutorial explaining how to use the Hugging Face `pipeline` for inference across different modalities, including text generation, audio classification, and image classification.",
            "spof": true
          },
          {
            "path": "docs/source/it/index.md",
            "description": "This file is the Italian language main index page for the Hugging Face Transformers library documentation. It provides an overview of the library's capabilities, supported models, and documentation structure.",
            "spof": true
          },
          {
            "path": "docs/source/it/add_new_model.md",
            "description": "This document, written in Italian, provides a step-by-step guide and overview for contributing a new model to the Hugging Face Transformers library, detailing design principles, code style, and best practices.",
            "spof": true
          },
          {
            "path": "docs/source/it/preprocessing.md",
            "description": "This document, written in Italian, explains how to preprocess text and audio data for use with Hugging Face Transformer models. It covers tokenization, padding, truncation for text, and introduces feature extraction for audio.",
            "spof": true
          },
          {
            "path": "docs/source/it/installation.md",
            "description": "This document provides comprehensive instructions for installing the Hugging Face Transformers library using various methods like pip, source, editable, and conda. It also details how to configure the library's cache and enable offline mode for pre-trained models and tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/it/community.md",
            "description": "This file is an Italian-language community page for Hugging Face Transformers, listing various external resources and Colab notebooks created by the community. It includes links to flashcards and tutorials for fine-tuning and training different Transformer models for various NLP tasks.",
            "spof": false
          },
          {
            "path": "docs/source/it/quicktour.md",
            "description": "This file is an Italian quick tour guide for the Hugging Face Transformers library, explaining how to use the `pipeline` for various tasks and the `AutoClass` for loading models and tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/it/_config.py",
            "description": "This configuration file for the Italian documentation of the Hugging Face Transformers library defines common installation instructions for notebooks and specifies patterns to be handled or ignored in code examples.",
            "spof": false
          },
          {
            "path": "docs/source/pt/create_a_model.md",
            "description": "This documentation file, written in Portuguese, explains how to create custom model architectures, configurations, and tokenizers within the Hugging Face Transformers library, detailing steps for loading, customizing, and saving these components.",
            "spof": true
          },
          {
            "path": "docs/source/pt/fast_tokenizers.md",
            "description": "This document explains how to use fast tokenizers from the ü§ó Tokenizers library within ü§ó Transformers. It covers creating a tokenizer, then loading it directly from a tokenizer object or from a saved JSON file using `PreTrainedTokenizerFast`.",
            "spof": false
          },
          {
            "path": "docs/source/pt/custom_models.md",
            "description": "This document provides a tutorial in Portuguese on how to create, configure, and share custom models with the Hugging Face Transformers library, using a ResNet example.",
            "spof": true
          },
          {
            "path": "docs/source/pt/quicktour.md",
            "description": "This file is a Portuguese quick tour/tutorial for the Hugging Face Transformers library. It introduces users to the `pipeline` for inference and `AutoClass` for loading models and tokenizers, providing code examples for various NLP, vision, and audio tasks.",
            "spof": true
          },
          {
            "path": "docs/source/pt/index.md",
            "description": "This file serves as the main index and introduction page for the Hugging Face Transformers library documentation in Portuguese. It provides an overview of the library's capabilities, supported models, and outlines the documentation structure.",
            "spof": true
          },
          {
            "path": "docs/source/pt/_config.py",
            "description": "This file contains configuration settings for the `transformers` documentation, including installation commands for Jupyter notebooks and patterns to be ignored by the `black` formatter.",
            "spof": false
          },
          {
            "path": "docs/source/zh/autoclass_tutorial.md",
            "description": "This tutorial in Chinese explains how to use `AutoClass` in Hugging Face Transformers to automatically load pre-trained instances such as tokenizers, image processors, feature extractors, processors, and models from checkpoints.",
            "spof": false
          },
          {
            "path": "docs/source/zh/create_a_model.md",
            "description": "This document provides a guide on how to create and customize Hugging Face Transformers models, configurations, tokenizers, image processors, feature extractors, and multimodal processors in Chinese, without relying on `AutoClass`.",
            "spof": false
          },
          {
            "path": "docs/source/zh/community.md",
            "description": "This document serves as a central hub for community-contributed resources and educational notebooks related to the Hugging Face Transformers library. It lists various tools and Colab notebooks demonstrating tasks like fine-tuning, training, and optimizing Transformer models.",
            "spof": true
          },
          {
            "path": "docs/source/zh/model_sharing.md",
            "description": "This file is a Chinese-language tutorial explaining how to share trained or fine-tuned models on the Hugging Face Model Hub. It covers both programmatic methods using the `push_to_hub` function and manual uploads via the web interface, along with instructions for setting up credentials and adding model cards.",
            "spof": true
          },
          {
            "path": "docs/source/zh/philosophy.md",
            "description": "This document outlines the design philosophy, key goals, and core concepts of the Hugging Face Transformers library. It explains how the library is built for various users and its two main objectives: simplicity and providing state-of-the-art models.",
            "spof": false
          },
          {
            "path": "docs/source/zh/installation.md",
            "description": "This document provides a comprehensive guide for installing the HuggingFace Transformers library in Chinese, covering various methods like pip, source, and editable installations, along with instructions for cache setup and offline usage.",
            "spof": false
          },
          {
            "path": "docs/source/zh/quicktour.md",
            "description": "This is the Chinese quick start guide for the Hugging Face Transformers library, introducing users to core functionalities like `pipeline` for inference and `AutoClass` for loading models and tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/zh/index.md",
            "description": "This file serves as the main introduction and index for the Chinese documentation of the Hugging Face Transformers library. It provides an overview of the library, its capabilities across various modalities, and outlines the structure of the documentation.",
            "spof": true
          },
          {
            "path": "docs/source/zh/gguf.md",
            "description": "This document explains the interaction between GGUF file format and Hugging Face Transformers. It details how to load GGUF models into Transformers for further training or fine-tuning, lists supported quantization types and model architectures, and provides usage examples.",
            "spof": true
          },
          {
            "path": "docs/source/zh/preprocessing.md",
            "description": "This document provides a guide to data preprocessing for Hugging Face Transformers models, covering tokenization for text, feature extraction for audio, image processing, and multimodal data handling, along with concepts like padding and truncation.",
            "spof": false
          },
          {
            "path": "docs/source/zh/run_scripts.md",
            "description": "This document provides a comprehensive guide in Chinese on how to run, configure, and customize example training scripts within the Hugging Face Transformers library, covering setup, various training methods, and model sharing.",
            "spof": false
          },
          {
            "path": "docs/source/zh/custom_models.md",
            "description": "This document guides users on how to write and integrate custom models and configurations with the Hugging Face Transformers library. It covers sharing these custom models and their associated code with the community via the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "docs/source/zh/tiktoken.md",
            "description": "This document describes how Hugging Face Transformers integrates with `tiktoken` models and tokenizers, explaining the automatic conversion of `tokenizer.model` files and providing examples for loading and manually converting `tiktoken` tokenizers to `tokenizer.json` format.",
            "spof": true
          },
          {
            "path": "docs/source/zh/task_summary.md",
            "description": "This document provides an overview of various audio, computer vision, and natural language processing tasks that can be performed using the Hugging Face Transformers library, along with code examples for each task.",
            "spof": false
          },
          {
            "path": "docs/source/zh/internal/file_utils.md",
            "description": "This document lists and describes various utility functions, enums, and decorators found in the `utils.py` file of the Transformers library, primarily for internal reference.",
            "spof": false
          },
          {
            "path": "docs/source/zh/internal/modeling_utils.md",
            "description": "This document, written in Chinese, lists and describes custom layers and utility functions used internally by the Transformers library for its models. It is primarily intended for developers studying the library's model code.",
            "spof": false
          },
          {
            "path": "docs/source/zh/main_classes/logging.md",
            "description": "This document explains how to configure and manage the logging system in the Hugging Face Transformers library. It details methods for setting logging verbosity, using environment variables to control output, and distinguishing between `logging` and `warnings` within the library.",
            "spof": true
          },
          {
            "path": "docs/source/ja/add_new_model.md",
            "description": "This document provides a comprehensive guide in Japanese on how to add a new model to the Hugging Face Transformers library. It outlines the step-by-step process, coding style, and debugging strategies for contributing a new model.",
            "spof": false
          },
          {
            "path": "docs/source/ja/create_a_model.md",
            "description": "This file is a Japanese-language guide explaining how to create custom models within the Hugging Face Transformers library. It covers customizing model configurations, architectures, tokenizers, image processors, and feature extractors.",
            "spof": false
          },
          {
            "path": "docs/source/ja/autoclass_tutorial.md",
            "description": "This tutorial explains how to use `AutoClass` in the Hugging Face Transformers library to automatically load pre-trained tokenizers, image processors, feature extractors, processors, and models for various tasks.",
            "spof": true
          },
          {
            "path": "docs/source/ja/glossary.md",
            "description": "This file is a glossary of common machine learning and Hugging Face Transformers terms, written in Japanese, to help users understand the documentation better.",
            "spof": true
          },
          {
            "path": "docs/source/ja/installation.md",
            "description": "This file provides comprehensive instructions in Japanese for installing the Hugging Face Transformers library using various methods (pip, source, editable, conda), configuring its cache, and setting up an offline mode for its usage.",
            "spof": true
          },
          {
            "path": "docs/source/ja/llm_tutorial.md",
            "description": "This document provides a tutorial on generating text with Large Language Models (LLMs) using the Hugging Face Transformers library, covering basic usage, common pitfalls, and advanced resources for optimizing LLM generation.",
            "spof": false
          },
          {
            "path": "docs/source/ja/bertology.md",
            "description": "This file introduces \"BERTology\" research, which explores the internal workings of large Transformer models. It details features added to BERT/GPT/GPT-2 within the Hugging Face library to enable researchers to access internal representations and analyze attention mechanisms, referencing a sample script for practical application.",
            "spof": true
          },
          {
            "path": "docs/source/ja/quicktour.md",
            "description": "This file is the Japanese 'Quick Tour' guide for the Hugging Face Transformers library, introducing users to key functionalities like pipelines, AutoClasses, and basic model training.",
            "spof": true
          },
          {
            "path": "docs/source/ja/custom_models.md",
            "description": "This document, written in Japanese, provides a tutorial on how to write, configure, and share custom models with the Hugging Face Transformers library. It demonstrates the process by wrapping a ResNet model from the `timm` library.",
            "spof": false
          },
          {
            "path": "docs/source/ja/index.md",
            "description": "This file is the main index page for the Japanese documentation of the Hugging Face Transformers library. It provides an introduction to the library, its capabilities across various modalities, a guide to its documentation structure, and a comprehensive list of supported models.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_summary.md",
            "description": "This document provides a comprehensive overview and categorization of the Transformer model family, detailing various architectures (encoder, decoder, encoder-decoder) and their applications across computer vision, natural language processing, audio, and multimodal tasks. It serves as a guide to understanding the diverse landscape of Transformer variants and their underlying principles.",
            "spof": false
          },
          {
            "path": "docs/source/ja/community.md",
            "description": "This Japanese documentation page lists community resources and notebooks related to Hugging Face Transformers. It provides links to various tutorials and examples for fine-tuning and using different Transformer models for a range of NLP tasks.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_sharing.md",
            "description": "This Japanese Markdown document provides a tutorial on how to share models to the Hugging Face Model Hub, covering programmatic uploads, web interface uploads, repository features, model conversion, and adding model cards.",
            "spof": true
          },
          {
            "path": "docs/source/ja/pad_truncation.md",
            "description": "This document explains how to use padding and truncation with the Hugging Face Transformers tokenizer in Japanese. It details the `padding`, `truncation`, and `max_length` arguments and provides a comprehensive table of recommended strategies.",
            "spof": true
          },
          {
            "path": "docs/source/ja/philosophy.md",
            "description": "This file outlines the core philosophy, design goals, and main concepts behind the Hugging Face Transformers library in Japanese. It explains why the library was built and its guiding principles for users and developers.",
            "spof": true
          },
          {
            "path": "docs/source/ja/troubleshooting.md",
            "description": "This document provides troubleshooting tips and solutions for common issues encountered when using Hugging Face Transformers, such as memory errors, import errors, and incorrect outputs with padding tokens. It also guides users on where to seek further help.",
            "spof": true
          },
          {
            "path": "docs/source/ja/preprocessing.md",
            "description": "This document provides a tutorial on data preprocessing for Hugging Face Transformers models, covering text, audio, and image data. It explains how to use tokenizers, feature extractors, and image processors to prepare various data types for model input.",
            "spof": false
          },
          {
            "path": "docs/source/ja/task_summary.md",
            "description": "This Japanese documentation page provides a comprehensive summary of the various AI tasks, including audio, computer vision, and natural language processing, that can be performed using the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ja/run_scripts.md",
            "description": "This document provides a Japanese guide on how to run training scripts within the Hugging Face Transformers library, covering setup, execution on various hardware, and handling custom datasets.",
            "spof": false
          },
          {
            "path": "docs/source/ja/testing.md",
            "description": "This document provides a comprehensive guide on testing the Hugging Face Transformers library, detailing how tests are structured, executed, and offering various advanced options for running, debugging, and optimizing the test suite in Japanese.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks_explained.md",
            "description": "This document explains how various models within the Hugging Face Transformers library, including both Transformer and CNN architectures, approach and solve different natural language processing, audio, and computer vision tasks. It details the internal mechanisms and specific components used by models like Wav2Vec2, ViT, DETR, and BERT for tasks such as audio classification, image classification, object detection, and text generation.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/logging.md",
            "description": "This document provides a Japanese guide on configuring and utilizing the logging system within the Hugging Face Transformers library. It explains how to set verbosity levels, disable warnings, and integrate the library's logger into custom applications.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/asr.md",
            "description": "This document provides a guide in Japanese on how to fine-tune a Wav2Vec2 model for Automatic Speech Recognition (ASR) using the MInDS-14 dataset, covering preprocessing, training, and evaluation steps.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/masked_language_modeling.md",
            "description": "This document provides a tutorial in Japanese on how to perform masked language modeling using the Hugging Face Transformers library, covering dataset preprocessing, model fine-tuning, and inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/image_to_image.md",
            "description": "This file is a Japanese language guide for performing image-to-image tasks, specifically super-resolution, using the Hugging Face Transformers library. It demonstrates how to use both the `image-to-image` pipeline and direct model/processor calls for super-resolution with the Swin2SR model.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/video_classification.md",
            "description": "This document is a Japanese guide on video classification using Hugging Face Transformers. It details how to fine-tune a VideoMAE model on the UCF101 dataset subset and use it for inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/prompting.md",
            "description": "This document is a guide to LLM prompting and prompt engineering best practices. It explains how to effectively design prompts for large language models to solve various NLP tasks, including classification, NER, translation, summarization, and question answering.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/beit.md",
            "description": "This file provides the Japanese documentation for the Hugging Face Transformers BEiT model, including its overview, usage tips, and API references for various components like `BeitConfig`, `BeitImageProcessor`, and `BeitModel`.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/auto.md",
            "description": "This document describes the Auto Classes in the Hugging Face Transformers library, explaining how they simplify loading pre-trained models, configurations, and tokenizers. It details their usage, extensibility, and lists various AutoModel classes for different machine learning tasks, categorized by domain (NLP, Computer Vision, Audio, Multimodal, Time Series).",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/chinese_clip.md",
            "description": "This document provides an overview, usage examples, and API references for the Chinese-CLIP model within the Hugging Face Transformers library, written in Japanese.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/deit.md",
            "description": "This file provides Japanese documentation for the DeiT (Data-efficient image Transformers) model within the Hugging Face Transformers library. It details the model's overview, key features, usage tips, and references to related classes and resources for image classification and masked image modeling.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/conditional_detr.md",
            "description": "This file provides Japanese documentation for the Conditional DETR model, including an overview, a summary of its research paper, and an API reference for its various components within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/autoformer.md",
            "description": "This file provides the Japanese documentation for the Autoformer model, detailing its architecture, purpose for long-term series forecasting, and API references within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/_config.py",
            "description": "This file contains configuration settings for documentation and notebooks within the Transformers library, including installation instructions in Korean and patterns to avoid during code formatting.",
            "spof": false
          },
          {
            "path": "docs/source/ko/conversations.md",
            "description": "This document provides a Korean-language guide on how to chat with Transformer models, covering quick start examples, advice on choosing models, and a detailed look at the underlying pipeline process.",
            "spof": true
          },
          {
            "path": "docs/source/ko/custom_models.md",
            "description": "This Korean Markdown document provides a tutorial on creating, configuring, and sharing custom models with the Hugging Face Transformers library, using a ResNet example to illustrate the process of integrating custom architectures and uploading them to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "docs/source/ko/community.md",
            "description": "This file is a Korean-language community page that lists various community-developed resources and notebooks related to Hugging Face Transformers, including guides for fine-tuning and training models for different NLP tasks.",
            "spof": true
          },
          {
            "path": "docs/source/ko/add_new_model.md",
            "description": "This document is a Korean-language guide providing step-by-step instructions and best practices for contributing new models to the Hugging Face Transformers library. It details the process from understanding the model's theoretical aspects to setting up the development environment, porting the model, and submitting a pull request.",
            "spof": false
          },
          {
            "path": "docs/source/ko/glossary.md",
            "description": "This file is a Korean glossary that defines common machine learning and Hugging Face Transformers related terms. It aims to help users better understand the documentation by providing clear definitions for key concepts.",
            "spof": true
          },
          {
            "path": "docs/source/ko/contributing.md",
            "description": "This file is the Korean language guide for contributing to the Hugging Face Transformers library, outlining various ways to contribute and the process for submitting changes.",
            "spof": false
          },
          {
            "path": "docs/source/ko/in_translation.md",
            "description": "This file serves as a placeholder or work-in-progress notice for Korean documentation, indicating that the translation is currently underway.",
            "spof": true
          },
          {
            "path": "docs/source/ko/index.md",
            "description": "This file serves as the main index for the Korean documentation of the Hugging Face Transformers library, providing an overview of the library's capabilities, supported tasks, and the structure of its documentation.",
            "spof": true
          },
          {
            "path": "docs/source/ko/models.md",
            "description": "This document provides a Korean-language guide on loading pre-trained models using the Hugging Face Transformers library, covering various loading methods, managing memory for large models, and handling different model data types.",
            "spof": true
          },
          {
            "path": "docs/source/ko/installation.md",
            "description": "This document provides comprehensive instructions for installing and configuring the Hugging Face Transformers library in Korean, covering various installation methods (pip, source, conda) as well as cache management and offline usage.",
            "spof": true
          },
          {
            "path": "docs/source/ko/testing.md",
            "description": "This document provides a comprehensive guide in Korean on how to test the Hugging Face Transformers library, including running tests, selecting specific tests, and advanced testing techniques like parallel execution and random ordering.",
            "spof": false
          },
          {
            "path": "docs/source/ko/philosophy.md",
            "description": "This file outlines the philosophy, goals, and key concepts of the Hugging Face Transformers library, explaining its design principles, ease of use, and model availability. It details the core classes (models, configurations, preprocessors) and how they interact.",
            "spof": true
          },
          {
            "path": "docs/source/ko/quicktour.md",
            "description": "This file is a Korean-language quick tour guide for the Hugging Face Transformers library, introducing users to pipelines, AutoClasses for loading models and tokenizers, and basic data preparation.",
            "spof": true
          },
          {
            "path": "docs/source/ko/troubleshooting.md",
            "description": "This document provides troubleshooting steps and solutions for common errors encountered while using the Hugging Face Transformers library, written in Korean.",
            "spof": true
          },
          {
            "path": "docs/source/ko/pr_checks.md",
            "description": "This document, written in Korean, details the various checks performed on Pull Requests (PRs) in the Hugging Face Transformers library. It covers general tests, documentation builds, code/documentation style, and repository consistency checks, along with instructions on how to debug them locally.",
            "spof": false
          },
          {
            "path": "docs/source/ko/run_scripts.md",
            "description": "This document provides instructions in Korean for running example training scripts within the Hugging Face Transformers library, including setup, execution on various hardware configurations, and using custom datasets.",
            "spof": false
          },
          {
            "path": "docs/source/ko/internal/time_series_utils.md",
            "description": "This file is a Korean documentation page listing utility functions and classes for time-series-based models within the Hugging Face Transformers library, primarily focusing on distributional output classes. It's intended for those studying or extending time-series model code.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/configuration.md",
            "description": "This file is a Korean documentation page explaining the `PreTrainedConfig` class in the Hugging Face Transformers library. It describes how configurations are loaded and saved, and lists common attributes of configuration classes.",
            "spof": false
          },
          {
            "path": "docs/source/ko/main_classes/model.md",
            "description": "This document provides Korean documentation for the `PreTrainedModel` class and related utility mixins within the Transformers library. It details how models are loaded, saved, and common functionalities implemented across different framework models.",
            "spof": false
          },
          {
            "path": "docs/source/ko/main_classes/logging.md",
            "description": "This document provides a guide to configuring and using the logging system in the Hugging Face Transformers library, explaining how to set verbosity levels and integrate with Python's standard logging and warnings modules. It is the Korean translation of the logging documentation.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/tokenizer.md",
            "description": "This document provides a comprehensive guide to tokenizers within the Hugging Face Transformers library, explaining their types (Python vs. 'Fast' Rust-based), core functionalities, and key classes like PreTrainedTokenizer and PreTrainedTokenizerFast, specifically for Korean-speaking users.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/autoformer.md",
            "description": "This file is the Korean documentation for the Autoformer model within the Hugging Face Transformers library, providing an overview, resources, and API references for its configuration and models.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/patchtsmixer.md",
            "description": "This file provides a Korean-language documentation for the `PatchTSMixer` model within the Hugging Face Transformers library, detailing its overview, usage examples, and available classes for various time-series tasks.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/informer.md",
            "description": "This file provides Korean-language documentation for the Informer model within the Hugging Face Transformers library. It includes an overview of the model, its key features, and references to its API components.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/mamba.md",
            "description": "This file provides Korean documentation for the Mamba model within the Hugging Face Transformers library, including an overview, technical details, and usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/timesformer.md",
            "description": "This file is a Korean documentation page for the TimeSformer model within the Hugging Face Transformers library, providing an overview, usage tips, and API references for its configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/gemma2.md",
            "description": "This file provides the Korean-language documentation for the Gemma2 model within the Hugging Face Transformers library, including an overview and references to its configuration and various model classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/time_series_transformer.md",
            "description": "This document provides an overview and usage guide for the Time Series Transformer model in Korean. It details the model's architecture, specific usage tips for prediction, and references to its configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/vit.md",
            "description": "This file provides Korean-language documentation for the Vision Transformer (ViT) model, including an overview of its architecture, key concepts, usage tips, and API references for its configuration, image processor, and various model classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/jamba.md",
            "description": "This document provides a Korean-language guide to the Jamba model within the Hugging Face Transformers library, detailing its architecture, usage examples for text generation and quantization, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/patchtst.md",
            "description": "This file provides Korean-language documentation for the PatchTST model within the Hugging Face Transformers library. It includes an overview of the model, its architecture, usage tips, and API references for various PatchTST classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/audio_classification.md",
            "description": "This document provides a guide in Korean on how to perform audio classification using Hugging Face Transformers, covering fine-tuning a Wav2Vec2 model and performing inference for speaker intent classification.",
            "spof": false
          },
          {
            "path": "docs/source/ko/tasks/image_to_image.md",
            "description": "This document is a Korean-language guide demonstrating how to perform Image-to-Image tasks, specifically image super-resolution, using the Hugging Face Transformers library. It covers both pipeline usage and direct model inference for the Swin2SR model.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/image_feature_extraction.md",
            "description": "This document provides a guide in Korean on how to perform image feature extraction using the Hugging Face Transformers library. It demonstrates building an image similarity system with the `image-feature-extraction` pipeline and `AutoModel`.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/monocular_depth_estimation.md",
            "description": "This file is a Korean-language Markdown document providing a guide on monocular depth estimation using Hugging Face Transformers. It covers both pipeline usage and manual inference for depth estimation.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/translation.md",
            "description": "This document is a Korean-language guide explaining how to fine-tune a T5 model for English-to-French translation using the Transformers library, covering data loading, preprocessing, evaluation, training, and inference steps.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/multiple_choice.md",
            "description": "This document provides a guide on how to fine-tune a Transformers model (specifically BERT) for multiple-choice question answering tasks using the SWAG dataset. It covers data loading, preprocessing, model training, evaluation, and inference steps.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Yuanyuan Chen",
            "percent": 23
          },
          {
            "name": "Ahmed Almaghz",
            "percent": 10
          },
          {
            "name": "Quentin Gallou√©dec",
            "percent": 10
          }
        ]
      },
      "Developer Experience and Contribution Tooling": {
        "files": [
          {
            "path": "utils/download_glue_data.py",
            "description": "This script downloads and prepares various datasets for the GLUE benchmark, including specific handling for MRPC and diagnostic tasks. It allows users to specify which tasks to download and where to save the data.",
            "spof": true
          },
          {
            "path": "utils/test_module/custom_tokenization.py",
            "description": "This file defines a `CustomTokenizer` class, which inherits from `BertTokenizer`, likely serving as a placeholder or example for extending Hugging Face's tokenization functionalities within a test module.",
            "spof": true
          },
          {
            "path": "utils/test_module/custom_processing.py",
            "description": "This file defines a `CustomProcessor` class, inheriting from `ProcessorMixin`, likely serving as an example or test for implementing custom data processing pipelines within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "examples/run_on_remote.py",
            "description": "This script facilitates running Hugging Face Transformers examples on a remote computing cluster using the Runhouse library. It allows users to configure and provision either a BYO (Bring Your Own) or on-demand cluster, install necessary dependencies, and then execute a specified Transformers example script remotely.",
            "spof": false
          },
          {
            "path": "examples/modular-transformers/README.md",
            "description": "This README describes the `modular_converter` linter for the `transformers` library, which converts modular Python model files into single-file models. It explains its usage, functionality, and why `libcst` is used for parsing.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_my_new_model.py",
            "description": "This file defines a custom configuration class, `MyNewModelConfig`, that extends the `LlamaConfig` to introduce new parameters and potentially modify existing ones for a modular transformer model, demonstrating how to customize model configurations in `transformers`.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_from_uppercase_model.py",
            "description": "This file defines a subclass of `CLIPEncoderLayer` named `FromUppercaseModelEncoderLayer`. Its purpose is to test or demonstrate dependency handling for models where the original model name might have been in uppercase.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/configuration_new_model.py",
            "description": "This file defines the `NewModelConfig` class, which inherits from `PreTrainedConfig` and is used to configure the architecture of a `NewModelModel`. It specifies various architectural parameters for the model.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_test_detr.py",
            "description": "This file defines core components for a TestDetr model, including deformable multi-scale attention, data structures for model outputs, and a custom frozen batch normalization layer. It is an auto-generated file, replicated from `modular_test_detr.py`.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_dummy_bert.py",
            "description": "This file defines a `DummyBertModel` class, which inherits from `BertModel` and primarily serves as a passthrough for its `forward` method, forwarding all arguments directly to the parent class. It likely functions as a placeholder or a minimal implementation for testing or modular design within the Transformers library.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_my_new_model2.py",
            "description": "This file defines example classes for a new model, `MyNewModel2`, demonstrating how to reuse and extend existing Hugging Face Transformers components like `LlamaConfig` and `GemmaForSequenceClassification`.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "description": "This file defines core PyTorch modules for a transformer model, including multi-head attention, a multi-layer perceptron (MLP), and an encoder layer, for a model named 'FromUppercaseModel'.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_test_suffix.py",
            "description": "This file defines test or placeholder decoder layers, including one that extends `LlamaDecoderLayer`, likely for modular testing or development within the transformers library, adhering to specific naming conventions involving suffixes.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_super.py",
            "description": "This file defines fundamental modular components for a 'Super' transformer model, including RMS normalization, rotary positional embeddings, a multi-layer perceptron (MLP), and multi-headed attention mechanisms. It is automatically generated from `modular_super.py` and should not be edited directly.",
            "spof": false
          },
          {
            "path": "examples/metrics-monitoring/metrics_example.py",
            "description": "This file provides example usage of the `attach_tracer` and `traced` decorators from `transformers.utils.metrics` to demonstrate how to instrument classes, methods, and functions for OpenTelemetry tracing within the Transformers library.",
            "spof": true
          },
          {
            "path": "examples/metrics-monitoring/README.md",
            "description": "This README provides instructions and code examples for setting up and integrating metrics monitoring, specifically for continuous batching in the Transformers library, using OpenTelemetry, Tempo, and Prometheus.",
            "spof": true
          },
          {
            "path": "examples/research_projects/README.md",
            "description": "This README.md informs users that research projects previously located in this directory have been moved to a separate Hugging Face repository.",
            "spof": true
          },
          {
            "path": "examples/pytorch/test_pytorch_examples.py",
            "description": "This file contains unit tests for various PyTorch example scripts within the Hugging Face Transformers library. It verifies that the examples run correctly and produce expected results for different NLP tasks like text classification, language modeling, Q&A, and more.",
            "spof": false
          },
          {
            "path": "docs/README.md",
            "description": "This file provides comprehensive instructions and guidelines for generating, building, previewing, and contributing to the `transformers` library's documentation.",
            "spof": false
          },
          {
            "path": "docs/source/ar/modular_transformers.md",
            "description": "This document introduces the concept of \"Modular Transformers\" in the Hugging Face `transformers` library, explaining how it aims to reduce code duplication and simplify model contributions by allowing inheritance and shared components. It details how a linter will convert these modular definitions into the existing \"one model, one file\" structure.",
            "spof": true
          },
          {
            "path": "docs/source/en/auto_docstring.md",
            "description": "This document explains how to use the `@auto_docstring` decorator in the Transformers library to automatically generate and manage consistent docstrings for model classes and methods. It covers usage for both classes and functions, including how to define and override argument descriptions.",
            "spof": false
          },
          {
            "path": "docs/source/en/_config.py",
            "description": "This file configures documentation parameters, including installation commands for notebooks and patterns to avoid during code formatting for the Transformers library documentation.",
            "spof": false
          },
          {
            "path": "docs/source/en/serve-cli/openweb_ui.md",
            "description": "This document explains how to integrate Open WebUI with the `transformers serve` command, focusing on enabling CORS and configuring speech-to-text functionality for local models.",
            "spof": true
          },
          {
            "path": "docs/source/en/serve-cli/jan.md",
            "description": "This documentation file describes how to configure and use Jan, an open-source ChatGPT alternative, with the `transformers serve` CLI.",
            "spof": true
          },
          {
            "path": "docs/source/en/serve-cli/cursor.md",
            "description": "This document provides instructions on how to integrate and use local AI models with the Cursor AI-powered code editor. It details steps for setting up `transformers serve` with CORS, creating a public tunnel with ngrok, and configuring Cursor settings to connect to the local server.",
            "spof": true
          },
          {
            "path": "docs/source/en/internal/import_utils.md",
            "description": "This document explains the import utilities in the Hugging Face `transformers` library, detailing how it manages soft dependencies and enables lazy, fast object imports through filename-based and explicit dependency specification methods.",
            "spof": true
          },
          {
            "path": "docs/source/ko/modular_transformers.md",
            "description": "This document explains the concept of 'Modular Transformers' in the `transformers` library, a new system designed to reduce code duplication and simplify model contributions by allowing inheritance and modularity in model definitions, while maintaining a single-file structure for end-users.",
            "spof": true
          },
          {
            "path": "tests/utils/test_auto_docstring.py",
            "description": "This file contains tests for the `auto_docstring` utility within the Hugging Face Transformers library. It defines example docstrings for Llama models to verify correct parsing, formatting, or auto-generation behavior.",
            "spof": true
          },
          {
            "path": "tests/utils/test_doc_samples.py",
            "description": "This file contains unit tests to run doctests on code examples found within various modules of the `transformers` library and its documentation files.",
            "spof": false
          },
          {
            "path": "src/transformers/cli/__init__.py",
            "description": "This `__init__.py` file marks the `cli` directory as a Python package. It likely serves as an entry point for command-line interface utilities within the Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/cli/add_new_model_like.py",
            "description": "This file provides a CLI utility to add a new model to the Transformers library, by templating off an existing model's structure and updating relevant auto-mapping configurations. It includes tools for parsing Python code, managing file content, and updating model metadata.",
            "spof": false
          },
          {
            "path": "src/transformers/cli/add_fast_image_processor.py",
            "description": "This file provides a CLI tool for adding a new 'fast image processor' to a specific model within the Hugging Face Transformers library. It automates the process of generating the new processor file and updating relevant module initialization files, auto-configuration, documentation, and tests.",
            "spof": true
          },
          {
            "path": "src/transformers/cli/system.py",
            "description": "Provides command-line interface (CLI) commands for the `transformers` library to display environment information and the library's version. This is primarily used for debugging and support purposes, gathering details about installed packages and system setup.",
            "spof": true
          },
          {
            "path": "src/transformers/cli/chat.py",
            "description": "This file implements a rich command-line interface (CLI) for interacting with and chatting with a language model. It provides functionalities to manage conversations, configure generation settings, and save chat history.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/attention_visualizer.py",
            "description": "This file provides utilities and a class (`AttentionMaskVisualizer`) for visualizing the attention masks generated by Hugging Face Transformers models. It includes functionality to render attention matrices with token-level details, supporting features like image tokens and sliding window attention.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/auto_docstring.py",
            "description": "This file defines reusable argument descriptions and other metadata used to automatically generate docstrings for various components like image processors, video processors, tokenizers, and models within the Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/byt5/convert_byt5_original_tf_checkpoint_to_pytorch.py",
            "description": "This script converts an original TensorFlow T5 model checkpoint to a PyTorch compatible format. It loads weights from the TensorFlow checkpoint and saves them into a new PyTorch model.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 41
          },
          {
            "name": "Yoni Gozlan",
            "percent": 14
          },
          {
            "name": "Arthur",
            "percent": 12
          }
        ]
      },
      "Task-Specific Example Scripts": {
        "files": [
          {
            "path": "examples/README.md",
            "description": "This README provides an introduction and guide to the example scripts within the Hugging Face Transformers library, detailing how to run them, adapt them, and access versions for older releases or remote execution.",
            "spof": false
          },
          {
            "path": "examples/modular-transformers/modular_new_model.py",
            "description": "This file defines a custom configuration class, `NewModelConfig`, that inherits from `GemmaConfig`. It overrides the default initialization parameters of the Gemma model configuration and adds a `num_heads` property, likely for a specialized or modular transformer example.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_new_task_model.py",
            "description": "This file defines a `NewTaskModelForNewTask` class that extends `PaliGemmaForConditionalGeneration`. It introduces a custom text projection layer to process the base model's hidden states, generating specialized embeddings for new tasks.",
            "spof": false
          },
          {
            "path": "examples/pytorch/README.md",
            "description": "This README provides an overview and guide to the actively maintained PyTorch examples for various machine learning tasks using Hugging Face Transformers. It details each example, its features, and instructions for running tests, resuming training, and uploading models to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "examples/pytorch/audio-classification/run_audio_classification.py",
            "description": "This script is an example from the Hugging Face Transformers library for fine-tuning a pre-trained Transformer model for audio classification. It handles dataset loading, model configuration, and training using the Trainer API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/semantic-segmentation/README.md",
            "description": "This README provides instructions and examples for fine-tuning semantic segmentation models using PyTorch, specifically showcasing the use of Hugging Face's Trainer API and Accelerate library. It also guides users on how to prepare custom datasets for training and perform inference.",
            "spof": false
          },
          {
            "path": "examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py",
            "description": "This script provides a manual training loop for finetuning Hugging Face Transformers models for semantic segmentation without using the `Trainer` class. It supports features like argument parsing, distributed training with Accelerate, and pushing models to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "examples/pytorch/contrastive-image-text/README.md",
            "description": "This README provides an example of how to train a CLIP-like vision-text dual encoder model using pre-trained vision and text encoders for tasks like natural language image search. It includes instructions for data setup, model creation, and running the training script.",
            "spof": false
          },
          {
            "path": "examples/pytorch/language-modeling/run_fim.py",
            "description": "This script is designed for fine-tuning causal language models using the Fill-in-the-Middle (FIM) objective, applying FIM transformations with configurable rates for prefix, middle, and suffix tokens. It supports training on text files or datasets with various model and data arguments.",
            "spof": false
          },
          {
            "path": "examples/pytorch/language-modeling/run_fim_no_trainer.py",
            "description": "This script fine-tunes Causal Language Models (CLM) using the Fill-in-the-Middle (FIM) objective on text files or datasets. It provides a way to train models without relying on the Hugging Face Trainer API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/language-modeling/run_clm.py",
            "description": "This script is designed for fine-tuning causal language models (like GPT, GPT-2) on a text file or a specified dataset. It handles configuration, data loading, preprocessing, and training for language modeling tasks.",
            "spof": false
          },
          {
            "path": "examples/pytorch/language-modeling/run_mlm.py",
            "description": "This script is designed for fine-tuning Hugging Face library models (like BERT, ALBERT, RoBERTa) for masked language modeling (MLM) tasks using text files or datasets.",
            "spof": false
          },
          {
            "path": "examples/pytorch/language-modeling/run_mlm_no_trainer.py",
            "description": "This script fine-tunes Masked Language Models (like BERT, ALBERT, RoBERTa) from the HuggingFace Transformers library on a text file or dataset, implementing the training loop manually without using the HuggingFace Trainer API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/language-modeling/README.md",
            "description": "This README provides instructions and examples for fine-tuning and training various language models (GPT, BERT, RoBERTa, XLNet) using different language modeling objectives (Causal, Masked, Permutation, Fill-in-the-middle) within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "examples/pytorch/language-modeling/run_plm.py",
            "description": "This script is designed for fine-tuning pre-trained transformer models, specifically for permutation language modeling (PLM) tasks. It handles arguments for model configuration, data loading, and training via the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/README.md",
            "description": "This README provides instructions and examples for fine-tuning Hugging Face Transformers models for question answering tasks on datasets like SQuAD, utilizing either the Trainer API or `Accelerate` for various model architectures such as BERT, XLNet, and T5.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/run_qa.py",
            "description": "This script is used for fine-tuning pre-trained Transformer models for question answering tasks. It handles data loading, preprocessing, model training, and evaluation using Hugging Face's `transformers` and `datasets` libraries.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/run_qa_no_trainer.py",
            "description": "This script fine-tunes a Hugging Face Transformers model for question answering tasks using the Accelerate library, providing an alternative to the `Trainer` API for training and evaluation.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py",
            "description": "This script fine-unes an XLNet model for question answering using beam search. It leverages the Hugging Face Accelerate library for distributed training without using the high-level `Trainer` API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/image-pretraining/README.md",
            "description": "This README provides instructions and scripts for pre-training Transformer-based vision models (like ViT, Swin) using PyTorch, specifically implementing SimMIM and MAE methods. It guides users on how to use Hugging Face datasets or their own data for pre-training.",
            "spof": false
          },
          {
            "path": "examples/pytorch/image-pretraining/run_mim_no_trainer.py",
            "description": "This script pre-trains a HuggingFace Transformers model for Masked Image Modeling (SimMIM) using PyTorch, without utilizing the `Trainer` class. It leverages the Accelerate library for distributed training and provides extensive command-line arguments for configuration.",
            "spof": false
          },
          {
            "path": "examples/pytorch/image-pretraining/run_mim.py",
            "description": "This script pre-trains a Hugging Face Transformers model for simple masked image modeling (SimMIM) using PyTorch. It handles dataset loading, model configuration, and training arguments for image pre-training tasks.",
            "spof": false
          },
          {
            "path": "examples/pytorch/image-pretraining/run_mae.py",
            "description": "This script is designed for pre-training a Vision Transformer (ViT) model as a Masked Autoencoder (MAE) using the Hugging Face Transformers library. It handles data loading, model configuration, and training arguments for the MAE pre-training task.",
            "spof": false
          },
          {
            "path": "examples/pytorch/instance-segmentation/README.md",
            "description": "This README provides examples for fine-tuning MaskFormer and Mask2Former models for instance segmentation using PyTorch, demonstrating both the Hugging Face Trainer API and Accelerate. It also includes instructions for inference and preparing custom datasets.",
            "spof": true
          },
          {
            "path": "examples/pytorch/instance-segmentation/run_instance_segmentation_no_trainer.py",
            "description": "This Python script finetunes a Hugging Face Transformers model for instance segmentation tasks using the Accelerate library, providing a custom training loop instead of the standard Trainer API.",
            "spof": true
          },
          {
            "path": "examples/pytorch/instance-segmentation/run_instance_segmentation.py",
            "description": "This script demonstrates how to fine-tune a HuggingFace Transformers model for instance segmentation using the Trainer API, including data preprocessing, augmentation, and evaluation with metrics like Mean Average Precision.",
            "spof": false
          },
          {
            "path": "examples/pytorch/multiple-choice/README.md",
            "description": "This README provides instructions and examples for fine-tuning models for multiple-choice tasks, specifically on the SWAG dataset, using Hugging Face Transformers. It covers both the `Trainer` API and a custom training loop with `Accelerate`.",
            "spof": false
          },
          {
            "path": "examples/pytorch/multiple-choice/run_swag_no_trainer.py",
            "description": "This script fine-tunes a ü§ó Transformers model on a multiple-choice task, such as SWAG, utilizing the Accelerate library without relying on the `Trainer` class. It supports various configurations for datasets, models, and training parameters.",
            "spof": false
          },
          {
            "path": "examples/pytorch/multiple-choice/run_swag.py",
            "description": "This script fine-tunes pre-trained models from the Hugging Face Transformers library for multiple-choice tasks, particularly demonstrating usage with the SWAG dataset. It handles model, tokenizer, and data loading, argument parsing, and logging for training and evaluation.",
            "spof": false
          },
          {
            "path": "examples/pytorch/image-classification/run_image_classification.py",
            "description": "This script facilitates fine-tuning a Hugging Face Transformers model for image classification, handling data loading, model configuration, and training arguments.",
            "spof": false
          },
          {
            "path": "examples/pytorch/image-classification/README.md",
            "description": "This README provides examples and instructions for fine-tuning image classification models using PyTorch scripts within the Transformers library. It covers using both the Hugging Face Trainer API and a custom training loop with Accelerate, supporting datasets from the Hugging Face Hub or custom local data.",
            "spof": false
          },
          {
            "path": "examples/pytorch/image-classification/run_image_classification_no_trainer.py",
            "description": "This script fine-tunes any Hugging Face Transformers model for image classification tasks. It leverages the Accelerate library for distributed training without relying on the Trainer API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/translation/run_translation.py",
            "description": "This script is designed for fine-tuning Hugging Face Transformers models for sequence-to-sequence tasks, specifically machine translation. It handles data loading, model configuration, and training/evaluation for translation benchmarks.",
            "spof": false
          },
          {
            "path": "examples/pytorch/translation/README.md",
            "description": "This README details how to finetune and evaluate transformer models for translation tasks, providing usage examples for `run_translation.py` with both the Hugging Face Trainer and Accelerate libraries.",
            "spof": false
          },
          {
            "path": "examples/pytorch/text-classification/README.md",
            "description": "This README provides instructions and examples for fine-tuning Hugging Face models for various text classification tasks, including GLUE benchmarks, general single/multi-label classification, and cross-lingual text classification (XNLI). It details how to use different scripts, dataset configurations, and mixed precision training.",
            "spof": false
          },
          {
            "path": "examples/pytorch/text-classification/run_glue.py",
            "description": "This script is designed for fine-tuning Transformer models on sequence classification tasks, particularly focusing on the GLUE benchmark. It handles data loading, model configuration, and training with various customizable arguments.",
            "spof": false
          },
          {
            "path": "examples/pytorch/text-classification/run_classification.py",
            "description": "This script is designed for finetuning Hugging Face Transformers models for text classification tasks. It provides extensive command-line arguments for configuring data loading, preprocessing, model selection, and training parameters.",
            "spof": false
          },
          {
            "path": "examples/pytorch/text-classification/run_glue_no_trainer.py",
            "description": "This script finetunes a Hugging Face Transformers model for sequence classification tasks on the GLUE benchmark using PyTorch and the Accelerate library, without using the Hugging Face Trainer API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/summarization/run_summarization_no_trainer.py",
            "description": "This script facilitates fine-tuning a Hugging Face Transformers model for summarization tasks, leveraging the Accelerate library for distributed training without using the Trainer API. It supports various datasets and model configurations for sequence-to-sequence summarization.",
            "spof": false
          },
          {
            "path": "examples/pytorch/summarization/README.md",
            "description": "This README provides instructions and examples for fine-tuning and evaluating various transformer models on summarization tasks. It covers usage with both the Hugging Face Trainer API and the Accelerate library, as well as handling custom CSV and JSONLINES datasets.",
            "spof": false
          },
          {
            "path": "examples/pytorch/token-classification/run_ner_no_trainer.py",
            "description": "This script fine-tunes a Hugging Face Transformers model for token classification tasks (such as NER, POS, or CHUNKS) using the Accelerate library, without relying on the built-in Trainer API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/token-classification/README.md",
            "description": "This README provides instructions and examples for fine-tuning HuggingFace models for token classification tasks (e.g., Named Entity Recognition, Parts-of-speech tagging) using PyTorch. It details usage with both the Trainer API and a more customizable version leveraging the Accelerate library.",
            "spof": false
          },
          {
            "path": "examples/pytorch/token-classification/run_ner.py",
            "description": "This script is designed for fine-tuning pre-trained models from the Hugging Face Transformers library for token classification tasks, such as Named Entity Recognition (NER), using PyTorch. It handles argument parsing, data loading, model configuration, and training setup.",
            "spof": false
          },
          {
            "path": "examples/pytorch/speech-pretraining/README.md",
            "description": "This README provides instructions and examples for pre-training Wav2Vec2 models for speech recognition, covering various model sizes and offering guidance on hyperparameters, stability, and data preprocessing.",
            "spof": false
          },
          {
            "path": "examples/training",
            "description": "This directory is intended to contain examples and scripts related to the training of models using the `transformers` library. It serves as a repository for demonstrating various training configurations and tasks within the `examples` section.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/summarization.md",
            "description": "This document serves as a guide for performing summarization tasks using the Hugging Face Transformers library. It specifically outlines how to finetune a T5 model for abstractive summarization on the BillSum dataset and use it for inference.",
            "spof": true
          },
          {
            "path": "docs/source/it/run_scripts.md",
            "description": "This document provides instructions on how to install and run example training scripts for Hugging Face Transformers models using PyTorch, TensorFlow, or JAX/Flax. It covers setup, execution, and advanced configurations like distributed training and TPU usage.",
            "spof": false
          },
          {
            "path": "docs/source/pt/run_scripts.md",
            "description": "This document provides a guide on how to set up and run example training scripts for the Hugging Face Transformers library, covering PyTorch, TensorFlow, and JAX/Flax, including distributed training and TPU usage.",
            "spof": false
          },
          {
            "path": "notebooks/README.md",
            "description": "This README.md file serves as a directory for official Hugging Face Transformers notebooks and community-contributed notebooks. It categorizes them by documentation, PyTorch examples, and various NLP tasks, providing descriptions and links to open them in different environments like Colab or AWS Studio.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 20
          },
          {
            "name": "Marc Sun",
            "percent": 10
          },
          {
            "name": "Pavel Iakubovskii",
            "percent": 7
          }
        ]
      },
      "Core Library Documentation and Tutorials": {
        "files": [
          {
            "path": "docs/source/de/pr_checks.md",
            "description": "This document explains the various automated checks performed on pull requests in the Hugging Face Transformers repository, detailing different types of checks and how to debug them locally.",
            "spof": false
          },
          {
            "path": "docs/source/de/_config.py",
            "description": "This file configures installation instructions for the Transformers library and related tools, intended for use in documentation notebooks. It also defines patterns to be avoided by a code formatter.",
            "spof": false
          },
          {
            "path": "docs/source/ar/run_scripts.md",
            "description": "This document provides instructions on how to use and run training scripts with the Hugging Face Transformers library. It covers setup, different training environments (PyTorch, TensorFlow, JAX/Flax), distributed training, mixed precision, TPUs, and custom datasets.",
            "spof": false
          },
          {
            "path": "docs/source/ar/bertology.md",
            "description": "This document, titled \"BERTology\", introduces the field of studying the internal mechanisms of large transformer models like BERT. It highlights relevant research papers and explains how the Hugging Face Transformers library provides features to access internal representations (hidden states, attention weights) of BERT/GPT/GPT-2 models for analytical purposes, referencing a Python example.",
            "spof": false
          },
          {
            "path": "docs/source/en/gguf.md",
            "description": "This document describes the GGUF file format for models used with GGML and explains how to load and save GGUF models using the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/add_new_model.md",
            "description": "This document provides a guide for contributing new models to the Hugging Face Transformers library using the legacy approach. It outlines the process, best practices, code style, and development environment setup for adding a new model.",
            "spof": true
          },
          {
            "path": "docs/source/en/index.md",
            "description": "This file is the main introductory page for the Hugging Face Transformers documentation, providing an overview of the library's purpose, features, design principles, and learning resources.",
            "spof": false
          },
          {
            "path": "docs/source/en/how_to_hack_models.md",
            "description": "This document provides a guide on customizing Hugging Face Transformers models, specifically demonstrating how to modify model components like attention mechanisms and apply Low-Rank Adaptation (LoRA) to tailor models for specific use cases.",
            "spof": false
          },
          {
            "path": "docs/source/en/conversations.md",
            "description": "This document provides a guide on how to use and optimize chat models within the Hugging Face Transformers library, covering command-line interface usage, programmatic interaction with TextGenerationPipeline, and performance considerations.",
            "spof": false
          },
          {
            "path": "docs/source/en/chat_extras.md",
            "description": "This document explains how to use 'tool use' (function calling) with chat models in the Transformers library. It covers defining tools, passing them to models, and processing tool calls and responses.",
            "spof": false
          },
          {
            "path": "docs/source/en/installation.md",
            "description": "This document provides comprehensive instructions for installing the Hugging Face Transformers library using various methods like `uv`, `pip`, `conda`, and from source, along with setup configurations for cache management and offline usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/community.md",
            "description": "This file serves as a community hub for Hugging Face Transformers, providing a curated list of community-contributed resources and interactive notebooks. It aims to showcase how the community uses and extends the Transformers library through various projects and tutorials.",
            "spof": false
          },
          {
            "path": "docs/source/en/custom_models.md",
            "description": "This document provides a guide on how to customize Transformer models, detailing the process of defining custom configurations, building model classes, enabling AutoClass support, and sharing them on the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "docs/source/en/pr_checks.md",
            "description": "This document explains the different checks (tests, documentation, code style, repository consistency) performed on Pull Requests in the Hugging Face Transformers library, and how to debug them locally.",
            "spof": false
          },
          {
            "path": "docs/source/en/fast_tokenizers.md",
            "description": "This documentation file provides a comprehensive guide to tokenizers in the Hugging Face Transformers library, explaining their purpose, different implementations (Python-based and fast Rust-based), loading mechanisms, and advanced features like training custom tokenizers and integrating with `tiktoken`.",
            "spof": true
          },
          {
            "path": "docs/source/en/glossary.md",
            "description": "This file provides a glossary of general machine learning and Hugging Face Transformers terms to help users understand the documentation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_sharing.md",
            "description": "This document provides a comprehensive guide on how to share models from the Transformers library to the Hugging Face Hub, covering setup, repository features, various upload methods, and creating model cards.",
            "spof": false
          },
          {
            "path": "docs/source/en/pipeline_gradio.md",
            "description": "This document explains how to create machine learning web applications using Hugging Face pipelines and Gradio. It provides code examples for integrating pipelines with Gradio's interface and launching them locally or sharing them.",
            "spof": true
          },
          {
            "path": "docs/source/en/troubleshooting.md",
            "description": "This file provides troubleshooting guidance and solutions for common issues encountered when using the Hugging Face Transformers library, such as 'CUDA out of memory' or 'ImportError', and explains how to get further assistance.",
            "spof": false
          },
          {
            "path": "docs/source/en/philosophy.md",
            "description": "This document outlines the philosophy, core tenets, target audience, and expected functionalities of the Hugging Face Transformers library. It details the guiding principles for development, user expectations, and the main classes used within the library.",
            "spof": false
          },
          {
            "path": "docs/source/en/models.md",
            "description": "This document provides a guide on how to load pretrained models within the Hugging Face Transformers library, covering various methods, model classes, and strategies for efficiently handling large models like sharded checkpoints and Big Model Inference.",
            "spof": true
          },
          {
            "path": "docs/source/en/testing.md",
            "description": "This document describes the testing process for Hugging Face Transformers models, including how tests are run in CI, various methods for running tests locally, and advanced testing features like parallel execution and failure detection.",
            "spof": false
          },
          {
            "path": "docs/source/en/modular_transformers.md",
            "description": "This document explains how to contribute new models to the Transformers library using the Modular Transformers approach. It details how to leverage inheritance and modular files to reduce boilerplate code and simplify the model contribution process.",
            "spof": false
          },
          {
            "path": "docs/source/en/run_scripts.md",
            "description": "This document guides users on how to run example training scripts provided by the HuggingFace Transformers library. It covers setup, various execution options for different training scenarios, and utilizing custom datasets.",
            "spof": true
          },
          {
            "path": "docs/source/en/models_timeline.md",
            "description": "This file provides documentation and embeds an interactive timeline showcasing the evolution of Transformer models across various modalities and tasks. It allows users to explore the history and capabilities of different architectures.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/logging.md",
            "description": "This document explains how to configure and manage the logging system in the Hugging Face Transformers library, including setting verbosity levels, using environment variables, and integrating the library's logger into custom code. It also differentiates between the Python `logging` and `warnings` modules within the library's context.",
            "spof": false
          },
          {
            "path": "docs/source/en/main_classes/feature_extractor.md",
            "description": "This file provides documentation for the Feature Extractor classes and utilities within the Hugging Face Transformers library. It details their role in preparing input features for models, including audio pre-processing and tensor conversion.",
            "spof": false
          },
          {
            "path": "docs/source/en/main_classes/configuration.md",
            "description": "This file documents the `PreTrainedConfig` base class within the Hugging Face Transformers library, explaining its role in loading and saving model configurations and outlining common configuration attributes.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/training_vision_backbone.md",
            "description": "This document provides a guide on training vision models using the Hugging Face Transformers library's Backbone API, specifically demonstrating object detection for license plates using DINOv3 and a DETR head. It covers model setup, data preprocessing, training, and result visualization.",
            "spof": true
          },
          {
            "path": "docs/source/en/internal/tokenization_utils.md",
            "description": "This documentation file describes internal utility functions, enums, and classes, such as `PreTrainedTokenizerBase`, that are used by the tokenizers within the Hugging Face Transformers library. It is primarily useful for developers studying the tokenizer's underlying code.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/arcee.md",
            "description": "This file documents the Arcee model within the Hugging Face Transformers library, detailing its architecture, key features, usage examples, and API references for its various implementations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/blt.md",
            "description": "This file provides documentation for the Byte Latent Transformer (BLT) model within the Hugging Face Transformers library. It details the model's architecture, key features like dynamic patching, usage tips, and API references for its configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/autoformer.md",
            "description": "This file provides documentation for the Autoformer model within the Hugging Face Transformers library. It describes the model's overview, its theoretical basis, and includes autodoc directives for its configuration and core components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/deepseek_v3.md",
            "description": "This file provides documentation for the DeepSeek-V3 model, including an overview of its architecture, usage instructions, limitations, and a call for community contributions within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/fnet.md",
            "description": "This file provides documentation for the FNet model within the Hugging Face Transformers library, detailing its overview, usage tips, and API references for various FNet-related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/jetmoe.md",
            "description": "This file provides documentation for the JetMoe model within the Hugging Face Transformers library, detailing its architecture, features, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/lfm2.md",
            "description": "This file provides documentation for the LFM2 model in the Hugging Face Transformers library, detailing its architecture, use cases, and providing a code example for inference. It serves as a guide for users looking to understand and implement LFM2 models.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/lfm2_moe.md",
            "description": "This file provides documentation for the Lfm2Moe model, including an overview of its features, an example of its usage, and API references for its related classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/lw_detr.md",
            "description": "This file provides documentation for the LW-DETR model, detailing its architecture, efficiency enhancements, usage examples, and API references within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/hunyuan_v1_dense.md",
            "description": "This file provides the documentation for the HunYuanDenseV1 model within the Hugging Face Transformers library, including its configuration and various model classes. It appears to be an initial draft or placeholder, with many sections awaiting content upon the model's official release.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/moshi.md",
            "description": "This document provides an overview and usage guide for the Moshi model within the Hugging Face Transformers library, detailing its architecture and how to perform generation and prepare for training.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/minimax_m2.md",
            "description": "This file provides documentation for the MiniMax-M2 model within the Hugging Face Transformers library, including an overview, usage examples, and API references for its configuration and model implementations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/mra.md",
            "description": "This file provides documentation for the MRA (Multi Resolution Analysis) model within the Hugging Face Transformers library. It details the model's overview, its original paper, and the various MRA-related classes implemented in the library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/patchtsmixer.md",
            "description": "This documentation file provides an overview, usage examples, and API references for the PatchTSMixer model within the Hugging Face Transformers library. It details its capabilities for multivariate time series forecasting, classification, and regression.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/swiftformer.md",
            "description": "This file provides documentation for the SwiftFormer model, including an overview of its architecture, performance, and links to its configuration and model classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/seed_oss.md",
            "description": "This file provides documentation for the SeedOss model within the Hugging Face Transformers library, outlining its configuration, model architecture, and various task-specific implementations, with content placeholders indicating an upcoming official release.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/time_series_transformer.md",
            "description": "This file provides documentation for the Time Series Transformer model within the Hugging Face Transformers library, detailing its overview, usage tips, and API references for its configuration, base model, and prediction head.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/timesfm.md",
            "description": "This file provides documentation for the TimesFM (Time Series Foundation Model) within the Hugging Face Transformers library, including an overview, its scientific paper, and usage examples. It also documents the `TimesFmConfig`, `TimesFmModel`, and `TimesFmModelForPrediction` classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/reference",
            "description": "This directory is intended to house reference documentation for the Transformers library, specifically the English version. It likely contains API specifications, detailed module explanations, or other technical references to guide users and developers.",
            "spof": false
          },
          {
            "path": "docs/source/es/philosophy.md",
            "description": "This file outlines the philosophy, design principles, and core concepts of the Hugging Face Transformers library, explaining its target audience, ease of use, and key components (Model, Configuration, Tokenizer classes).",
            "spof": false
          },
          {
            "path": "docs/source/it/model_sharing.md",
            "description": "This document provides instructions on how to share trained or fine-tuned models on the Hugging Face Model Hub. It covers both programmatic methods and using the web interface, along with details on versioning, framework conversion, and adding model cards.",
            "spof": true
          },
          {
            "path": "docs/source/it/migration.md",
            "description": "This file provides a migration guide for users transitioning their code from older versions (v3.x) of the Hugging Face Transformers library or related predecessor libraries (pytorch-transformers, pytorch-pretrained-bert) to the current version (v4.x). It details breaking changes and provides instructions on how to adapt existing code.",
            "spof": true
          },
          {
            "path": "docs/source/pt/installation.md",
            "description": "This document provides a comprehensive guide for installing Hugging Face Transformers, including instructions for various deep learning libraries, configuring the local cache, and setting up the library for offline use.",
            "spof": true
          },
          {
            "path": "docs/source/zh/contributing.md",
            "description": "This document provides a comprehensive guide for contributors to the Hugging Face Transformers library. It covers various ways to contribute, such as fixing bugs, adding features, implementing new models, and improving documentation, along with detailed instructions for the pull request process, testing, and style guidelines.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Steven Liu",
            "percent": 37
          },
          {
            "name": "Cyril Vallez",
            "percent": 7
          },
          {
            "name": "Joao Gante",
            "percent": 6
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 319,
      "spofCount": 172
    },
    "busFactor": 12,
    "authorCount": 91
  },
  "Training & Inference Framework": {
    "description": "A robust toolkit that simplifies the end-to-end machine learning workflow, featuring a high-level `Trainer` API for efficient fine-tuning and `Pipelines` for easy, production-ready inference.",
    "functions": {
      "Automated Component Loading (`Auto*` Factory)": {
        "files": [
          {
            "path": "utils/sort_auto_mappings.py",
            "description": "This utility script sorts entries within `OrderedDict` auto-mappings found in the `transformers` library's auto modules. It can either fix incorrectly sorted mappings or simply check if they are sorted correctly.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/auto.md",
            "description": "This documentation file explains Hugging Face Transformers' \"Auto Classes,\" which automatically load appropriate model architectures, tokenizers, or configurations based on pretrained model names. It covers their usage, extension, and lists various AutoModel classes for diverse NLP, computer vision, audio, multimodal, and time series tasks.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/auto.md",
            "description": "This document provides an explanation and reference for the `Auto` classes in the Transformers library, detailing how they automatically load models, tokenizers, and configurations for various machine learning tasks. It covers their usage, extension, and lists specific `AutoModel` classes for NLP, computer vision, audio, multimodal, and time series applications.",
            "spof": false
          },
          {
            "path": "tests/utils/test_offline.py",
            "description": "This file contains unit tests for the 'offline mode' functionality of the Hugging Face Transformers library. It verifies that models, tokenizers, and pipelines can be loaded from cached files when network access is restricted by environment variables like TRANSFORMERS_OFFLINE or HF_HUB_OFFLINE.",
            "spof": true
          },
          {
            "path": "tests/models/auto/test_processor_auto.py",
            "description": "This file contains unit tests for the `AutoProcessor` class in the Hugging Face Transformers library. It verifies the functionality of `AutoProcessor` to load and instantiate various processor types from different sources and configurations.",
            "spof": false
          },
          {
            "path": "tests/models/auto/test_modeling_auto.py",
            "description": "This file contains unit tests for the `AutoModel` and `AutoConfig` classes in the Hugging Face Transformers library, verifying their `from_pretrained` methods for various model architectures and tasks.",
            "spof": false
          },
          {
            "path": "tests/models/auto/test_feature_extraction_auto.py",
            "description": "This file contains unit tests for the `AutoFeatureExtractor` class in the Hugging Face Transformers library, verifying its ability to load and manage feature extractors from various sources, including handling dynamic code and custom registrations.",
            "spof": false
          },
          {
            "path": "tests/models/auto/test_configuration_auto.py",
            "description": "This file contains unit tests for the `AutoConfig` class in the Hugging Face Transformers library, verifying its ability to load and register model configurations from various sources, including pretrained models, local files, and dynamic code.",
            "spof": true
          },
          {
            "path": "src/transformers/file_utils.py",
            "description": "This module provides file-related utilities, primarily for downloading and caching models. It is maintained solely for backward compatibility and should not be updated.",
            "spof": false
          },
          {
            "path": "src/transformers/__init__.py",
            "description": "This `__init__.py` file serves as the main entry point for the `transformers` library, defining its version, managing lazy loading of submodules and their components, and setting up checks for optional dependencies. It structures the public API of the library for efficient imports.",
            "spof": false
          },
          {
            "path": "src/transformers/dynamic_module_utils.py",
            "description": "This file provides utilities for dynamically loading, creating, and managing Python modules, especially those related to Hugging Face models from the Hub. It includes functions for sanitizing module names, managing a module cache, inspecting imports, and ensuring module dependencies are met.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/hub.py",
            "description": "This file provides utility functions for interacting with the Hugging Face Hub, specifically for downloading, caching, and managing model and template files.",
            "spof": false
          },
          {
            "path": "src/transformers/models/auto/feature_extraction_auto.py",
            "description": "This file provides auto-loading and configuration utilities for feature extractors in the Hugging Face Transformers library. It maps model types to their corresponding feature extractor classes and handles the retrieval of feature extractor configurations from pretrained models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/auto/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `auto` module in Hugging Face Transformers. It utilizes lazy loading to consolidate access to automatically configured model components such as factories, configurations, tokenizers, and models.",
            "spof": true
          },
          {
            "path": "src/transformers/models/auto/auto_factory.py",
            "description": "This file contains the factory functions and logic for dynamically building and instantiating 'AutoModel' classes within the Hugging Face Transformers library, allowing models to be loaded from configurations or pretrained checkpoints.",
            "spof": false
          },
          {
            "path": "src/transformers/models/auto/configuration_auto.py",
            "description": "This file defines the AutoConfig class and a mapping of model types to their corresponding configuration classes within the Hugging Face Transformers library, enabling automatic configuration loading for various models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/auto/modeling_auto.py",
            "description": "This file defines the `AutoModel` class and manages a comprehensive mapping of various model architectures to their corresponding base model classes within the Hugging Face Transformers library. It facilitates automatic model loading based on model names.",
            "spof": false
          },
          {
            "path": "src/transformers/models/auto/tokenization_auto.py",
            "description": "This file implements the `AutoTokenizer` class, providing a mechanism to automatically select and load the appropriate tokenizer for a given model type or configuration within the Hugging Face Transformers library. It maps model types to specific tokenizer classes, simplifying the process of instantiating tokenizers.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deprecated/__init__.py",
            "description": "This `__init__.py` file initializes the `deprecated` models subpackage using `_LazyModule`. It is designed to manage and potentially lazy-load models that are no longer actively maintained or are slated for removal from the `transformers` library.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Lysandre Debut",
            "percent": 32
          },
          {
            "name": "Ita Zaporozhets",
            "percent": 12
          },
          {
            "name": "Cyril Vallez",
            "percent": 9
          }
        ]
      },
      "Inference Abstraction (`Pipeline` API)": {
        "files": [
          {
            "path": "utils/check_pipeline_typing.py",
            "description": "This script automatically generates and updates `@overload` type signatures for the `pipeline` function in the `transformers` library, ensuring type-checking correctness for various supported tasks. It reads the existing `pipeline` definition, creates new overloaded signatures based on `SUPPORTED_TASKS`, and can fix or overwrite the `__init__.py` file.",
            "spof": true
          },
          {
            "path": "utils/test_module/custom_pipeline.py",
            "description": "This file defines a custom Hugging Face Transformers pipeline, `PairClassificationPipeline`, for handling text pair classification tasks. It includes methods for preprocessing text pairs, performing model inference, and post-processing model outputs to extract labels, scores, and logits.",
            "spof": true
          },
          {
            "path": "docs/source/de/add_new_pipeline.md",
            "description": "This document, written in German, provides a guide on how to create, register, share on the Hugging Face Hub, and contribute custom pipelines to the Transformers library, including code examples and explanations of pipeline components.",
            "spof": true
          },
          {
            "path": "docs/source/de/pipeline_tutorial.md",
            "description": "This file is a German-language tutorial explaining how to use Hugging Face `pipelines` for various inference tasks, including text generation, audio classification, image classification, and visual question answering.",
            "spof": false
          },
          {
            "path": "docs/source/ar/pipeline_tutorial.md",
            "description": "This file is an Arabic tutorial that guides users on how to effectively use the Hugging Face Transformers `pipeline` for inference across various tasks. It details specifying models, optimizing with parameters like `device` and `batch_size`, and processing datasets.",
            "spof": true
          },
          {
            "path": "docs/source/en/add_new_pipeline.md",
            "description": "This document provides a guide on how to add a new custom pipeline to the Hugging Face Transformers library, covering implementation, registration, and sharing on the Hub or directly within the library.",
            "spof": true
          },
          {
            "path": "docs/source/en/pipeline_tutorial.md",
            "description": "This documentation file provides a tutorial on using the `Pipeline` API in the Hugging Face Transformers library. It explains how to perform various machine learning tasks, configure parameters, and optimize inference with batching and device settings.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/mlx.md",
            "description": "This documentation file explains how to integrate and use Hugging Face Transformers models with the MLX array framework, especially for machine learning on Apple silicon and CUDA. It provides installation instructions, code examples for loading and generating text, and details on the bidirectional integration between MLX and Transformers.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/pipelines.md",
            "description": "This document describes the `transformers` library's pipeline feature, explaining how to use pipelines for various NLP tasks and detailing strategies for batching to optimize inference performance.",
            "spof": false
          },
          {
            "path": "docs/source/en/internal/pipelines_utils.md",
            "description": "This file documents internal utility functions and classes provided by the `transformers` library specifically for its pipelines, covering argument handling, data formats, and exceptions.",
            "spof": true
          },
          {
            "path": "docs/source/es/add_new_pipeline.md",
            "description": "This document is a Spanish guide explaining how to create a custom Hugging Face pipeline, register it, share it on the Hub, and contribute it to the Transformers library. It details the necessary steps and code examples for implementing a custom pipeline class.",
            "spof": true
          },
          {
            "path": "docs/source/es/pipeline_tutorial.md",
            "description": "This document is a Spanish-language tutorial for the Hugging Face Transformers library, specifically focusing on how to use the `pipeline` API for inference across various tasks. It covers basic usage, selecting specific models, and optimizing performance with parameters like device, batch size, and task-specific settings.",
            "spof": true
          },
          {
            "path": "docs/source/hi/pipeline_tutorial.md",
            "description": "This document is a tutorial in Hindi on how to use the Hugging Face Transformers `pipeline` for various inference tasks, including examples and parameter explanations.",
            "spof": true
          },
          {
            "path": "docs/source/fr/tutoriel_pipeline.md",
            "description": "This tutorial explains how to use the Hugging Face Transformers `pipeline` object for inference across various tasks, including audio, vision, and multimodal. It covers using specific models, managing devices, batching, and task-specific parameters.",
            "spof": true
          },
          {
            "path": "docs/source/it/add_new_pipeline.md",
            "description": "This document, written in Italian, provides a guide on how to create, register, share, and contribute custom pipelines to the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/pt/pipeline_tutorial.md",
            "description": "This file is a tutorial explaining how to use the Hugging Face Transformers `pipeline` for inference across various tasks, including text generation, audio classification, and computer vision. It covers general usage, selecting specific models and tokenizers, and examples for different data modalities.",
            "spof": false
          },
          {
            "path": "docs/source/zh/pipeline_tutorial.md",
            "description": "This file is a Chinese tutorial that explains how to use the `pipeline` feature in the Hugging Face Transformers library for various tasks including audio, vision, NLP, and multimodal applications, covering basic usage, parameters, and advanced topics like batching and large model inference.",
            "spof": true
          },
          {
            "path": "docs/source/zh/add_new_pipeline.md",
            "description": "This document provides a guide on how to create a custom pipeline for the Hugging Face Transformers library, including steps to define, register, share on the Hub, and contribute it to the library.",
            "spof": true
          },
          {
            "path": "docs/source/zh/internal/pipelines_utils.md",
            "description": "This file documents internal utility functions and classes provided by the `transformers` library for its `pipelines` module, covering aspects like argument handling, data formats, and exceptions.",
            "spof": true
          },
          {
            "path": "docs/source/zh/main_classes/pipelines.md",
            "description": "This document provides a comprehensive guide to using pipelines in the Hugging Face Transformers library, covering their functionality, usage examples, batch processing, and customization options for various tasks across audio, computer vision, and natural language processing.",
            "spof": false
          },
          {
            "path": "docs/source/ja/pipeline_tutorial.md",
            "description": "This document is a Japanese tutorial explaining how to use the `pipeline` feature in the Hugging Face Transformers library for various inference tasks, including language, computer vision, audio, and multimodal applications. It covers basic usage, task-specific parameters, and how to use pipelines with datasets.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/pipelines.md",
            "description": "This document, written in Japanese, describes the `pipeline` abstraction in the Hugging Face Transformers library, explaining its usage for various AI tasks, features like batching, and how to customize or implement new pipelines.",
            "spof": false
          },
          {
            "path": "docs/source/ja/internal/pipelines_utils.md",
            "description": "This is a Japanese documentation page that lists and describes utility functions provided by the Transformers library for its pipelines. It covers argument handling, data formats, and general utilities.",
            "spof": true
          },
          {
            "path": "docs/source/ko/add_new_pipeline.md",
            "description": "This Korean guide explains how to create custom pipelines for the Hugging Face Transformers library, including how to define their components, share them on the Hub, and contribute them to the library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/pipeline_tutorial.md",
            "description": "This document is a Korean tutorial explaining how to use the Hugging Face Transformers `pipeline` for inference across various tasks, including language, computer vision, audio, and multimodal applications. It covers basic usage, parameter customization, and advanced topics like processing datasets and web server integration.",
            "spof": true
          },
          {
            "path": "docs/source/ko/pipeline_gradio.md",
            "description": "This document, written in Korean, explains how to build and share machine learning applications using Hugging Face Transformers `Pipeline` and Gradio. It provides code examples for installing Gradio, creating a pipeline, and launching a Gradio interface from it.",
            "spof": true
          },
          {
            "path": "docs/source/ko/internal/pipelines_utils.md",
            "description": "This document lists and describes various utility functions and classes provided by the Hugging Face Transformers library for handling pipelines, including argument handlers, data formats, and exceptions. It is a Korean-language internal documentation file.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/pipelines.md",
            "description": "This document provides Korean-language documentation for the `pipelines` feature in Hugging Face Transformers, explaining how to use them for various tasks, including batch processing, FP16 inference, and custom implementations, while also listing specific pipeline types for audio and computer vision.",
            "spof": true
          },
          {
            "path": "tests/test_pipeline_mixin.py",
            "description": "This file defines the `PipelineTesterMixin` class, which provides common testing utilities and methods for various Hugging Face Transformers pipelines. It orchestrates the execution of pipeline-specific tests by mapping task names to their respective test classes and models.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_zero_shot.py",
            "description": "This file contains unit tests for the ZeroShotClassificationPipeline in the Hugging Face Transformers library, covering various inputs, error conditions, and model behaviors.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_text_classification.py",
            "description": "This file contains unit tests for the `TextClassificationPipeline` within the Hugging Face Transformers library. It verifies the functionality of the text classification pipeline with various models, input formats, and configurations.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_common.py",
            "description": "This file contains common unit tests for the `transformers` library's pipeline functionality, verifying aspects like iteration, batching, custom pipeline classes, and scikit-learn compatibility.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_feature_extraction.py",
            "description": "This file contains unit tests for the feature extraction pipeline functionality within the Hugging Face Transformers library, specifically for PyTorch models.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_video_classification.py",
            "description": "This file contains unit tests for the video classification pipeline in the Hugging Face Transformers library. It verifies the functionality and output format of the `VideoClassificationPipeline` using example video data.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_image_feature_extraction.py",
            "description": "This file contains unit tests for the `ImageFeatureExtractionPipeline` in the Hugging Face Transformers library. It verifies the pipeline's functionality, including handling different models, pooling options, image processing parameters, and return tensor types.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_any_to_any.py",
            "description": "This file contains unit tests for the `AnyToAnyPipeline` in the Hugging Face Transformers library, verifying its functionality with multimodal inputs (images, videos, audio) and text generation, including chat templates.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_table_question_answering.py",
            "description": "This file contains unit tests for the Table Question Answering (TQA) pipeline in the Hugging Face Transformers library. It verifies the pipeline's functionality, including handling different models, data types, and various query scenarios for table question answering tasks.",
            "spof": true
          },
          {
            "path": "tests/pipelines/test_pipelines_image_text_to_text.py",
            "description": "This file contains unit tests for the `ImageTextToTextPipeline` in the Hugging Face Transformers library. It verifies the pipeline's functionality with various models, handling both text-only and image-text inputs.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_image_classification.py",
            "description": "This file contains unit tests for the `ImageClassificationPipeline` in the Transformers library. It verifies the pipeline's functionality with various models, input types, data types, and classification scenarios, including multi-label classification.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_image_to_image.py",
            "description": "This file contains unit tests for the `ImageToImagePipeline` in the Hugging Face Transformers library. It verifies the pipeline's functionality for tasks like image upscaling, including tests for different data types and initialization methods, requiring PyTorch and vision capabilities.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_fill_mask.py",
            "description": "This file contains unit tests for the `FillMaskPipeline` functionality within the Hugging Face Transformers library. It verifies the pipeline's behavior with various models, inputs, and configurations, including small and large models, FP16 casting, and tokenizer specific settings.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_token_classification.py",
            "description": "This file contains unit tests for the TokenClassificationPipeline in the Hugging Face Transformers library. It verifies the pipeline's functionality, including different aggregation strategies and handling of long texts through chunking.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_keypoint_matching.py",
            "description": "This file contains unit tests for the `KeypointMatchingPipeline` in the Transformers library. It verifies the pipeline's functionality for finding matching keypoints between image pairs using various input formats and asserts the correctness of the output keypoint coordinates and scores.",
            "spof": true
          },
          {
            "path": "src/transformers/pipelines/image_classification.py",
            "description": "This file defines the `ImageClassificationPipeline` for Hugging Face Transformers, enabling image classification tasks. It handles preprocessing images, running them through a model, and post-processing the results, including applying activation functions like sigmoid or softmax.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/image_segmentation.py",
            "description": "This file implements the `ImageSegmentationPipeline` for the Hugging Face Transformers library, allowing users to perform image segmentation tasks using various models. It handles image loading, preprocessing, model inference, and postprocessing to extract object masks and labels from input images.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/image_feature_extraction.py",
            "description": "This file defines the `ImageFeatureExtractionPipeline` class, which extracts hidden states or features from pre-trained image transformer models for use in downstream tasks. It handles image loading, preprocessing, model inference without a head, and postprocessing to return the extracted features.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/text_classification.py",
            "description": "This file defines the `TextClassificationPipeline` class, which provides a high-level API for performing text classification tasks using various sequence classification models. It handles preprocessing, model inference, and postprocessing of results, including applying sigmoid or softmax functions to outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/video_classification.py",
            "description": "This file implements the `VideoClassificationPipeline` for the Hugging Face Transformers library, enabling users to perform video classification tasks using pre-trained models. It handles video loading, preprocessing, model inference, and post-processing of results to output classified labels and scores.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/image_text_to_text.py",
            "description": "This file defines the `ImageTextToTextPipeline` for the Transformers library. It provides a pipeline to generate text based on an input image and accompanying text, including support for conversational models and chat formats.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/image_to_image.py",
            "description": "This file defines the `ImageToImagePipeline` class for the Hugging Face Transformers library, enabling image-to-image transformation tasks such as super-resolution. It provides methods for preprocessing input images, running model inference, and postprocessing model outputs into PIL Image format.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/keypoint_matching.py",
            "description": "This file implements the `KeypointMatchingPipeline` for the Hugging Face Transformers library, enabling users to perform keypoint matching between pairs of images using appropriate models.",
            "spof": true
          },
          {
            "path": "src/transformers/pipelines/__init__.py",
            "description": "This file serves as the main entry point for the Hugging Face Transformers pipelines module, defining and registering all supported tasks like text classification, question answering, and image classification, along with their respective pipeline implementations and default models. It centralizes the configuration and access for various pre-trained model pipelines.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/token_classification.py",
            "description": "This file implements the Token Classification pipeline for tasks like Named Entity Recognition (NER) within the Transformers library. It defines argument handling, aggregation strategies for token predictions, and the core pipeline logic for token classification models.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/feature_extraction.py",
            "description": "This file defines the `FeatureExtractionPipeline` class, which extracts hidden states from a transformer model as features from input text without a specific model head. These extracted features can then be used for various downstream tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/audio_utils.py",
            "description": "This file provides utility functions for reading audio data from files and microphones using ffmpeg, supporting chunked and live streaming audio processing for pipelines.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/document_question_answering.py",
            "description": "This file implements the Document Question Answering pipeline within the Hugging Face Transformers library. It enables answering questions based on document images, optionally using OCR (Tesseract) to extract text and bounding boxes if not provided.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/table_question_answering.py",
            "description": "This file implements the `TableQuestionAnsweringPipeline` for the Transformers library, which enables answering natural language questions based on tabular data. It includes an argument handler for various table and query input formats and supports both single-shot and sequential (conversational) question answering.",
            "spof": true
          },
          {
            "path": "src/transformers/pipelines/base.py",
            "description": "This file provides core utilities and base functionalities for Hugging Face Transformers pipelines, including data collation, padding for batch processing, and a robust `load_model` function for various model types.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/pt_utils.py",
            "description": "This file provides utility classes (`PipelineDataset`, `PipelineIterator`, `PipelineChunkIterator`, `PipelinePackIterator`) for efficiently processing and iterating over data within `transformers` pipelines. These classes handle common patterns like applying functions to items, unrolling batches, and flattening nested iterative data structures.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 22
          },
          {
            "name": "Matt",
            "percent": 11
          },
          {
            "name": "Joao Gante",
            "percent": 9
          }
        ]
      },
      "Model Training & Evaluation (`Trainer` API)": {
        "files": [
          {
            "path": "utils/get_test_info.py",
            "description": "This file provides utility functions to parse and extract structured information, such as test classes, model classes, and model tester classes, from Hugging Face Transformers model test files for analysis or reporting purposes.",
            "spof": false
          },
          {
            "path": "examples/pytorch/semantic-segmentation/run_semantic_segmentation.py",
            "description": "This script facilitates finetuning Hugging Face Transformers models for semantic segmentation using the Trainer API, including data loading, preprocessing, and metric computation.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/trainer_qa.py",
            "description": "This file defines a custom `Trainer` subclass (`QuestionAnsweringTrainer`) tailored for Question Answering tasks, extending functionality for evaluation and prediction, including post-processing of results and metric computation.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/trainer_seq2seq_qa.py",
            "description": "This file defines a custom `Seq2SeqTrainer` subclass, `QuestionAnsweringSeq2SeqTrainer`, specifically designed for Question-Answering tasks. It extends the base trainer by implementing specialized evaluation and prediction logic, including post-processing functions for metrics calculation.",
            "spof": false
          },
          {
            "path": "docs/source/ar/trainer.md",
            "description": "This document provides an Arabic-language guide to the `Trainer` class in the Hugging Face Transformers library, explaining its basic usage for model training, customization options, and related classes.",
            "spof": false
          },
          {
            "path": "docs/source/ar/training.md",
            "description": "This document provides a guide in Arabic on how to fine-tune pre-trained models using the Hugging Face `Trainer` API or native PyTorch, demonstrating the process with code examples for dataset preparation and training.",
            "spof": false
          },
          {
            "path": "docs/source/en/optimizers.md",
            "description": "This document describes various optimizers compatible with HuggingFace Transformers, including how to install and configure them using `TrainingArguments`. It covers optimizers like APOLLO, GrokAdamW, LOMO, Schedule Free, and StableAdamW.",
            "spof": true
          },
          {
            "path": "docs/source/en/hpo_train.md",
            "description": "This document explains how to perform hyperparameter search using the Hugging Face `Trainer`. It details setting up a `model_init` function and integrating with different backends like Optuna, Ray Tune, and Weights & Biases.",
            "spof": false
          },
          {
            "path": "docs/source/en/training.md",
            "description": "This file serves as a guide for fine-tuning Hugging Face Transformer models using the `Trainer` API, providing a step-by-step example of text classification with the Yelp Reviews dataset.",
            "spof": true
          },
          {
            "path": "docs/source/en/trainer.md",
            "description": "This document serves as a comprehensive guide to the `Trainer` class in the Hugging Face Transformers library, detailing its use for training and evaluating PyTorch models. It explains configuration, checkpointing, logging, and customization options for the training loop.",
            "spof": false
          },
          {
            "path": "docs/source/en/quicktour.md",
            "description": "This file is a quickstart guide for the Hugging Face Transformers library, explaining how to set up, load pretrained models, perform inference using `Pipeline`, and fine-tune models with `Trainer`.",
            "spof": false
          },
          {
            "path": "docs/source/en/community_integrations/axolotl.md",
            "description": "This document provides an overview of Axolotl, a fine-tuning framework for large language models, and details its integration with Hugging Face Transformers.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/trl.md",
            "description": "This document describes the TRL (Transformer Reinforcement Learning) framework, its functionalities for post-training foundation models, and its integration with the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/callback.md",
            "description": "This document explains the concept and usage of Callbacks within the HuggingFace PyTorch `Trainer` to customize and inspect the training loop. It lists various available callbacks and demonstrates how to register custom ones.",
            "spof": false
          },
          {
            "path": "docs/source/en/main_classes/trainer.md",
            "description": "This documentation file describes the `Trainer` and `Seq2SeqTrainer` classes in the Hugging Face Transformers library, along with their respective `TrainingArguments` classes, which provide an API for feature-complete model training in PyTorch.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/optimizer_schedules.md",
            "description": "This documentation file describes the optimization module within the Hugging Face Transformers library, detailing various learning rate schedulers, the Adafactor optimizer, and gradient accumulation. It provides explanations and references for different schedule types and functions to obtain them.",
            "spof": false
          },
          {
            "path": "docs/source/en/internal/trainer_utils.md",
            "description": "This file documents internal utility functions and classes specifically used by the `Trainer` class within the Transformers library. It serves as a reference for developers studying the `Trainer`'s implementation.",
            "spof": true
          },
          {
            "path": "docs/source/es/trainer.md",
            "description": "This documentation file, written in Spanish, provides a comprehensive guide to the `Trainer` class in the Hugging Face Transformers library. It explains its basic usage for model training and evaluation, as well as methods for customization.",
            "spof": false
          },
          {
            "path": "docs/source/es/training.md",
            "description": "This document provides a Spanish-language guide on how to fine-tune pre-trained models using the Hugging Face Transformers library. It covers preparing datasets, and outlines fine-tuning processes using `Trainer`, Keras, and native PyTorch.",
            "spof": true
          },
          {
            "path": "docs/source/it/training.md",
            "description": "This document provides an Italian guide on fine-tuning pre-trained models using Hugging Face Transformers, covering data preparation, training with the `Trainer` API, and native PyTorch examples.",
            "spof": true
          },
          {
            "path": "docs/source/pt/training.md",
            "description": "This file is a Portuguese-language tutorial on fine-tuning pre-trained models using the Hugging Face Transformers library. It covers fine-tuning with the `Trainer` API, Keras, and native PyTorch, demonstrating data preparation and model training.",
            "spof": true
          },
          {
            "path": "docs/source/zh/training.md",
            "description": "This document provides a guide on fine-tuning pretrained models using the Hugging Face Transformers library. It details the process of data preparation, training with the Trainer API, and implementing a native PyTorch training loop.",
            "spof": true
          },
          {
            "path": "docs/source/zh/hpo_train.md",
            "description": "This document explains how to perform hyperparameter search using the Trainer API in the Hugging Face Transformers library. It covers the setup, supported backends (Optuna, Ray Tune, Weights & Biases), and provides code examples for defining search spaces and executing the search.",
            "spof": false
          },
          {
            "path": "docs/source/zh/internal/trainer_utils.md",
            "description": "This file provides internal documentation in Chinese for utility functions and classes used by the `Trainer` class in the Hugging Face Transformers library, primarily for developers examining its source code.",
            "spof": false
          },
          {
            "path": "docs/source/zh/main_classes/callback.md",
            "description": "This document explains how to use and implement callbacks within the Hugging Face `Trainer` to customize training loop behavior in PyTorch. It details available callbacks, `TrainerState`, and `TrainerControl`, and provides examples for registering custom callbacks.",
            "spof": false
          },
          {
            "path": "docs/source/zh/main_classes/trainer.md",
            "description": "This document provides a comprehensive guide to the Hugging Face Transformers `Trainer` class, detailing its usage, customization options, and integration with various training features like distributed training, logging, and GPU management. It also covers related classes like `TrainingArguments` and integration with external libraries such as DeepSpeed.",
            "spof": true
          },
          {
            "path": "docs/source/ja/hpo_train.md",
            "description": "This document provides a guide in Japanese on how to perform hyperparameter search using the Hugging Face Transformers Trainer API, detailing the setup and usage of various optimization backends like Optuna, Ray Tune, and Wandb.",
            "spof": false
          },
          {
            "path": "docs/source/ja/training.md",
            "description": "This document provides a Japanese-language tutorial on how to fine-tune pretrained models using the Hugging Face Transformers library, covering methods with the `Trainer` API, native PyTorch, and TensorFlow/Keras.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/callback.md",
            "description": "This document explains callbacks in the Hugging Face Transformers library, detailing how they customize the PyTorch `Trainer`'s behavior. It covers built-in callbacks, integration callbacks, and provides guidance on implementing and registering custom callbacks.",
            "spof": false
          },
          {
            "path": "docs/source/ja/main_classes/optimizer_schedules.md",
            "description": "This document provides an overview of optimization techniques and learning rate schedules available in the Transformers library, including `AdaFactor` and various `get_scheduler` functions. It serves as a reference for configuring optimizers and learning rate schedules in Japanese.",
            "spof": false
          },
          {
            "path": "docs/source/ja/main_classes/trainer.md",
            "description": "This documentation file, written in Japanese, describes the `Trainer` class in Hugging Face Transformers, covering its use for model training, customization, and advanced features like distributed training, logging, GPU selection, and integrations with DeepSpeed and PyTorch FSDP.",
            "spof": true
          },
          {
            "path": "docs/source/ja/internal/trainer_utils.md",
            "description": "This document provides a Japanese reference for utility functions used within the `Trainer` class in the HuggingFace Transformers library. It details various internal utilities, callbacks, and argument parsing tools, primarily for those studying the library's trainer code.",
            "spof": false
          },
          {
            "path": "docs/source/ko/hpo_train.md",
            "description": "Ïù¥ Î¨∏ÏÑúÎäî Hugging Face `Trainer` APIÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâÏùÑ ÏàòÌñâÌïòÎäî Î∞©Î≤ïÏùÑ ÏÑ§Î™ÖÌï©ÎãàÎã§. Optuna, Ray Tune, WandBÏôÄ Í∞ôÏùÄ Îã§ÏñëÌïú Î∞±ÏóîÎìúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥Î•º ÌôúÏÑ±ÌôîÌïòÎäî ÏòàÏãúÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_sharing.md",
            "description": "This document, written in Korean, provides a tutorial on how to share trained or fine-tuned models on the Hugging Face Model Hub. It covers methods like pushing models via API during training, using the `push_to_hub` function, and uploading through the web interface, along with best practices like framework conversion and adding a model card.",
            "spof": true
          },
          {
            "path": "docs/source/ko/trainer.md",
            "description": "This document provides a comprehensive guide to using the `Trainer` class in the Hugging Face Transformers library for training and evaluating PyTorch models, covering basic usage, customization, and advanced features like logging, NEFTune, and GaLore.",
            "spof": false
          },
          {
            "path": "docs/source/ko/training.md",
            "description": "This document provides a Korean-language guide on how to fine-tune pre-trained models using the Hugging Face Transformers library, covering data preparation, training with Trainer or native PyTorch, and evaluation.",
            "spof": true
          },
          {
            "path": "docs/source/ko/internal/trainer_utils.md",
            "description": "This document lists and describes various internal utility functions and classes used by the Hugging Face Transformers `Trainer` component, primarily for developers examining its source code.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/optimizer_schedules.md",
            "description": "This document provides Korean-language documentation for the `optimization` module within the Hugging Face Transformers library, detailing optimizers like AdaFactor and various learning rate schedules, including those with warmup strategies.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/trainer.md",
            "description": "This document provides a Korean-language guide to the `Trainer` and `Seq2SeqTrainer` classes in the Hugging Face Transformers library, detailing their use for model training, including distributed training and mixed precision support, along with their associated `TrainingArguments` classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/callback.md",
            "description": "This document explains callbacks in the context of Hugging Face's PyTorch Trainer. It details their purpose, lists available integrations, and provides guidance on implementing and registering custom callbacks.",
            "spof": true
          },
          {
            "path": "tests/test_training_mixin.py",
            "description": "This file defines a mixin class, `TrainingTesterMixin`, used in model tests to verify that a tiny model can successfully overfit a small, fixed batch of data, ensuring basic training functionality.",
            "spof": true
          },
          {
            "path": "tests/test_training_args.py",
            "description": "This file contains unit tests for the `TrainingArguments` class within the `transformers` library, verifying its behavior regarding output directory handling and `torch_empty_cache_steps` validation.",
            "spof": false
          },
          {
            "path": "tests/sagemaker/README.md",
            "description": "This document outlines the testing strategy for new Hugging Face Deep Learning Container releases on AWS SageMaker, covering test procedures for new `transformers` versions and new AWS framework DLCs (e.g., PyTorch).",
            "spof": true
          },
          {
            "path": "tests/sagemaker/__init__.py",
            "description": "This file provides a utility function to check if the 'sagemaker' module is available in the current environment, likely for conditional execution of SageMaker-related tests or features.",
            "spof": true
          },
          {
            "path": "tests/trainer/test_trainer.py",
            "description": "This file contains unit tests for the `Trainer` class within the Hugging Face Transformers library, covering various training scenarios, data handling, and callback functionalities.",
            "spof": false
          },
          {
            "path": "tests/trainer/test_trainer_callback.py",
            "description": "This file contains unit tests for the `TrainerCallback` functionality in the Hugging Face Transformers library. It verifies the proper registration, execution, and event flow of callbacks within the `Trainer` class.",
            "spof": false
          },
          {
            "path": "tests/trainer/test_trainer_jit_checkpoint.py",
            "description": "This file contains unit tests for the JIT (Just-In-Time) checkpointing functionality within the HuggingFace Transformers Trainer, specifically testing the `CheckpointManager` and `JITCheckpointCallback` classes. It verifies the setup, execution, and signal handling related to dynamic checkpoint creation during training.",
            "spof": true
          },
          {
            "path": "tests/trainer/test_trainer_seq2seq.py",
            "description": "This file contains unit tests for the `Seq2SeqTrainer` class in the Hugging Face Transformers library, specifically verifying its functionality for sequence-to-sequence model training, generation, and error handling with generation configurations.",
            "spof": false
          },
          {
            "path": "tests/trainer/test_trainer_fsdp.py",
            "description": "This file contains unit tests for the Hugging Face Trainer's Fully Sharded Data Parallel (FSDP) capabilities, covering various configurations like mixed precision (FP8) and `torch_compile`.",
            "spof": false
          },
          {
            "path": "tests/trainer/test_trainer_tpu.py",
            "description": "This file contains integration tests for the Hugging Face `Trainer` class, specifically designed to run on TPUs. It verifies the distributed evaluation and prediction functionality of the Trainer by ensuring all samples are processed correctly and in order.",
            "spof": false
          },
          {
            "path": "tests/trainer/test_trainer_utils.py",
            "description": "This file contains unit tests for utility functions and classes used by the Hugging Face Transformers Trainer, covering aspects like data sampling, label smoothing, and parameter introspection.",
            "spof": false
          },
          {
            "path": "benchmark_v2/framework/benchmark_runner.py",
            "description": "This file defines the `BenchmarkRunner` class, which orchestrates the benchmarking of Hugging Face Transformers models for text generation tasks, including setup, execution, and performance measurement with GPU metrics.",
            "spof": true
          },
          {
            "path": "src/transformers/hyperparameter_search.py",
            "description": "This file defines abstract and concrete classes for integrating different hyperparameter search backends like Optuna, Ray Tune, and Weights & Biases into the Transformers library. It provides a unified interface for running hyperparameter optimization with various tools.",
            "spof": false
          },
          {
            "path": "src/transformers/optimization.py",
            "description": "This file implements various learning rate schedulers for PyTorch optimizers, including schedules with constant, linear, cosine, and polynomial decay, often combined with a linear warmup phase.",
            "spof": false
          },
          {
            "path": "src/transformers/trainer.py",
            "description": "This file defines the `Trainer` class, which provides a comprehensive training and evaluation loop for PyTorch models, specifically optimized for ü§ó Transformers models. It allows users to easily train models from scratch or fine-tune them on new tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/modelcard.py",
            "description": "This file provides utilities and data structures (like `TrainingSummary`) for generating and managing model card metadata for Hugging Face models, including task mappings, metric tags, and inference of training details.",
            "spof": true
          },
          {
            "path": "src/transformers/trainer_callback.py",
            "description": "This file defines classes and utilities for managing the state of the training process within the HuggingFace Trainer, including `TrainerState` for checkpointing and `ExportableState` for callbacks whose internal state needs to be saved and restored.",
            "spof": false
          },
          {
            "path": "src/transformers/training_args_seq2seq.py",
            "description": "This file defines `Seq2SeqTrainingArguments`, an extension of `TrainingArguments` tailored for sequence-to-sequence models, incorporating parameters specifically for generation during training and evaluation.",
            "spof": false
          },
          {
            "path": "src/transformers/trainer_jit_checkpoint.py",
            "description": "This file implements a Just-In-Time (JIT) checkpointing mechanism for the Hugging Face Transformers Trainer. It allows the trainer to gracefully save its state and exit when a SIGTERM signal is received, preventing data loss during unexpected shutdowns.",
            "spof": true
          },
          {
            "path": "src/transformers/trainer_seq2seq.py",
            "description": "This file defines the `Seq2SeqTrainer` class, which extends the base `Trainer` to provide specialized functionalities for training, evaluation, and prediction of sequence-to-sequence models, including handling generation-specific arguments.",
            "spof": false
          },
          {
            "path": "src/transformers/training_args.py",
            "description": "This file defines the `TrainingArguments` dataclass, which centralizes all hyperparameters, optimization settings, logging preferences, and infrastructure choices for training models using the Hugging Face Transformers Trainer. It also includes utility functions and definitions for various optimizers.",
            "spof": false
          },
          {
            "path": "src/transformers/trainer_utils.py",
            "description": "This file provides PyTorch-independent utility functions for the Hugging Face Transformers `Trainer` class, including tools for reproducibility (seeding), handling evaluation and prediction outputs, and managing model checkpoints.",
            "spof": false
          },
          {
            "path": "src/transformers/trainer_pt_utils.py",
            "description": "This file provides PyTorch-specific utility functions for the Hugging Face `Trainer` class. It includes functionalities for tensor manipulation, data handling, and distributed training operations such as concatenation, data preparation, and process synchronization.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/notebook.py",
            "description": "This file provides utility classes and functions for displaying interactive progress bars and training metrics within Jupyter notebooks. It includes a `NotebookProgressBar` for general progress tracking and a `NotebookTrainingTracker` for monitoring model training, complete with a metrics table.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/hp_naming.py",
            "description": "This file defines a utility class `TrialShortNamer` for generating and parsing compact, human-readable short names for hyperparameter trials. It maps hyperparameter names and their values to shorter, unique strings, primarily highlighting parameters that deviate from their default values.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Marc Sun",
            "percent": 24
          },
          {
            "name": "Yuanyuan Chen",
            "percent": 8
          },
          {
            "name": "Steven Liu",
            "percent": 6
          }
        ]
      },
      "Core Model Primitives & Utilities": {
        "files": [
          {
            "path": "utils/test_module/custom_configuration.py",
            "description": "This file defines a `CustomConfig` class, which inherits from `PreTrainedConfig`, to create a custom configuration for a Hugging Face Transformers model. It initializes with a specific attribute and sets the model type.",
            "spof": true
          },
          {
            "path": "utils/test_module/custom_modeling.py",
            "description": "This file defines a custom PyTorch model, `CustomModel`, which inherits from `transformers.PreTrainedModel` and uses a `CustomConfig`. It serves as an example or test implementation for a simple custom model within the Hugging Face Transformers ecosystem.",
            "spof": true
          },
          {
            "path": "docs/source/en/weightconverter.md",
            "description": "This documentation explains dynamic weight loading in the Transformers library, focusing on the `WeightConverter` for transforming checkpoint tensors to match a model's runtime requirements. It details various conversion operations and highlights the efficiency and reusability of this loading mechanism.",
            "spof": true
          },
          {
            "path": "docs/source/en/community_integrations/candle.md",
            "description": "This file documents the Candle machine learning framework, explaining its native Rust implementation of Transformers models and its integration with Hugging Face models and `safetensors`.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/model.md",
            "description": "This documentation file describes the base classes and mixins for models in the Transformers library, primarily focusing on `PreTrainedModel` for loading, saving, and general model utilities, and `PushToHubMixin` for integrating with Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "docs/source/it/create_a_model.md",
            "description": "This documentation page in Italian guides users on how to create and customize models, configurations, and tokenizers in the Hugging Face Transformers library. It explains how to build a custom model architecture by manually configuring components instead of relying on `AutoClass`.",
            "spof": true
          },
          {
            "path": "docs/source/zh/internal/time_series_utils.md",
            "description": "This documentation file, written in Chinese, lists utility functions and classes for time series models within the Hugging Face Transformers library, specifically focusing on output distributions.",
            "spof": true
          },
          {
            "path": "docs/source/zh/main_classes/configuration.md",
            "description": "This document describes the `PreTrainedConfig` base class in the Hugging Face Transformers library, detailing its role in loading, saving, and managing model configurations. It also highlights common configuration attributes shared across different model types.",
            "spof": true
          },
          {
            "path": "docs/source/zh/main_classes/model.md",
            "description": "This document describes the `PreTrainedModel` class in Hugging Face Transformers, outlining methods for loading, saving, and managing models. It focuses on efficient loading of large models using techniques like `device_map` for device placement and `dtype` for memory optimization.",
            "spof": false
          },
          {
            "path": "docs/source/ja/main_classes/configuration.md",
            "description": "This Japanese Markdown file documents the `PreTrainedConfig` class in the HuggingFace Transformers library, explaining its functionality for loading/saving model configurations and detailing common as well as model-specific attributes.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/model.md",
            "description": "This document provides an overview of the core model classes in the Hugging Face Transformers library, primarily focusing on `PreTrainedModel`. It details functionalities for loading, saving, and managing large models, including device mapping and dtype specifications, as well as common utility mixins.",
            "spof": false
          },
          {
            "path": "docs/source/ja/internal/modeling_utils.md",
            "description": "This file is a Japanese documentation page detailing internal custom PyTorch layers and utility functions used in the `transformers` library's modeling code.",
            "spof": false
          },
          {
            "path": "docs/source/ja/internal/time_series_utils.md",
            "description": "This file provides Japanese documentation for internal utility functions and classes used with time-series models, specifically detailing various distributional output classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/internal/modeling_utils.md",
            "description": "This document lists and describes custom layers and utility functions used for modeling within the Hugging Face Transformers library, primarily for those studying the library's internal model code. It focuses on PyTorch custom modules and helper functions.",
            "spof": false
          },
          {
            "path": "tests/test_configuration_common.py",
            "description": "This file contains a utility class, `ConfigTester`, designed to test common functionalities of configuration classes within the Hugging Face Transformers library. It includes methods for verifying property handling, serialization to JSON, and saving/loading configurations to/from pretrained models.",
            "spof": true
          },
          {
            "path": "tests/test_backbone_common.py",
            "description": "This file defines a `BackboneTesterMixin` class, providing a set of common tests for backbone models in the Hugging Face Transformers library. It ensures consistent configuration handling, attribute initialization, and output structure across different backbone architectures.",
            "spof": false
          },
          {
            "path": "tests/repo_utils/modular/test_conversion_order.py",
            "description": "This file contains a unit test to verify the correct conversion order of modular model files within the `transformers` library. It ensures that specific models appear after their dependencies or predecessors in a generated priority list.",
            "spof": false
          },
          {
            "path": "tests/utils/test_backbone_utils.py",
            "description": "This file contains unit tests for the backbone utility functions and mixins in the `transformers` library. It covers the configuration, output feature handling, and loading mechanisms for various backbone models.",
            "spof": true
          },
          {
            "path": "tests/utils/test_modeling_utils.py",
            "description": "This file contains unit tests for the utility functions and base classes within `transformers.modeling_utils`, including tests for model loading, weight tying, and various model configurations and behaviors.",
            "spof": false
          },
          {
            "path": "tests/utils/test_configuration_utils.py",
            "description": "This file contains unit tests for the `transformers` library's configuration utilities, including testing the ability to save, load, and push configurations to the Hugging Face Hub, as well as handling various configuration formats and versioning.",
            "spof": false
          },
          {
            "path": "tests/utils/test_core_model_loading.py",
            "description": "This file contains unit tests for the core model loading utilities in the Hugging Face Transformers library. It specifically tests functionalities like weight glob matching, renaming, conversion, and loading state dictionaries into models, including complex operations for architectures like MoE and QKV projections.",
            "spof": false
          },
          {
            "path": "tests/models/autoformer/test_modeling_autoformer.py",
            "description": "This file contains the testing suite for the PyTorch Autoformer model within the Hugging Face Transformers library, including configuration tests, model component tests, and common model behavior tests.",
            "spof": false
          },
          {
            "path": "tests/models/doge/test_modeling_doge.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch implementation of the Doge model within the Hugging Face Transformers library, covering various functionalities and configurations.",
            "spof": true
          },
          {
            "path": "tests/fixtures/xcodec",
            "description": "This directory is designated to store test fixtures specifically for 'xcodec' related functionalities within the Transformers library. While currently empty, it serves as a logical placeholder for test data or configuration files required for testing components interacting with 'xcodec'.",
            "spof": false
          },
          {
            "path": "src/transformers/activations.py",
            "description": "This file defines various activation functions used in neural networks, including different implementations and approximations of GELU, SiLU, Mish, and other custom activation functions. It provides a collection of activation modules for the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/core_model_loading.py",
            "description": "This file contains core utilities and operations for loading and transforming model weights in the Hugging Face Transformers library, including functionalities like chunking, concatenating, merging, splitting, and transposing tensors.",
            "spof": false
          },
          {
            "path": "src/transformers/conversion_mapping.py",
            "description": "This file defines comprehensive mapping and conversion rules for adapting model checkpoints from various architectures, particularly focusing on Mixture-of-Experts (MoE) models, to the internal format used by the Transformers library. It includes operations like weight renaming, merging, concatenating, and transposing for seamless model loading and compatibility.",
            "spof": false
          },
          {
            "path": "src/transformers/pytorch_utils.py",
            "description": "This file provides a collection of PyTorch utility functions and classes for the Transformers library, including version compatibility checks, memory management for forward passes, custom layers, and tensor storage identification.",
            "spof": false
          },
          {
            "path": "src/transformers/initialization.py",
            "description": "This file provides guarded PyTorch weight initialization functions, preventing re-initialization of already processed tensors, and includes context managers for patching `torch.nn.init` functions or temporarily disabling weight initialization for performance.",
            "spof": true
          },
          {
            "path": "src/transformers/time_series_utils.py",
            "description": "This file provides classes and utilities for defining and handling probabilistic outputs for time series models, mapping model predictions to parameters of various PyTorch distributions (e.g., Normal, StudentT, Negative Binomial).",
            "spof": false
          },
          {
            "path": "src/transformers/modeling_utils.py",
            "description": "This file provides essential utility functions and classes for the Transformers library's model infrastructure, handling tasks like model loading, saving, weight conversion, configuration management, and integration with distributed training and performance optimization techniques.",
            "spof": false
          },
          {
            "path": "src/transformers/loss/__init__.py",
            "description": "This is an empty `__init__.py` file that marks the `loss` directory as a Python package. It indicates that this directory is intended to contain loss functions for the `transformers` library.",
            "spof": true
          },
          {
            "path": "src/transformers/loss/loss_utils.py",
            "description": "This file provides various loss functions and utilities for different machine learning tasks, including causal language modeling, masked language modeling, sequence classification, question answering, token classification, and object detection. It consolidates these losses into a `LOSS_MAPPING` dictionary for easy retrieval and use within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/type_validators.py",
            "description": "This file contains utility functions for validating various input types and parameters, such as positive numbers, padding strategies, truncation strategies, image/video metadata, device specifications, and tensor types, used across the `transformers` library.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/loading_report.py",
            "description": "This file provides utilities for creating, formatting, and logging detailed reports on the status of state dictionary loading operations in models, specifically handling missing, unexpected, or mismatched keys and conversion errors. It helps users diagnose issues when loading pre-trained weights.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/backbone_utils.py",
            "description": "This file provides deprecated re-exports of `BackboneConfigMixin` and `BackboneMixin`, issuing a `FutureWarning` to guide users to their new import location in `transformers.backbone_utils`. It serves as a temporary backward-compatibility layer before these re-exports are removed.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/__init__.py",
            "description": "This file serves as the main `__init__.py` for the `transformers.utils` package, consolidating and exposing a broad range of helper functions, constants, and dependency availability checks used throughout the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/autoformer/configuration_autoformer.py",
            "description": "This file defines the `AutoformerConfig` class, which is used to configure the architecture and hyperparameters of the Autoformer model within the Hugging Face Transformers library. It specifies parameters for time series forecasting, transformer layers, and other model specifics.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 42
          },
          {
            "name": "Arthur",
            "percent": 23
          },
          {
            "name": "Raushan Turganbay",
            "percent": 8
          }
        ]
      },
      "Command-Line Interface & Serving": {
        "files": [
          {
            "path": "docs/source/ar/pipeline_webserver.md",
            "description": "This document provides a guide on setting up a web server using Hugging Face Transformers pipelines for inference, focusing on efficient request handling via asynchronous queues. It discusses optimizations like dynamic batching and best practices for production deployment.",
            "spof": true
          },
          {
            "path": "docs/source/en/pipeline_webserver.md",
            "description": "This document provides a guide on how to build a web server for inference using Hugging Face's `Pipeline` API, addressing challenges such as managing resource-intensive models with concurrent web requests and implementing features like dynamic batching and error handling.",
            "spof": true
          },
          {
            "path": "docs/source/en/serve-cli/serving.md",
            "description": "This file documents the `transformers serve` CLI, explaining its use for local or self-hosted model serving and detailing its OpenAI-compatible API endpoints for text and image-based completions and responses.",
            "spof": false
          },
          {
            "path": "docs/source/es/pipeline_webserver.md",
            "description": "This file provides a guide and example code for setting up a web server to serve Hugging Face Transformers pipelines for inference. It focuses on handling concurrent requests efficiently using asynchronous queues and discusses various considerations for robust production deployment.",
            "spof": true
          },
          {
            "path": "docs/source/ja/pipeline_webserver.md",
            "description": "This document provides a guide on how to integrate Hugging Face pipelines into a web server for model inference, demonstrating an example with Starlette. It covers setting up a basic server, handling concurrent requests, and important considerations like batching, error handling, and threading for production environments.",
            "spof": true
          },
          {
            "path": "docs/source/ko/pipeline_webserver.md",
            "description": "This document explains how to use Hugging Face pipelines to build a web server, focusing on handling requests efficiently, implementing batching, and discussing various considerations for production environments.",
            "spof": true
          },
          {
            "path": "docs/source/ko/serving.md",
            "description": "This document provides guidance in Korean on serving Transformer models using specialized inference libraries such as Text Generation Inference (TGI) and vLLM, including practical examples and configuration options.",
            "spof": true
          },
          {
            "path": "tests/cli/conftest.py",
            "description": "This file provides a pytest fixture for testing the command-line interface (CLI) of the `transformers` library. It uses `typer.testing.CliRunner` to invoke CLI commands within tests.",
            "spof": false
          },
          {
            "path": "tests/cli/test_download.py",
            "description": "This file contains unit tests for the Hugging Face Transformers CLI's model download functionality, ensuring models can be downloaded and cached correctly, including scenarios with `trust-remote-code`.",
            "spof": true
          },
          {
            "path": "tests/cli/test_serve.py",
            "description": "This file contains unit and integration tests for the `transformers` library's CLI `serve` command. It verifies the functionality of the model serving capabilities, API responses (like chat completion chunks and response events), and CLI argument parsing.",
            "spof": true
          },
          {
            "path": "src/transformers/cli/download.py",
            "description": "This file provides a command-line interface (CLI) function to download a specified model and its corresponding tokenizer from the Hugging Face Hub. It leverages `typer` for argument parsing and `AutoModel` and `AutoTokenizer` for the download logic.",
            "spof": true
          },
          {
            "path": "src/transformers/cli/transformers.py",
            "description": "This file serves as the main entry point for the Hugging Face Transformers Command Line Interface (CLI). It registers various subcommands like `chat`, `download`, `serve`, and others, allowing users to interact with the library's functionalities from the command line.",
            "spof": true
          },
          {
            "path": "src/transformers/cli/serve.py",
            "description": "This file implements the server-side logic for the `transformers` CLI, providing an OpenAI-compatible API to serve various Hugging Face models (LLM, VLM, STT, TTS) for inference.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Lysandre Debut",
            "percent": 35
          },
          {
            "name": "Lucain",
            "percent": 16
          },
          {
            "name": "Lysandre",
            "percent": 15
          }
        ]
      },
      "Data Processing & Collation": {
        "files": [
          {
            "path": "docs/source/en/main_classes/data_collator.md",
            "description": "This document provides an overview and detailed documentation for various `DataCollator` classes within the Transformers library, explaining their role in forming batches from dataset elements, including processing like padding and data augmentation.",
            "spof": false
          },
          {
            "path": "docs/source/es/preprocessing.md",
            "description": "This document provides a guide in Spanish on preprocessing data (text, audio, and images) for machine learning models, covering tokenization, padding, truncation, and tensor conversion for text, and feature extraction for audio and images.",
            "spof": true
          },
          {
            "path": "docs/source/es/pad_truncation.md",
            "description": "This document explains padding and truncation strategies for handling variable-length input sequences when batching data for models. It details the `padding`, `truncation`, and `max_length` arguments, providing configuration examples, particularly in the context of the Hugging Face `transformers` library.",
            "spof": true
          },
          {
            "path": "docs/source/zh/main_classes/data_collator.md",
            "description": "This file is a Chinese documentation page explaining data collators in the Hugging Face Transformers library. It describes their purpose in batching and preprocessing datasets, and lists various specific data collator classes available.",
            "spof": false
          },
          {
            "path": "docs/source/zh/main_classes/feature_extractor.md",
            "description": "This document, written in Chinese, explains the concept of 'Feature Extractor' in the Hugging Face Transformers library. It details how feature extractors prepare input features for audio and visual models, including preprocessing, extraction, padding, normalization, and conversion to tensors, and references several related classes.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/feature_extractor.md",
            "description": "This file is a Japanese documentation page describing the Feature Extractor component in the Transformers library. It explains its role in preparing input features for audio and vision models, including pre-processing, feature extraction, padding, and normalization.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/data_collator.md",
            "description": "This file provides Japanese documentation for various data collator classes in the Transformers library. It explains their function in forming batches, applying padding, and performing data augmentation for different NLP tasks.",
            "spof": false
          },
          {
            "path": "docs/source/ko/main_classes/processors.md",
            "description": "This document explains the concept of 'processors' in the Hugging Face Transformers library, distinguishing between current multimodal processors and deprecated processors used for tasks like GLUE, XNLI, and SQuAD, and provides usage examples.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/data_collator.md",
            "description": "This document provides Korean-language documentation for data collators in the Hugging Face Transformers library. It explains their role in preparing data batches and details various specialized data collator classes.",
            "spof": true
          },
          {
            "path": "tests/test_feature_extraction_common.py",
            "description": "This file defines a mixin class, `FeatureExtractionSavingTestMixin`, which provides common test methods for feature extraction classes in the Hugging Face Transformers library. It primarily focuses on testing the serialization and deserialization capabilities of these classes, including saving to and loading from JSON strings and files, and using `save_pretrained` and `from_pretrained` methods.",
            "spof": false
          },
          {
            "path": "tests/test_processing_common.py",
            "description": "This file defines `ProcessorTesterMixin`, a common test utility class for `Processor` components within the Hugging Face Transformers library. It provides methods for setting up and testing processors, either from pretrained models or by dynamically creating their constituent components (like feature extractors, tokenizers, or image processors).",
            "spof": false
          },
          {
            "path": "tests/trainer/test_data_collator.py",
            "description": "This file contains unit tests for the data collator functionalities in the Hugging Face Transformers library, ensuring they correctly process and prepare input data for various tasks like padding, flattening, and token classification.",
            "spof": false
          },
          {
            "path": "tests/utils/test_feature_extraction_utils.py",
            "description": "This file contains unit tests for the feature extraction utilities in the Transformers library. It primarily tests the `BatchFeature` class for data handling, tensor conversion to NumPy and PyTorch, and error handling for various input types.",
            "spof": true
          },
          {
            "path": "src/transformers/feature_extraction_utils.py",
            "description": "This file provides utility classes and functions for handling outputs of feature extractors, including the `BatchFeature` class for managing and converting feature data to various tensor types.",
            "spof": false
          },
          {
            "path": "src/transformers/feature_extraction_sequence_utils.py",
            "description": "This file defines `SequenceFeatureExtractor`, a base class for processing sequential data, handling operations like padding and truncation for common feature extraction tasks in the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/processing_utils.py",
            "description": "This file provides utility classes and functions for processing various data modalities (text, image, audio, video) within the Hugging Face Transformers library. It defines common keyword arguments and mechanisms for saving and loading processor configurations.",
            "spof": false
          },
          {
            "path": "src/transformers/data/datasets/__init__.py",
            "description": "This `__init__.py` file serves to expose specific dataset and data training argument classes, such as Glue and SQuAD, directly under the `transformers.data.datasets` package. It acts as a package initializer for dataset-related components within the Hugging Face Transformers library.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Raushan Turganbay",
            "percent": 36
          },
          {
            "name": "Yoni Gozlan",
            "percent": 23
          },
          {
            "name": "Joao Gante",
            "percent": 7
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 211,
      "spofCount": 93
    },
    "busFactor": 13,
    "authorCount": 83
  },
  "Natural Language Processing (NLP) Model Library": {
    "description": "A comprehensive collection of state-of-the-art models for text-based tasks, including text generation, classification, translation, and question answering.",
    "functions": {
      "Task-Specific Execution and Inference": {
        "files": [
          {
            "path": "utils/models_to_deprecate.py",
            "description": "This script identifies candidate models for deprecation within the Hugging Face Transformers library by analyzing their download counts and the date of their first commit.",
            "spof": true
          },
          {
            "path": "examples/pytorch/language-modeling/run_clm_no_trainer.py",
            "description": "This script facilitates fine-tuning causal language models (like GPT) on text datasets or files, utilizing the HuggingFace `transformers` library and `accelerate` for distributed training, without relying on the HuggingFace Trainer API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/run_qa_beam_search.py",
            "description": "This script is designed for fine-tuning XLNet models for question answering tasks using beam search, leveraging a modified version of the Hugging Face Trainer.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/utils_qa.py",
            "description": "This file contains utilities for post-processing predictions from a question-answering model, converting raw logits into human-readable answers by extracting substrings from the original contexts. It specifically handles the logic for determining the best answer spans and managing negative examples.",
            "spof": false
          },
          {
            "path": "examples/pytorch/question-answering/run_seq2seq_qa.py",
            "description": "This script is designed for fine-tuning Hugging Face Transformers' sequence-to-sequence models for question answering tasks, utilizing the ü§ó Seq2SeqTrainer.",
            "spof": false
          },
          {
            "path": "examples/pytorch/translation/run_translation_no_trainer.py",
            "description": "This script fine-tunes a Hugging Face Transformers model for text translation tasks. It handles dataset loading, model configuration, training, and evaluation, without relying on the `Trainer` API.",
            "spof": false
          },
          {
            "path": "examples/pytorch/text-classification/run_xnli.py",
            "description": "This script fine-unes multi-lingual transformer models (e.g., Bert, DistilBERT, XLM) for text classification tasks on the XNLI dataset. It handles data loading, model configuration, and training/evaluation using the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "examples/pytorch/summarization/run_summarization.py",
            "description": "This script is designed for fine-tuning Hugging Face Transformers sequence-to-sequence models for summarization tasks. It handles data loading, preprocessing, and training/evaluation with various configurable arguments.",
            "spof": false
          },
          {
            "path": "docs/source/de/llm_tutorial.md",
            "description": "This file is a German-language tutorial explaining how to generate text with Large Language Models (LLMs) using the Hugging Face Transformers library. It covers basic text generation, common pitfalls, and strategies to address them.",
            "spof": false
          },
          {
            "path": "docs/source/ar/conversations.md",
            "description": "This Arabic-language document provides a comprehensive guide on interacting with transformer-based chat models, covering a quick start using pipelines, criteria for selecting appropriate models, and a detailed breakdown of the underlying processes involved in text generation.",
            "spof": true
          },
          {
            "path": "docs/source/ar/multilingual.md",
            "description": "This document, written in Arabic, explains how to use various multilingual models (e.g., XLM, BERT, M2M100, MBart) from the Hugging Face Transformers library for inference. It details specific usage patterns, including handling language embeddings and translation tasks, with code examples for each model type.",
            "spof": true
          },
          {
            "path": "docs/source/ar/perplexity.md",
            "description": "This document explains Perplexity (PPL) as an evaluation metric for fixed-length language models, detailing its calculation, limitations, and the use of a sliding window strategy for more accurate results. It includes a practical example using GPT-2 and Hugging Face Transformers to demonstrate PPL calculation.",
            "spof": true
          },
          {
            "path": "docs/source/ar/tasks/question_answering.md",
            "description": "This file is an Arabic documentation page for question answering tasks using the Hugging Face Transformers library. It provides a guide on how to fine-tune a DistilBERT model on the SQuAD dataset and perform inference.",
            "spof": false
          },
          {
            "path": "docs/source/ar/tasks/multiple_choice.md",
            "description": "This document provides a guide in Arabic on how to fine-tune a BERT model for multiple-choice tasks using the SWAG dataset and then use the trained model for inference. It covers preprocessing data, evaluating performance, and training the model.",
            "spof": false
          },
          {
            "path": "docs/source/ar/tasks/sequence_classification.md",
            "description": "This document provides a guide in Arabic on how to perform text classification using the Hugging Face Transformers library. It details fine-tuning a DistilBERT model for sentiment analysis on the IMDb dataset and demonstrates inference with the trained model.",
            "spof": false
          },
          {
            "path": "docs/source/ar/tasks/masked_language_modeling.md",
            "description": "This file is an Arabic documentation page explaining Masked Language Modeling (MLM) and providing a step-by-step guide on how to fine-tune a DistilRoBERTa model for an MLM task using the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ar/tasks/language_modeling.md",
            "description": "This file is an Arabic tutorial explaining causal language modeling. It demonstrates how to fine-tune a model for text generation using the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ar/tasks/translation.md",
            "description": "This document provides an Arabic-language guide on how to fine-tune a T5 model for English-to-French translation using the Hugging Face Transformers and Datasets libraries, covering data preprocessing, model evaluation, training, and inference.",
            "spof": false
          },
          {
            "path": "docs/source/ar/tasks/token_classification.md",
            "description": "This document provides an Arabic-language tutorial on how to perform token classification using the Hugging Face Transformers library. It specifically guides users through fine-tuning a DistilBERT model for Named Entity Recognition (NER) on the WNUT 17 dataset.",
            "spof": false
          },
          {
            "path": "docs/source/en/llm_tutorial.md",
            "description": "This file is a tutorial explaining text generation with Large Language Models (LLMs) using the Hugging Face Transformers library, focusing on the `generate` API, its configuration, and common usage options.",
            "spof": false
          },
          {
            "path": "docs/source/en/perplexity.md",
            "description": "This documentation file explains perplexity (PPL) as an evaluation metric for fixed-length language models, detailing its calculation using a sliding-window strategy and providing an example with GPT-2 in Hugging Face Transformers.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/multiple_choice.md",
            "description": "This file is a documentation guide explaining how to finetune a BERT model for multiple-choice tasks using the SWAG dataset. It covers preprocessing, training, evaluation, and inference steps for this task.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/token_classification.md",
            "description": "This document is a tutorial on token classification, demonstrating how to finetune a DistilBERT model for Named Entity Recognition (NER) on the WNUT 17 dataset using the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/question_answering.md",
            "description": "This documentation file provides a guide on how to finetune a DistilBERT model for extractive question answering on the SQuAD dataset using the Hugging Face Transformers library, and how to perform inference with the finetuned model.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/sequence_classification.md",
            "description": "This file provides a tutorial on how to perform sequence classification using Hugging Face Transformers, specifically demonstrating finetuning DistilBERT on the IMDb dataset for sentiment analysis and then using the model for inference.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/translation.md",
            "description": "This file provides a guide on how to finetune a T5 model for English-French translation using the Hugging Face Transformers library, covering preprocessing, training, and inference steps.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/prompting.md",
            "description": "This document serves as a guide to prompt engineering for Large Language Models (LLMs), detailing best practices and various techniques like few-shot and chain-of-thought prompting. It illustrates how to craft effective prompts to improve LLM performance and when fine-tuning might be more suitable.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/masked_language_modeling.md",
            "description": "This file provides a tutorial and guide on masked language modeling using the Hugging Face Transformers library, detailing how to finetune a DistilRoBERTa model on the ELI5 dataset.",
            "spof": true
          },
          {
            "path": "docs/source/es/multilingual.md",
            "description": "This document provides a guide on how to perform inference with various multilingual models available in Hugging Face Transformers, detailing specific usage patterns such as language embeddings for different model architectures. It covers models like XLM, BERT, XLM-RoBERTa, M2M100, and MBart.",
            "spof": true
          },
          {
            "path": "docs/source/es/perplexity.md",
            "description": "This document explains perplexity (PPL) for fixed-length language models in Spanish, detailing its calculation, the challenges of limited context, and demonstrating a sliding window approach using Hugging Face Transformers and GPT-2.",
            "spof": true
          },
          {
            "path": "docs/source/es/tasks/question_answering.md",
            "description": "This document provides a guide on how to fine-tune a model for extractive question answering using Hugging Face Transformers. It demonstrates the process of loading, preprocessing, and training DistilBERT on the SQuAD dataset.",
            "spof": true
          },
          {
            "path": "docs/source/es/tasks/language_modeling.md",
            "description": "This document provides a guide in Spanish on language modeling tasks, specifically demonstrating how to fine-tune models like DistilGPT2 and DistilRoBERTa for causal and masked language modeling using the Hugging Face Transformers library and datasets.",
            "spof": true
          },
          {
            "path": "docs/source/it/multilingual.md",
            "description": "This document is an Italian guide on using various multilingual models in the Hugging Face Transformers library for inference. It details specific configurations and requirements for models like XLM, BERT, XLM-RoBERTa, M2M100, and MBart, including handling language embeddings and forced beginning-of-sequence tokens.",
            "spof": true
          },
          {
            "path": "docs/source/pt/multilingual.md",
            "description": "This document provides a guide for using various multilingual models like XLM, BERT, XLM-RoBERTa, M2M100, and MBart for inference within the Hugging Face Transformers library, detailing specific usage patterns for each model, especially regarding language embeddings and translation tasks.",
            "spof": true
          },
          {
            "path": "docs/source/pt/tasks/sequence_classification.md",
            "description": "This file is a Portuguese documentation page explaining text classification, specifically demonstrating how to fine-tune DistilBERT on the IMDb dataset for sentiment analysis.",
            "spof": true
          },
          {
            "path": "docs/source/pt/tasks/token_classification.md",
            "description": "This documentation page in Portuguese explains token classification, focusing on Named Entity Recognition (NER). It provides a step-by-step guide on how to fine-tune a DistilBERT model on the WNUT 17 dataset for this task using the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/zh/llm_tutorial.md",
            "description": "This tutorial provides a comprehensive guide in Chinese on using Large Language Models (LLMs) for text generation with the Hugging Face Transformers library, covering basic usage, common pitfalls, and advanced generation strategies.",
            "spof": false
          },
          {
            "path": "docs/source/zh/multilingual.md",
            "description": "This document serves as a guide for using various multilingual models within the Hugging Face Transformers library for inference, detailing their specific usage patterns including handling language embeddings and setting source/target languages for translation tasks.",
            "spof": true
          },
          {
            "path": "docs/source/zh/bertology.md",
            "description": "This document introduces \"BERTology,\" the study of the internal mechanisms of large transformer models, and explains how the Hugging Face `transformers` library provides features and tools to facilitate this research, including an example script.",
            "spof": false
          },
          {
            "path": "docs/source/ja/perplexity.md",
            "description": "This document explains how to calculate perplexity (PPL) for fixed-length language models, emphasizing the sliding window strategy for accurate evaluation. It includes a practical example using GPT-2 with the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ja/multilingual.md",
            "description": "This document provides a guide in Japanese on how to use various multilingual models (like XLM, BERT, XLM-RoBERTa, M2M100, and MBart) for inference within the Hugging Face Transformers library, detailing differences in their usage, especially regarding language embeddings.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/token_classification.md",
            "description": "This document provides a Japanese-language guide on how to perform token classification using the Hugging Face Transformers library. It specifically details fine-tuning a DistilBERT model for Named Entity Recognition (NER) on the WNUT 17 dataset and using the trained model for inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/translation.md",
            "description": "This document, written in Japanese, provides a guide on how to perform a translation task using the Hugging Face Transformers library. It details the process of fine-tuning a T5 model for English-French translation, including data loading, preprocessing, evaluation, training, and inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/multiple_choice.md",
            "description": "This documentation page, written in Japanese, provides a guide on fine-tuning a BERT model for multiple-choice tasks using the SWAG dataset. It includes steps for data loading, preprocessing, evaluation, training, and performing inference with the fine-tuned model.",
            "spof": false
          },
          {
            "path": "docs/source/ja/tasks/question_answering.md",
            "description": "This file is a Japanese documentation guide on how to fine-tune a Hugging Face Transformers model for question answering, including training, pushing to the Hugging Face Hub, and performing inference.",
            "spof": true
          },
          {
            "path": "docs/source/ko/llm_tutorial.md",
            "description": "This file is a Korean tutorial that guides users on generating text with Large Language Models (LLMs) using the Hugging Face Transformers library. It covers the basics of autoregressive generation, practical implementation with code examples, common pitfalls, and resources for advanced usage.",
            "spof": true
          },
          {
            "path": "docs/source/ko/perplexity.md",
            "description": "This document explains perplexity (PPL) as an evaluation metric for fixed-length language models, detailing its calculation using a sliding window strategy. It includes a practical example of computing PPL with GPT-2 and Hugging Face Transformers in Korean.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/masked_language_modeling.md",
            "description": "This document is a Korean-language guide explaining Masked Language Modeling (MLM), demonstrating how to fine-tune a DistilRoBERTa model on the ELI5 dataset for MLM, and performing inference.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/question_answering.md",
            "description": "This document is a Korean-language guide on performing question answering tasks using the Hugging Face Transformers library. It details fine-tuning DistilBERT on the SQuAD dataset, preprocessing steps, training, and inference.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/language_modeling.md",
            "description": "This Korean-language Markdown document serves as a tutorial on causal language modeling using Hugging Face Transformers. It guides users through fine-tuning a DistilGPT2 model on the ELI5 dataset, covering data preprocessing, training, and inference steps.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/token_classification.md",
            "description": "This document is a Korean-language guide on how to perform token classification using the Hugging Face Transformers library. It provides instructions for fine-tuning a DistilBERT model for Named Entity Recognition (NER) on the WNUT 17 dataset and then using it for inference.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/summarization.md",
            "description": "This file is a Korean-language guide that explains how to perform text summarization using the Hugging Face Transformers library. It details fine-tuning a T5 model on the BillSum dataset and covers essential preprocessing steps.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/prompting.md",
            "description": "This document provides a comprehensive guide in Korean to Large Language Model (LLM) prompting, covering fundamental concepts, best practices, advanced techniques, and practical examples for various NLP tasks like classification, NER, translation, summarization, and question answering.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/sequence_classification.md",
            "description": "This document provides a guide on performing text classification using Hugging Face Transformers. It demonstrates how to fine-tune a DistilBERT model on the IMDb dataset for sentiment analysis and then use the trained model for inference.",
            "spof": true
          },
          {
            "path": "tests/cli/test_chat.py",
            "description": "This file contains unit tests for the Hugging Face Transformers CLI chat functionality, covering features like help output, saving and loading chat history, and parsing generation flags.",
            "spof": true
          },
          {
            "path": "tests/pipelines/test_pipelines_document_question_answering.py",
            "description": "This file contains unit tests for the Document Question Answering (DQA) pipeline in the Hugging Face Transformers library, covering various models, input formats, and configurations, including small, large, and bfloat16 models.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_question_answering.py",
            "description": "This file contains unit tests for the HuggingFace Transformers question-answering pipeline. It validates various functionalities, such as input handling, output format, error conditions, and performance across different models and configurations, including specific tests for small models and different data types.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/GermEval",
            "description": "This directory is intended to store test fixtures or sample data specifically for GermEval-related tests within the Transformers library. It provides necessary resources for verifying the library's functionality against the GermEval dataset.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/wmt16",
            "description": "This directory is designated to store test sample data or configurations related to the WMT 2016 machine translation task. It serves as a fixture location within the `transformers` library's testing suite, supporting tests that require specific input or setup relevant to WMT16.",
            "spof": false
          },
          {
            "path": "src/transformers/data/metrics/squad_metrics.py",
            "description": "This file implements evaluation metrics (Exact Match and F1 score) for the SQuAD (Stanford Question Answering Dataset) task, including handling unanswerable questions and finding optimal thresholds for no-answer probabilities.",
            "spof": false
          },
          {
            "path": "src/transformers/data/metrics/__init__.py",
            "description": "This file provides deprecated functions for computing evaluation metrics such as accuracy, F1-score, and correlation coefficients, primarily for GLUE and XNLI tasks. Users are advised to transition to the Hugging Face Evaluate library for metric computation.",
            "spof": true
          },
          {
            "path": "src/transformers/pipelines/zero_shot_classification.py",
            "description": "This file implements the Zero-Shot Classification pipeline for the Transformers library, enabling text classification using Natural Language Inference (NLI) models with dynamic, user-provided labels at runtime. It processes input sequences and candidate labels into NLI premise/hypothesis pairs for model inference.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/question_answering.py",
            "description": "This file implements the `QuestionAnsweringPipeline` for the Transformers library, providing functionalities to process inputs and interpret model outputs for question answering tasks. It includes utilities for decoding answer spans and handling various input formats for question-answering models.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/fill_mask.py",
            "description": "This file implements the `FillMaskPipeline` for masked language modeling prediction, allowing users to fill masked tokens in text using pre-trained models.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Yuanyuan Chen",
            "percent": 38
          },
          {
            "name": "Ahmed Almaghz",
            "percent": 17
          },
          {
            "name": "Joao Gante",
            "percent": 13
          }
        ]
      },
      "Text and Data Processing": {
        "files": [
          {
            "path": "utils/test_module/custom_tokenization_fast.py",
            "description": "This file defines a custom fast tokenizer, `CustomTokenizerFast`, which inherits from `BertTokenizerFast`. It integrates with the Hugging Face `transformers` library's auto-mapping system by linking to its slow counterpart, `CustomTokenizer`.",
            "spof": true
          },
          {
            "path": "docs/source/ar/pad_truncation.md",
            "description": "This document explains padding and truncation strategies for preparing variable-length input sequences for models, detailing the `padding`, `truncation`, and `max_length` parameters and providing usage examples with tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/ar/tiktoken.md",
            "description": "This document explains how tiktoken models and tokenizers are integrated with Hugging Face Transformers, including automatic loading, conversion to `PreTrainedTokenizerFast`, and instructions for creating custom tiktoken tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/ar/tokenizer_summary.md",
            "description": "This document provides a detailed explanation of tokenization processes, focusing on subword tokenization methods like BPE, WordPiece, and SentencePiece used in Hugging Face Transformers. It discusses the challenges and approaches to splitting text into subwords for natural language processing models.",
            "spof": true
          },
          {
            "path": "docs/source/en/pad_truncation.md",
            "description": "This document explains how to use padding and truncation with the Hugging Face Transformers tokenizer to handle input sequences of varying lengths. It details the `padding`, `truncation`, and `max_length` arguments and provides usage examples.",
            "spof": true
          },
          {
            "path": "docs/source/en/tokenizer_summary.md",
            "description": "This document provides a summary and explanation of various tokenization methods, including subword tokenization algorithms like BPE, WordPiece, and SentencePiece, used in Hugging Face Transformers. It details how these tokenizers work and their significance in natural language processing models.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/tokenizer.md",
            "description": "This documentation file explains the `Tokenizer` class in the Hugging Face Transformers library, detailing its purpose, functionalities, and the distinction between Python and 'Fast' (Rust-based) implementations. It also covers related classes like `BatchEncoding` and multimodal tokenizers.",
            "spof": false
          },
          {
            "path": "docs/source/es/fast_tokenizers.md",
            "description": "This document explains how to integrate and use tokenizers from the `ü§ó Tokenizers` library with `ü§ó Transformers`, detailing methods to load `PreTrainedTokenizerFast` from an instantiated tokenizer object or a saved JSON file.",
            "spof": false
          },
          {
            "path": "docs/source/zh/fast_tokenizers.md",
            "description": "This document explains how to use tokenizers from the ü§ó Tokenizers library within ü§ó Transformers, specifically demonstrating how to load them into `PreTrainedTokenizerFast` from a tokenizer object or a JSON file.",
            "spof": true
          },
          {
            "path": "docs/source/zh/tokenizer_summary.md",
            "description": "This document provides a summary and detailed explanation of various subword tokenization algorithms used in Hugging Face Transformers, including BPE, WordPiece, Unigram, and SentencePiece. It describes their mechanisms, advantages, and which Transformer models utilize each type.",
            "spof": true
          },
          {
            "path": "docs/source/zh/internal/tokenization_utils.md",
            "description": "This file is a Chinese documentation page listing utility functions and base classes used by tokenizers in the Hugging Face Transformers library. It primarily details the `PreTrainedTokenizerBase` and `SpecialTokens Mixin` classes, intended for developers studying the tokenizer code.",
            "spof": false
          },
          {
            "path": "docs/source/zh/main_classes/tokenizer.md",
            "description": "This document explains the tokenizer classes within the HuggingFace Transformers library, covering both the standard Python and the faster Rust-based implementations. It details their functionalities for preparing text input for models, including tokenization, encoding, and managing special tokens.",
            "spof": false
          },
          {
            "path": "docs/source/ja/fast_tokenizers.md",
            "description": "This document explains how to integrate and use tokenizers created with the ü§ó Tokenizers library within ü§ó Transformers. It covers methods for loading tokenizers directly from an object or from a saved JSON file using `PreTrainedTokenizerFast`.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tokenizer_summary.md",
            "description": "This document provides a detailed summary of the tokenization algorithms used in ü§ó Transformers, including Byte-Pair Encoding (BPE), WordPiece, and Unigram, explaining their mechanisms and applications. It also highlights the challenges of text tokenization and the advantages of subword tokenization.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/tokenizer.md",
            "description": "This document provides an overview and detailed documentation for the `Tokenizer` classes in the Hugging Face Transformers library, including `PreTrainedTokenizer` and `PreTrainedTokenizerFast`. It explains their role in preparing model inputs, handling tokenization, encoding, decoding, and the benefits of fast tokenizers.",
            "spof": false
          },
          {
            "path": "docs/source/ja/internal/tokenization_utils.md",
            "description": "This file is a Japanese documentation page (`ja/internal/tokenization_utils.md`) for the Hugging Face Transformers library. It describes the internal utility functions and classes, such as `PreTrainedTokenizerBase`, used by tokenizers within the library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/fast_tokenizers.md",
            "description": "This document explains how to use tokenizers from the Hugging Face Tokenizers library within the Hugging Face Transformers framework, focusing on `PreTrainedTokenizerFast`. It covers loading tokenizers directly from objects and from JSON files.",
            "spof": false
          },
          {
            "path": "docs/source/ko/tokenizer_summary.md",
            "description": "This document provides a summary and detailed explanation of different tokenization methods (BPE, WordPiece, Unigram) used in Hugging Face Transformers, including their mechanisms and examples.",
            "spof": true
          },
          {
            "path": "docs/source/ko/pad_truncation.md",
            "description": "This document explains padding and truncation strategies for handling variable-length input sequences in Transformer models. It details the `padding`, `truncation`, and `max_length` arguments and their various settings for proper input preparation.",
            "spof": true
          },
          {
            "path": "docs/source/ko/internal/tokenization_utils.md",
            "description": "This Korean documentation file lists and describes utility functions, classes, enums, and namedtuples used by tokenizers in the Transformers library, primarily focusing on `PreTrainedTokenizerBase`.",
            "spof": false
          },
          {
            "path": "tests/test_tokenizers_backend_mixin.py",
            "description": "This file contains a mixin class for testing the core functionality of the `transformers` library's tokenizer backend. It specifically verifies the correctness of alignment methods between words, tokens, and characters.",
            "spof": true
          },
          {
            "path": "tests/test_sentencepiece_backend_mixin.py",
            "description": "This file defines a mixin class (`SentencePieceBackendTesterMixin`) for testing the functionalities of SentencePiece tokenizers within the Transformers library. It includes various tests for tokenization, decoding, saving, and handling added tokens for both Python and Rust backends.",
            "spof": true
          },
          {
            "path": "tests/test_sequence_feature_extraction_common.py",
            "description": "This file defines a common test mixin for sequence feature extractors within the Hugging Face Transformers library. It provides shared test logic to verify fundamental functionalities like common properties, batching, padding, and truncation for sequence-based feature extraction classes.",
            "spof": true
          },
          {
            "path": "tests/test_tokenization_common.py",
            "description": "This file contains common utilities, helper functions, and test mixins used across various tokenization tests within the Hugging Face Transformers library. It provides tools for testing tokenizer behavior, handling caches, filtering models, and extracting information from tokenizer configuration files.",
            "spof": true
          },
          {
            "path": "tests/utils/test_chat_parsing_utils.py",
            "description": "This file contains unit tests for the `chat_parsing_utils` module, specifically verifying the functionality of parsing various model-specific chat output schemas and integrating this parsing into `AutoTokenizer`.",
            "spof": true
          },
          {
            "path": "tests/utils/test_convert_slow_tokenizer.py",
            "description": "This file contains unit tests for the `SpmConverter` class, specifically verifying that a warning is issued when converting a SentencePiece tokenizer that uses byte fallback, as this feature is not supported by fast tokenizers.",
            "spof": true
          },
          {
            "path": "tests/utils/test_tokenization_utils.py",
            "description": "This file contains unit tests for various tokenizer utility functions, including caching mechanisms when internet is unavailable and the ability to push and load tokenizers from the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "tests/tokenization/test_tokenization_utils.py",
            "description": "This file contains unit tests for various utility functions and core functionalities of tokenizers within the Hugging Face Transformers library, including pretrained tokenizer loading, batch encoding, padding, and decoding.",
            "spof": false
          },
          {
            "path": "tests/tokenization/test_tokenization_fast.py",
            "description": "This file contains unit tests for the `PreTrainedTokenizerFast` class in the Hugging Face Transformers library. It verifies functionalities such as tokenizer saving, loading, training, and proper handling of special tokens, padding, and truncation settings.",
            "spof": false
          },
          {
            "path": "tests/models/albert/test_tokenization_albert.py",
            "description": "This file contains unit tests for the `AlbertTokenizer` in the Hugging Face Transformers library, ensuring its correct functionality for tokenization and decoding using SentencePiece.",
            "spof": true
          },
          {
            "path": "tests/models/bartpho/test_tokenization_bartpho.py",
            "description": "This file contains unit tests for the BartphoTokenizer class, ensuring its tokenization, vocabulary handling, and special token functionality work as expected. It extends common tokenizer test functionalities provided by Hugging Face's testing utilities.",
            "spof": true
          },
          {
            "path": "tests/models/auto/test_tokenization_auto.py",
            "description": "This file contains unit tests for the `AutoTokenizer` class in the Hugging Face Transformers library, verifying its ability to load and instantiate various tokenizer types from pretrained models and configurations, including fast tokenizers.",
            "spof": false
          },
          {
            "path": "tests/models/bert/test_tokenization_bert.py",
            "description": "This file contains unit tests for the `BertTokenizer` class within the HuggingFace Transformers library, verifying its tokenization, de-tokenization, and overall behavior.",
            "spof": true
          },
          {
            "path": "tests/models/bertweet/test_tokenization_bertweet.py",
            "description": "This file contains unit tests for the BertweetTokenizer, ensuring its tokenization, vocabulary loading, and special token handling functionalities work as expected within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/big_bird/test_tokenization_big_bird.py",
            "description": "This file contains unit tests for the `BigBirdTokenizer` in the HuggingFace transformers library. It verifies the correct tokenization and de-tokenization behavior for the BigBird model, including various edge cases and integration tests.",
            "spof": true
          },
          {
            "path": "tests/models/barthez/test_tokenization_barthez.py",
            "description": "This file contains unit and integration tests for the `BarthezTokenizer` within the Hugging Face Transformers library. It verifies the correct tokenization and detokenization behavior of the Barthez model's tokenizer.",
            "spof": true
          },
          {
            "path": "tests/models/blenderbot/test_tokenization_blenderbot.py",
            "description": "This file contains unit tests for the `BlenderbotTokenizer` class, ensuring its tokenization, encoding, and decoding functionalities work as expected. It inherits from `TokenizerTesterMixin` and `unittest.TestCase` to leverage common tokenizer testing utilities.",
            "spof": false
          },
          {
            "path": "tests/models/blenderbot_small/test_tokenization_blenderbot_small.py",
            "description": "This file contains unit tests for the Blenderbot small tokenizer. It verifies the tokenizer's core functionality, special token handling, and encoding/decoding processes.",
            "spof": true
          },
          {
            "path": "tests/models/biogpt/test_tokenization_biogpt.py",
            "description": "This file contains unit tests for the `BioGptTokenizer` class, ensuring its tokenization, vocabulary, merging, and sequence building functionalities work as expected.",
            "spof": true
          },
          {
            "path": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
            "description": "This file contains unit tests for the `BertJapaneseTokenizer` and its underlying Japanese word tokenizers (Mecab, Sudachi, etc.) within the Transformers library. It verifies various tokenization functionalities and configurations.",
            "spof": false
          },
          {
            "path": "tests/models/bloom/test_tokenization_bloom.py",
            "description": "This file contains unit tests for the Bloom tokenizer within the Hugging Face Transformers library. It verifies the tokenizer's encoding, decoding, padding, and save/load functionalities, including integration tests with a dataset.",
            "spof": false
          },
          {
            "path": "tests/models/camembert/test_tokenization_camembert.py",
            "description": "This file contains unit tests for the Camembert tokenizer, verifying its tokenization, encoding, and decoding functionalities against expected outputs.",
            "spof": true
          },
          {
            "path": "tests/models/canine/test_tokenization_canine.py",
            "description": "This file contains unit tests for the `CanineTokenizer` class, verifying its tokenization, special token handling, and serialization functionalities.",
            "spof": true
          },
          {
            "path": "tests/models/cohere/test_tokenization_cohere.py",
            "description": "This file contains unit tests for the Cohere tokenizer within the Hugging Face Transformers library, verifying its tokenization, special token handling, and template application functionalities.",
            "spof": false
          },
          {
            "path": "tests/models/code_llama/test_tokenization_code_llama.py",
            "description": "This file contains unit tests for the CodeLlamaTokenizer, ensuring its tokenization, saving/loading, and special token handling functionalities work as expected.",
            "spof": true
          },
          {
            "path": "tests/models/byt5/test_tokenization_byt5.py",
            "description": "This file contains unit tests for the ByT5 tokenizer, verifying its tokenization, encoding, decoding, and integration functionalities within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/cpm/test_tokenization_cpm.py",
            "description": "This file contains unit tests for the CPM tokenizer (`CpmTokenizer`) in the Hugging Face Transformers library. It verifies the correct functioning of tokenization, detokenization, and pre-tokenization steps for the CPM model.",
            "spof": false
          },
          {
            "path": "tests/models/deberta/test_tokenization_deberta.py",
            "description": "This file contains unit tests for the Deberta tokenizer within the Hugging Face Transformers library, verifying its tokenization, ID conversion, and decoding functionalities.",
            "spof": true
          },
          {
            "path": "tests/models/deberta_v2/test_tokenization_deberta_v2.py",
            "description": "This file contains unit tests for the DebertaV2 tokenizer, verifying its tokenization behavior under various configurations such as lowercasing and punctuation splitting.",
            "spof": true
          },
          {
            "path": "tests/models/dia/test_tokenization_dia.py",
            "description": "This file contains unit tests for the `DiaTokenizer` in the HuggingFace Transformers library. It verifies the tokenizer's core functionalities such as token conversion, vocabulary handling, and full tokenization processes.",
            "spof": true
          },
          {
            "path": "tests/models/ctrl/test_tokenization_ctrl.py",
            "description": "This file contains unit tests for the `CTRLTokenizer` class, verifying its tokenization, vocabulary, and merge file handling logic within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/cpmant/test_tokenization_cpmant.py",
            "description": "This file contains unit tests for the `CpmAntTokenizer` from the Hugging Face Transformers library. It verifies the tokenizer's pre-tokenization, token-to-ID conversion, and decoding functionalities for the CPM-Ant model.",
            "spof": false
          },
          {
            "path": "tests/models/dpr/test_tokenization_dpr.py",
            "description": "This file contains unit tests for the tokenization functionalities of DPR (Dense Passage Retriever) model components, including ContextEncoder, QuestionEncoder, and Reader tokenizers. It extends BERT tokenization tests and includes specific tests for DPRReader's span decoding and encoding methods.",
            "spof": false
          },
          {
            "path": "tests/models/flaubert/test_tokenization_flaubert.py",
            "description": "This file contains unit tests for the HuggingFace Flaubert tokenizer, ensuring its correct functionality for tasks like tokenization and sequence building.",
            "spof": false
          },
          {
            "path": "tests/models/funnel/test_tokenization_funnel.py",
            "description": "This file contains unit tests for the `FunnelTokenizer` class within the Hugging Face Transformers library. It verifies the tokenization, de-tokenization, and ID conversion functionalities of the Funnel model's tokenizer.",
            "spof": true
          },
          {
            "path": "tests/models/gemma/test_tokenization_gemma.py",
            "description": "This file contains unit tests for the `GemmaTokenizer` class, verifying its tokenization, encoding, and decoding functionalities, including handling special characters and specific language examples.",
            "spof": true
          },
          {
            "path": "tests/models/fsmt/test_tokenization_fsmt.py",
            "description": "This file contains unit tests for the FSMT tokenizer in the Hugging Face Transformers library. It verifies the tokenizer's functionality, including configuration loading, tokenization, special token handling, and encoding/decoding consistency.",
            "spof": true
          },
          {
            "path": "tests/fixtures/tests_samples/wiki_text",
            "description": "This directory is intended to house sample text data, specifically from Wikipedia, which would be used as fixtures for testing purposes within the `transformers` library. Despite currently being empty, its path indicates its role as a placeholder for such test assets.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/wmt_en_ro",
            "description": "This directory is intended to store fixtures or sample data specifically for testing functionalities related to the WMT English-Romanian dataset within the `transformers` library. Although currently empty, its path indicates it would house test-specific files for this particular dataset configuration.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/swag",
            "description": "This directory is intended to house test samples or fixtures specifically related to the SWAG (Situations With Adversarial Generations) dataset or model within the Hugging Face Transformers library. Its purpose is to provide necessary data for testing SWAG-related functionalities.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/conll",
            "description": "This directory is intended to store test samples formatted according to the CoNLL standard. These samples would typically be used as fixtures for various tests within the `transformers` library, particularly for tasks involving sequence labeling or parsing. Although currently empty, its purpose is to provide standardized CoNLL data for testing purposes.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/MRPC",
            "description": "This directory serves as a placeholder for test samples related to the MRPC (Microsoft Research Paraphrase Corpus) within the `transformers` library's testing suite. Although currently empty, it is intended to house specific input/output data or configurations required for MRPC-focused tests.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/STS-B",
            "description": "This directory is intended to contain specific sample data or configurations for the STS-B (Semantic Textual Similarity Benchmark) dataset, utilized within the `transformers` library's testing suite. Although currently empty, its path indicates it serves as a placeholder or potential location for test fixtures related to STS-B tasks.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/SQUAD",
            "description": "This directory is designated to contain test fixtures and sample data for SQUAD (Stanford Question Answering Dataset) related tests within the `transformers` library. It acts as a dedicated location for test-specific inputs or expected outputs concerning SQUAD functionalities, even if currently empty.",
            "spof": false
          },
          {
            "path": "src/transformers/tokenization_utils_sentencepiece.py",
            "description": "This file defines the `SentencePieceBackend` class, which handles SentencePiece-based tokenization for the HuggingFace Transformers library, including loading models, managing vocabulary, and processing tokens.",
            "spof": true
          },
          {
            "path": "src/transformers/tokenization_python.py",
            "description": "This file provides core tokenization classes and utilities for implementing pure Python ('slow') tokenizers within the Hugging Face Transformers library. It includes the `Trie` data structure, essential for efficiently splitting text based on added special tokens.",
            "spof": true
          },
          {
            "path": "src/transformers/tokenization_utils_base.py",
            "description": "This file defines base classes and utility functions for tokenization in the Transformers library, supporting both slow and fast tokenizers. It includes definitions for `BatchEncoding`, `AddedToken`, truncation strategies, and type aliases for various tokenizer inputs and outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/tokenization_utils_tokenizers.py",
            "description": "This file provides tokenization classes for fast tokenizers, wrapping HuggingFace's `tokenizers` library. It handles the integration, serialization, and conversion of various tokenizer formats to the `tokenizers.Tokenizer` backend.",
            "spof": true
          },
          {
            "path": "src/transformers/tokenization_mistral_common.py",
            "description": "This file implements a wrapper class, `MistralCommonBackend`, to integrate Mistral AI's official `mistral-common` tokenizer library into the Hugging Face Transformers framework, enabling the use of Mistral AI tokenizers with the standard `PreTrainedTokenizerBase` API.",
            "spof": true
          },
          {
            "path": "src/transformers/data/__init__.py",
            "description": "This file serves as the main entry point for the `transformers.data` module, aggregating and exposing various utilities for data processing, collation, and metric computation for different NLP tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/data/data_collator.py",
            "description": "This file defines various data collators for the Hugging Face Transformers library, used to prepare batches of data for model training by handling tasks such as dynamic padding and converting data into PyTorch tensors or NumPy arrays.",
            "spof": false
          },
          {
            "path": "src/transformers/data/processors/xnli.py",
            "description": "This file defines the `XnliProcessor` class for loading and preparing the XNLI (Cross-lingual Natural Language Inference) dataset, including methods for retrieving training and testing examples and defining labels. It also provides utility mappings for XNLI processors, output modes, and label counts.",
            "spof": false
          },
          {
            "path": "src/transformers/data/processors/__init__.py",
            "description": "This file serves as the package initializer for the `processors` module, exposing various data processors, examples, and utility functions for common NLP tasks like GLUE, SQuAD, and XNLI within the Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/data/processors/glue.py",
            "description": "This file contains data processors and conversion utilities for various GLUE (General Language Understanding Evaluation) benchmark tasks. It defines how to prepare data for tasks like MRPC, MNLI, and CoLA within the Hugging Face Transformers library, though it includes a deprecation warning.",
            "spof": false
          },
          {
            "path": "src/transformers/data/processors/utils.py",
            "description": "This file provides utility classes (`InputExample`, `InputFeatures`) and a base `DataProcessor` for preparing text data for machine learning models, with a specific implementation (`SingleSentenceClassificationProcessor`) for single-sentence classification tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/data/processors/squad.py",
            "description": "This file contains utilities and functions for processing SQuAD (Stanford Question Answering Dataset) examples, converting them into features suitable for training and evaluation with transformer models.",
            "spof": false
          },
          {
            "path": "src/transformers/data/datasets/squad.py",
            "description": "This file defines a PyTorch Dataset for the SQuAD (Stanford Question Answering Dataset) task, handling data loading, preprocessing, and caching for training and evaluation with question answering models.",
            "spof": false
          },
          {
            "path": "src/transformers/data/datasets/glue.py",
            "description": "This file defines the `GlueDataset` class, a PyTorch Dataset for the GLUE benchmark, responsible for loading, processing, and caching data for various GLUE tasks, along with `GlueDataTrainingArguments` for configuration. It facilitates preparing data for training and evaluation of models on GLUE tasks within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/sentencepiece_model_pb2_new.py",
            "description": "This file is a generated Protocol Buffer definition for SentencePiece models. It defines the data structures used for SentencePiece training specifications, normalizer specifications, and the overall model proto.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/sentencepiece_model_pb2.py",
            "description": "This file is a Python output generated by the Protocol Buffer compiler from 'sentencepiece_model.proto'. It defines the data structures (messages and enums) used to represent SentencePiece models, their training specifications, and normalization rules.",
            "spof": false
          },
          {
            "path": "src/transformers/models/albert/tokenization_albert.py",
            "description": "This file implements the fast tokenizer for the ALBERT model, utilizing the HuggingFace `tokenizers` library for efficient tokenization.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bart/tokenization_bart.py",
            "description": "This file provides compatibility shims for BART tokenizers in `transformers` v5. It aliases `BartTokenizer` and `BartTokenizerFast` to `RobertaTokenizer` to maintain the public API, as BART uses the same byte-level BPE tokenizer as RoBERTa.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert/tokenization_bert_legacy.py",
            "description": "This file implements a legacy BERT tokenizer, `BertTokenizerLegacy`, based on the WordPiece algorithm. It provides functionalities for loading vocabulary, tokenizing text, converting between tokens and IDs, and constructing model inputs with special tokens for BERT-like models.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert/tokenization_bert.py",
            "description": "This file defines the `BertTokenizer` class, which is a fast tokenizer for BERT models. It leverages HuggingFace's `tokenizers` library to implement WordPiece tokenization, including handling special tokens, lowercasing, and Chinese characters.",
            "spof": true
          },
          {
            "path": "src/transformers/models/barthez/tokenization_barthez.py",
            "description": "This file implements the `BarthezTokenizer` class, providing a fast tokenizer for the BARThez model based on SentencePiece. It handles tokenization logic, special tokens, and integrates with the `tokenizers` library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "description": "This file implements the fast tokenizer for the Big Bird model, leveraging HuggingFace's `tokenizers` library to provide efficient tokenization based on the Unigram model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "description": "This file implements the `BertJapaneseTokenizer` class, providing specialized tokenization for Japanese text. It supports various word and subword tokenization strategies, including MeCab, Sudachi, Jumanpp, WordPiece, Character, and SentencePiece.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bertweet/tokenization_bertweet.py",
            "description": "This file implements the BERTweet tokenizer, which is based on Byte-Pair-Encoding (BPE) and includes specialized normalization steps for processing tweet text. It handles tweet-specific elements like mentions, URLs, and emojis.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bertweet/__init__.py",
            "description": "This `__init__.py` file sets up lazy loading for the `bertweet` model's tokenization module within the `transformers` library. It defers the import of `tokenization_bertweet` until it is explicitly accessed, improving initial import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bartpho/tokenization_bartpho.py",
            "description": "This file implements the `BartphoTokenizer` class, providing tokenization functionalities for the BARTpho-syllable model. It adapts the SentencePiece tokenizer for Vietnamese, including handling special tokens and vocabulary files specific to BARTpho.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blenderbot_small/tokenization_blenderbot_small.py",
            "description": "This file defines the BlenderbotSmallTokenizer, a BPE-based tokenizer specifically designed for the Blenderbot-90M model. It handles tokenization, converts tokens to IDs, and manages vocabulary and merge files.",
            "spof": false
          },
          {
            "path": "src/transformers/models/biogpt/tokenization_biogpt.py",
            "description": "This file implements the tokenization logic for the BioGPT model. It defines the BioGptTokenizer class, which combines Moses tokenization with Byte-Pair Encoding (BPE).",
            "spof": false
          },
          {
            "path": "src/transformers/models/blenderbot/tokenization_blenderbot.py",
            "description": "This file defines the `BlenderbotTokenizer` class, a fast byte-level Byte-Pair-Encoding tokenizer for Blenderbot models, built on HuggingFace's `tokenizers` library. It handles special tokens and specific considerations for prefix spaces during tokenization.",
            "spof": true
          },
          {
            "path": "src/transformers/models/canine/tokenization_canine.py",
            "description": "This file implements the `CanineTokenizer` class, a character-level tokenizer for the CANINE model. It converts text into sequences of Unicode code points, handling special tokens defined by the CANINE architecture.",
            "spof": true
          },
          {
            "path": "src/transformers/models/byt5/tokenization_byt5.py",
            "description": "This file implements the ByT5Tokenizer, which uses raw UTF-8 byte encoding for tokenization. It defines methods for converting text to and from byte-level tokens, handling special tokens, and managing the vocabulary for the ByT5 model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/camembert/tokenization_camembert.py",
            "description": "This file defines the 'fast' CamembertTokenizer, which is an optimized tokenizer for the CamemBERT model, implemented using HuggingFace's tokenizers library and based on the Unigram model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere/tokenization_cohere.py",
            "description": "This file defines the `CohereTokenizer` class within the Hugging Face Transformers library, responsible for handling the tokenization logic specific to Cohere models. It uses byte-level Byte-Pair-Encoding (BPE) and manages special tokens, pre-tokenization, and decoding processes.",
            "spof": false
          },
          {
            "path": "src/transformers/models/code_llama/tokenization_code_llama.py",
            "description": "This file implements the CodeLlama tokenizer using a byte-level Byte-Pair-Encoding (BPE) approach. It includes specific handling for special tokens used in infilling tasks and a default system prompt.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cpmant/tokenization_cpmant.py",
            "description": "This file defines the `CpmAntTokenizer` class, which is responsible for tokenizing text for the CPMAnt model. It utilizes a Wordpiece tokenizer and integrates with the `rjieba` library for Chinese text segmentation, converting text into a sequence of tokens that can be understood by the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convbert/tokenization_convbert.py",
            "description": "This file defines the `ConvBertTokenizer` class, which is a tokenizer for the ConvBERT model. It inherits functionality from the `BertTokenizer`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cpm/tokenization_cpm_fast.py",
            "description": "This file implements the fast tokenizer for CPM models, `CpmTokenizerFast`. It uses Jieba-RS for Chinese text segmentation and SentencePiece for tokenization, building on `PreTrainedTokenizerFast` from the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cpm/tokenization_cpm.py",
            "description": "This file implements the CpmTokenizer, a tokenization class for CPM models. It leverages SentencePiece and Jieba-RS for efficient text processing and token conversion.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ctrl/tokenization_ctrl.py",
            "description": "This file implements the `CTRLTokenizer` class, which is a Byte-Pair-Encoding (BPE) tokenizer for the Salesforce CTRL model. It handles tokenization, conversion between tokens and IDs, and includes specific control codes relevant to the CTRL model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "description": "This file implements the DeBERTa-v2 tokenizer using HuggingFace's tokenizers library and Unigram tokenization. It defines the tokenization logic, including normalization and pre-tokenization steps, for the DeBERTa-v2 model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deberta/tokenization_deberta.py",
            "description": "This file implements a fast tokenizer for the DeBERTa model using Hugging Face's `tokenizers` library, based on byte-level Byte-Pair-Encoding.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dpr/tokenization_dpr.py",
            "description": "This file provides tokenization classes for the Dense Passage Retrieval (DPR) models, including specialized tokenizers for context and question encoders that inherit from BertTokenizer, and utilities for processing input for DPR reader models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpr/tokenization_dpr_fast.py",
            "description": "This file implements 'fast' tokenizers for DPR Context Encoder and Question Encoder models, leveraging HuggingFace's `tokenizers` library. It also provides a custom tokenizer mixin for DPR Reader, handling complex input combinations of questions, titles, and texts, and methods for decoding best spans from reader outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie4_5/convert_ernie4_5_tokenizer.py",
            "description": "This script converts a Llama-based tokenizer, configured with specific special tokens and a chat template for ERNIE 4.5, from its 'slow' version to a 'fast' tokenizer, saving it locally or pushing it to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "src/transformers/models/flaubert/tokenization_flaubert.py",
            "description": "This file implements the tokenizer for the Flaubert model, leveraging Byte-Pair Encoding and Moses for text preprocessing. It handles the loading of vocabulary and merge files to convert text into token IDs for the Flaubert model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fnet/tokenization_fnet.py",
            "description": "This file defines the FNetTokenizer class for the FNet model, which is based on and inherits most of its functionality from the AlbertTokenizer. It provides tokenization classes for processing text input for FNet models within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fsmt/tokenization_fsmt.py",
            "description": "This file defines the FSMTTokenizer class, an implementation of a Byte-Pair Encoding (BPE) tokenizer for the FAIRSEQ Transformer (FSMT) model, which includes Moses preprocessing and handles source and target language vocabularies.",
            "spof": false
          },
          {
            "path": "src/transformers/models/funnel/tokenization_funnel.py",
            "description": "This file implements the `FunnelTokenizer` class, which is responsible for tokenizing text for the Funnel Transformer model. It uses the HuggingFace `tokenizers` library and the WordPiece algorithm.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma/tokenization_gemma.py",
            "description": "This file implements the fast tokenizer for the Gemma model, leveraging HuggingFace's tokenizers library with a BPE model, byte fallback, and specific normalization rules. It provides a `GemmaTokenizer` class to handle tokenization for Gemma-based models.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Ita Zaporozhets",
            "percent": 64
          },
          {
            "name": "Julien Denize",
            "percent": 10
          },
          {
            "name": "Arthur",
            "percent": 5
          }
        ]
      },
      "Encoder-Only Model Architectures": {
        "files": [
          {
            "path": "examples/modular-transformers/modular_roberta.py",
            "description": "This file defines custom RobertaEmbeddings and RobertaModel classes, extending the base BertEmbeddings and BertModel from the Hugging Face Transformers library to adapt them for a modular Roberta architecture.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_dummy_bert.py",
            "description": "This file defines the core modeling components (embeddings, self-attention, cross-attention) for a dummy BERT model. It is an automatically generated example within the `transformers` library, likely used for testing or demonstrating modular model implementations.",
            "spof": false
          },
          {
            "path": "examples/modular-transformers/configuration_duplicated_method.py",
            "description": "This file defines the configuration class `DuplicatedMethodConfig` for a transformer-based model, specifying its architectural parameters and initialization settings. It is an auto-generated file, duplicated from a modular source.",
            "spof": false
          },
          {
            "path": "examples/modular-transformers/modeling_roberta.py",
            "description": "This file defines core components like embeddings and attention mechanisms for a RoBERTa model. It is an auto-generated implementation derived from a modular RoBERTa definition within the Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bert-japanese.md",
            "description": "This file provides documentation for the `BertJapanese` model, a BERT variant specifically trained for Japanese text. It details the model's two tokenization methods (MeCab and character-based) and includes usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/albert.md",
            "description": "This document provides a comprehensive overview of the ALBERT model, detailing its architecture, key features, usage examples, and API references within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/canine.md",
            "description": "This file provides comprehensive documentation for the CANINE model within the Hugging Face Transformers library. It describes the model's architecture, features, usage examples, and API references for its various components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/bert.md",
            "description": "This file provides comprehensive documentation for the BERT model, including its theoretical background, usage examples with Hugging Face Transformers (Pipeline, AutoModel, CLI), and API references for its various components like `BertConfig`, `BertTokenizer`, and different `BertFor` tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/camembert.md",
            "description": "This file provides documentation for the CamemBERT model in Hugging Face Transformers, detailing its purpose, common applications, and providing code examples for usage and quantization. It also includes API references for various CamemBERT classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bertweet.md",
            "description": "This document provides an overview and usage guide for the BERTweet model within the Hugging Face Transformers library. It details the model's characteristics, its suitability for tweet-related tasks, and provides code examples for its implementation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/big_bird.md",
            "description": "This file provides comprehensive documentation for the BigBird transformer model within the Hugging Face Transformers library. It explains the model's architecture, its benefits for long sequence processing, and demonstrates its usage with various code examples and available classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/deberta.md",
            "description": "This file provides documentation for the DeBERTa model, explaining its architecture, key features, and offering code examples for its usage within the Hugging Face Transformers library. It also details the various DeBERTa-related classes and their functionalities.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/deberta-v2.md",
            "description": "This file is a documentation page for the DeBERTa-v2 model in the Hugging Face Transformers library, detailing its features, improvements, and providing usage examples for various tasks including quantization. It also includes API references for DeBERTa-v2 specific classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/convbert.md",
            "description": "This document provides an overview and API reference for the ConvBERT model within the Hugging Face Transformers library, including its architecture, usage tips, and links to relevant tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/electra.md",
            "description": "This file provides comprehensive documentation for the ELECTRA model within the Hugging Face Transformers library, including its architecture, efficiency, usage examples, and API references for related classes and methods.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/ernie4_5.md",
            "description": "This file documents the Ernie 4.5 model within the Hugging Face Transformers library. It provides an overview of the model, usage examples for text generation, and references to its specific configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/funnel.md",
            "description": "This file provides comprehensive documentation for the Funnel Transformer model, including an overview, usage tips, and API references for its various components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/ernie.md",
            "description": "This documentation file describes the ERNIE (Enhanced Representation through kNowledge IntEgration) family of models within the Hugging Face Transformers library. It explains their features, provides usage examples for different tasks, lists available model variants, and details the API for ERNIE-related classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/herbert.md",
            "description": "This file provides documentation for the HerBERT model, a BERT-based language model for Polish, including its overview, usage examples, and details on its tokenizers.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/flaubert.md",
            "description": "This file provides documentation for the FlauBERT model, detailing its overview, pre-training, and available classes within the Hugging Face Transformers library. It serves as a comprehensive guide for understanding and using FlauBERT for French NLP tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/mluke.md",
            "description": "This file provides documentation for the mLUKE model within the Hugging Face Transformers library, including an overview of the model, its usage tips, and details about its specific tokenizer, MLukeTokenizer.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/luke.md",
            "description": "This file is a documentation page for the LUKE model within the Hugging Face Transformers library, detailing its architecture, entity-aware self-attention mechanism, specialized head models for various entity-related tasks, and providing usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/ibert.md",
            "description": "This file provides documentation for the I-BERT model within the Hugging Face Transformers library. It describes I-BERT's architecture, its purpose as an integer-only quantized RoBERTa for faster inference, and details its various classes and functionalities.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mobilebert.md",
            "description": "This file documents the MobileBERT model within the Hugging Face Transformers library, providing an overview of its architecture, features, and usage examples. It also includes auto-generated documentation for various MobileBERT-specific classes and functions.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/megatron-bert.md",
            "description": "This file provides documentation for the MegatronBERT model within the Hugging Face Transformers library, including its overview, instructions for checkpoint conversion, and API reference for various MegatronBert classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/nystromformer.md",
            "description": "This file provides documentation for the Nystr√∂mformer model within the Hugging Face Transformers library, including an overview of the model, its capabilities, and API references for its configurations and various task-specific implementations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/modernbert.md",
            "description": "This document provides an overview and usage guide for the ModernBERT model within the Hugging Face Transformers library, detailing its features, offering code examples, and serving as an API reference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/longformer.md",
            "description": "This file provides documentation for the Longformer model within the Hugging Face Transformers library, detailing its architecture, usage examples, and API references for its configurations, tokenizers, and various task-specific models.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mpnet.md",
            "description": "This file provides documentation for the MPNet model within the Hugging Face Transformers library. It includes an overview, proposed method, usage tips, and references to its configurations, tokenizers, and various task-specific model implementations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/rembert.md",
            "description": "This file provides documentation for the RemBERT model within the Hugging Face Transformers library, detailing its overview, usage, and various associated classes like configurations, tokenizers, and model implementations for different tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/phobert.md",
            "description": "This file provides documentation for the PhoBERT model, including an overview, its origins, performance, and usage examples within the Hugging Face Transformers library. It also details the PhobertTokenizer.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/roberta-prelayernorm.md",
            "description": "This file provides documentation for the RoBERTa-PreLayerNorm model within the Hugging Face Transformers library. It details the model's overview, usage tips, and various task-specific implementations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/roberta.md",
            "description": "This document provides comprehensive documentation for the RoBERTa model within the Hugging Face Transformers library, including its background, usage examples for masked language modeling, and API references for its configurations, tokenizers, and various task-specific models.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/roc_bert.md",
            "description": "This file provides documentation for the RoCBert model within the Hugging Face Transformers library. It describes the model's features, usage examples for various tasks, and API references for its components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/squeezebert.md",
            "description": "This file provides documentation for the SqueezeBERT model within the Hugging Face Transformers library. It details the model's architecture, its origin, usage tips, and links to relevant classes and tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/roformer.md",
            "description": "This file provides documentation for the RoFormer model within the Hugging Face Transformers library, detailing its architecture, usage examples for various tasks, and API references for its related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/splinter.md",
            "description": "This file provides documentation for the Splinter model, including an overview, usage tips, and details on its configuration, tokenizer, and specific model implementations within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/tapas.md",
            "description": "This file provides comprehensive documentation for the TAPAS model within the Hugging Face Transformers library, detailing its architecture, pre-training, fine-tuning processes, and usage tips for question answering over tabular data.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/xlm-v.md",
            "description": "This document provides an overview of the XLM-V multilingual language model, detailing its architecture, improvements over XLM-R, and usage tips within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/xlnet.md",
            "description": "This file provides comprehensive documentation for the XLNet model within the Hugging Face Transformers library, detailing its architecture, usage, and available classes like configurations, tokenizers, and various model implementations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/xlm-roberta-xl.md",
            "description": "This file provides the documentation for the XLM-RoBERTa-XL model, detailing its capabilities, offering usage examples with `Pipeline` and `AutoModel`, demonstrating quantization, and listing available model classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/xlm-roberta.md",
            "description": "This file is a documentation page for the XLM-RoBERTa model, providing an overview, usage examples, quantization instructions, task-specific resources, and API references within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/xlm.md",
            "description": "This file is a documentation page for the XLM model within the Hugging Face Transformers library, detailing its features, usage examples, and API references for its various components like configuration, tokenizer, and specific model classes.",
            "spof": false
          },
          {
            "path": "docs/source/zh/model_doc/bert.md",
            "description": "This file provides the Chinese documentation for the BERT model within the Hugging Face Transformers library, explaining its architecture, pre-training objectives, and demonstrating its usage with code examples. It also includes API references for various BERT-related classes.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/bert-japanese.md",
            "description": "This documentation file, written in Japanese, describes the BertJapanese model, including its two tokenization methods (MeCab/WordPiece and character-based), required dependencies, and provides code examples for its usage within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bertweet.md",
            "description": "This file is a Japanese documentation page for the BERTweet model within the Hugging Face Transformers library, providing an overview, usage examples, and details on its tokenizer.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/albert.md",
            "description": "This file provides the Japanese documentation for the ALBERT model within the Hugging Face Transformers library, detailing its architecture, usage, and related classes.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bert.md",
            "description": "This file provides the Japanese documentation for the BERT model within the Hugging Face Transformers library. It includes an overview, usage tips, and resources for various tasks such as text classification, token classification, masked language modeling, and question answering.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/deberta-v2.md",
            "description": "This file is a Japanese documentation page for the DeBERTa-v2 model, providing an overview of its architecture, improvements, and details on its configuration, tokenizer, and various task-specific model implementations.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/canine.md",
            "description": "This file provides the Japanese documentation for the CANINE model within the Hugging Face Transformers library, covering its overview, usage examples, and API references for various tasks.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/camembert.md",
            "description": "This file is the Japanese documentation page for the CamemBERT model within the Transformers library. It provides an overview of the model, relevant resources, and API references for its various classes like CamembertModel, CamembertTokenizer, and specialized task heads.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/big_bird.md",
            "description": "This file provides the Japanese documentation for the Hugging Face BigBird model, including an overview of its architecture, key features for handling long sequences, usage tips, and API references for its various components and task-specific models.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/convbert.md",
            "description": "This file provides the Japanese-language documentation for the ConvBERT model within the HuggingFace Transformers library, detailing its architecture, research paper, usage tips, and API references for its various components and task-specific models.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/deberta.md",
            "description": "This document provides an overview of the DeBERTa model, including its architecture, key features, and research paper summary, in Japanese. It also lists resources, usage examples for various NLP tasks, and API references for different DeBERTa classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/bertweet.md",
            "description": "This file is a Korean-language documentation for the BERTweet model, providing an overview, usage examples, and details on its tokenizer within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/bert.md",
            "description": "This file provides Korean-language documentation for the BERT model, including an overview, usage tips, and relevant resources for its application within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/albert.md",
            "description": "This document provides the Korean-language documentation for the ALBERT model within the Hugging Face Transformers library, explaining its architecture, features, and usage examples for various NLP tasks.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/big_bird.md",
            "description": "This file provides the Korean documentation for the BigBird transformer model within the Hugging Face Transformers library. It describes the model's architecture, capabilities, usage examples, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/deberta-v2.md",
            "description": "This file provides Korean-language documentation for the DeBERTa-v2 model, detailing its overview, improvements over DeBERTa v1, and API references for its configuration, tokenizer, and various task-specific models within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/electra.md",
            "description": "This file provides Korean documentation for the ELECTRA model within the Hugging Face Transformers library. It details the model's architecture, pre-training method, usage tips, and lists associated classes and functions.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/convbert.md",
            "description": "This file provides the Korean language documentation for the Hugging Face Transformers library's ConvBERT model, including an overview, usage tips, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/bert-japanese.md",
            "description": "This document provides Korean-language documentation for the Japanese BERT model, detailing its two tokenization methods (MeCab/WordPiece and character-based) and offering usage examples for each.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/roberta.md",
            "description": "This document provides Korean-language documentation for the RoBERTa model within the Hugging Face Transformers library. It details the model's overview, usage tips, and resources for various NLP tasks.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/deberta.md",
            "description": "This file provides Korean-language documentation for the DeBERTa model in the Hugging Face Transformers library, including an overview, usage resources, and API references for its various components.",
            "spof": false
          },
          {
            "path": "tests/models/albert/test_modeling_albert.py",
            "description": "This file contains unit tests for the Albert model and its various task-specific heads (e.g., pretraining, masked LM, question answering, sequence classification) within the Hugging Face Transformers library, ensuring its correct functionality and integration.",
            "spof": false
          },
          {
            "path": "tests/models/bert/test_modeling_bert.py",
            "description": "This file contains unit tests for the BERT model implementations within the Hugging Face Transformers library, covering various configurations and functionalities of BERT models.",
            "spof": true
          },
          {
            "path": "tests/models/camembert/test_modeling_camembert.py",
            "description": "This file contains integration tests for the `CamembertModel` within the Hugging Face Transformers library, specifically verifying its output embeddings with different attention implementations.",
            "spof": false
          },
          {
            "path": "tests/models/canine/test_modeling_canine.py",
            "description": "This file contains a testing suite for the PyTorch implementation of the CANINE model, including tests for its base model and various task-specific heads like sequence classification, question answering, and token classification.",
            "spof": false
          },
          {
            "path": "tests/models/deberta/test_modeling_deberta.py",
            "description": "This file contains unit tests for the Deberta model in the Hugging Face Transformers library, including its various task-specific heads like Masked LM, Sequence Classification, Token Classification, and Question Answering. It defines a model tester to set up configurations and inputs for these tests.",
            "spof": false
          },
          {
            "path": "tests/models/deberta_v2/test_modeling_deberta_v2.py",
            "description": "This file contains unit tests for the DebertaV2 model and its various task-specific heads within the Hugging Face Transformers library. It ensures the correct functionality and integration of the DebertaV2 architecture for tasks like masked language modeling, sequence classification, token classification, question answering, and multiple choice.",
            "spof": false
          },
          {
            "path": "tests/models/data2vec/test_modeling_data2vec_text.py",
            "description": "This file contains the testing suite for the PyTorch Data2VecText model within the Hugging Face Transformers library. It includes a model tester class to verify the functionality and various configurations of Data2VecText models and their different heads (e.g., for masked LM, causal LM, sequence classification).",
            "spof": true
          },
          {
            "path": "tests/models/convbert/test_modeling_convbert.py",
            "description": "This file contains unit tests for the PyTorch ConvBERT model implementation within the Hugging Face Transformers library, covering various model configurations and task-specific heads.",
            "spof": false
          },
          {
            "path": "tests/models/electra/test_modeling_electra.py",
            "description": "This file contains unit tests for the Electra model and its various heads (e.g., for masked LM, causal LM, token classification, pretraining) within the Hugging Face Transformers library. It defines a model tester class to set up configurations and inputs, and then verifies the functionality and output shapes of different Electra models.",
            "spof": true
          },
          {
            "path": "tests/models/flaubert/test_modeling_flaubert.py",
            "description": "This file contains unit tests for the Flaubert model and its various task-specific heads within the Hugging Face Transformers library. It includes a `FlaubertModelTester` class to set up configurations and inputs, and then runs checks on models like FlaubertModel, FlaubertWithLMHeadModel, and FlaubertForQuestionAnswering.",
            "spof": false
          },
          {
            "path": "tests/models/funnel/test_modeling_funnel.py",
            "description": "This file contains unit tests for the Hugging Face Funnel model, including its various configurations and functionalities like pretraining, masked language modeling, sequence classification, and multiple choice tasks. It defines a `FunnelModelTester` class to facilitate these tests.",
            "spof": false
          },
          {
            "path": "tests/models/ernie/test_modeling_ernie.py",
            "description": "This file contains unit tests for the Hugging Face Transformers Ernie model, including its core components and various task-specific heads like Masked LM and Causal LM.",
            "spof": true
          },
          {
            "path": "tests/models/fnet/test_modeling_fnet.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch FNet model within the Hugging Face Transformers library. It defines various test configurations and checks the functionality of different FNet model types and their pipelines.",
            "spof": false
          },
          {
            "path": "src/transformers/modeling_layers.py",
            "description": "This file provides foundational classes for various NLP tasks within the Transformers library, including a utility for gradient checkpointing and generic model wrappers for sequence classification, question answering, and token classification.",
            "spof": true
          },
          {
            "path": "src/transformers/models/albert/__init__.py",
            "description": "This `__init__.py` file defines the `albert` model package within the Transformers library. It uses lazy-loading to import the model's configuration, modeling, and tokenizer components, optimizing import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/albert/configuration_albert.py",
            "description": "This file defines the `AlbertConfig` class, which is used to store and manage the architectural configuration parameters for the ALBERT model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/albert/modeling_albert.py",
            "description": "This file implements the core PyTorch architecture for the ALBERT (A Lite BERT) model, including its embeddings, attention mechanisms, and transformer layers.",
            "spof": false
          },
          {
            "path": "src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py",
            "description": "This script converts an ALBERT model checkpoint from a TensorFlow format to a PyTorch format. It loads weights from a TensorFlow checkpoint into a PyTorch model based on a specified configuration and then saves the PyTorch model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert/configuration_bert.py",
            "description": "This file defines the `BertConfig` class, which stores the configuration parameters for a BERT model within the Hugging Face Transformers library, specifying its architecture and hyperparameters. It allows for instantiating BERT models with various settings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bert/__init__.py",
            "description": "This file serves as the `__init__.py` for the BERT model within the `transformers` library, implementing lazy loading of its components (configuration, modeling, and tokenization) to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py",
            "description": "This script converts a pre-trained BERT model from its original TensorFlow checkpoint format into a PyTorch compatible model state dictionary.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert/modeling_bert.py",
            "description": "This file defines the PyTorch model architecture for BERT, including its core components such as embeddings, self-attention, and cross-attention mechanisms, as part of the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py",
            "description": "This script converts a head-less TensorFlow 2.x BERT model checkpoint to a PyTorch format, re-mapping variable names for compatibility with Hugging Face Transformers. It loads TF2.x weights and saves them as a PyTorch model state dictionary.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert/convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py",
            "description": "This script converts a TensorFlow checkpoint from the 'Token Dropping' implementation of BERT into a PyTorch-compatible `BertForMaskedLM` model. It loads weights from the TensorFlow checkpoint and maps them to the corresponding PyTorch model layers.",
            "spof": false
          },
          {
            "path": "src/transformers/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py",
            "description": "This script facilitates the conversion of BigBird model weights from a TensorFlow checkpoint to a PyTorch format. It supports both general BigBird models and those with a TriviaQA head, mapping TensorFlow variable names to their PyTorch counterparts.",
            "spof": true
          },
          {
            "path": "src/transformers/models/big_bird/configuration_big_bird.py",
            "description": "This file defines the `BigBirdConfig` class, which is used to store and manage the configuration parameters for instantiating a BigBird model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/big_bird/__init__.py",
            "description": "This `__init__.py` file for the BigBird model in the Transformers library uses lazy loading to import its configuration, modeling, and tokenization modules. It ensures that these components are only loaded when explicitly accessed, improving initial import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bert_japanese/__init__.py",
            "description": "This `__init__.py` file sets up lazy loading for the `bert_japanese` model components within the Hugging Face Transformers library. It primarily handles the import structure for the `bert_japanese` tokenizer.",
            "spof": true
          },
          {
            "path": "src/transformers/models/canine/__init__.py",
            "description": "Initializes the Canine model package, enabling lazy loading of its core components like configuration, modeling, and tokenization classes to optimize import performance within the HuggingFace Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/canine/convert_canine_original_tf_checkpoint_to_pytorch.py",
            "description": "This script converts a pre-trained TensorFlow CANINE model checkpoint into a PyTorch model, making it compatible with the Hugging Face Transformers library. It loads TensorFlow weights and transfers them to a PyTorch `CanineModel` and saves it along with the tokenizer.",
            "spof": true
          },
          {
            "path": "src/transformers/models/canine/configuration_canine.py",
            "description": "This file defines the `CanineConfig` class, which stores the configuration parameters for the CANINE model architecture within the Hugging Face Transformers library. It specifies various model hyperparameters and character-level processing settings for instantiating a CANINE model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/canine/modeling_canine.py",
            "description": "This file implements the core PyTorch model architecture for the CANINE (Character-level Neural Encoder) model, including its specialized embedding layers and character-to-molecule conversion mechanism.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blt/__init__.py",
            "description": "This `__init__.py` file defines the BLT model package, utilizing lazy loading for its configuration, modeling, and tokenization modules to optimize import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blt/convert_blt_weights_to_hf.py",
            "description": "This script converts the weights and configurations of a BLT model to be compatible with the Hugging Face Transformers library. It merges various configuration components and remaps state dictionary keys to the Hugging Face format.",
            "spof": true
          },
          {
            "path": "src/transformers/models/camembert/configuration_camembert.py",
            "description": "This file defines the `CamembertConfig` class, which specifies the architecture and hyper-parameters for the CamemBERT model in the Hugging Face Transformers library. It inherits from `PreTrainedConfig` and is used to instantiate CamemBERT models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/camembert/modular_camembert.py",
            "description": "This file implements PyTorch CamemBERT models by extending RoBERTa models for tasks such as Masked Language Modeling, Sequence Classification, and Multiple Choice, adapting the base RoBERTa architecture for CamemBERT-specific needs.",
            "spof": true
          },
          {
            "path": "src/transformers/models/camembert/modeling_camembert.py",
            "description": "This file implements the core neural network architecture components for the CamemBERT model, including embeddings and self-attention mechanisms, and is automatically generated from a modular source.",
            "spof": false
          },
          {
            "path": "src/transformers/models/camembert/__init__.py",
            "description": "This `__init__.py` file defines the `camembert` module, handling the lazy loading of its core components, including configuration, model, and tokenizer, to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/convbert/configuration_convbert.py",
            "description": "This file defines the `ConvBertConfig` class, which stores the configuration for a ConvBERT model, including hyperparameters and architecture details. It extends `PreTrainedConfig` and is used to instantiate and control ConvBERT models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch.py",
            "description": "This script converts an original TensorFlow 1.x ConvBERT model checkpoint to a PyTorch-compatible format. It loads weights from the TF checkpoint, maps them to a PyTorch ConvBERT model, and saves the resulting model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/convbert/modeling_convbert.py",
            "description": "This file implements the PyTorch ConvBERT model, including its embedding layer, separable convolutional attention mechanism, and the base pre-trained model class within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convbert/__init__.py",
            "description": "This `__init__.py` file defines the ConvBERT model's public API for the Hugging Face Transformers library. It uses lazy loading to import its configuration, modeling, and tokenization components, improving startup performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/data2vec/convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts an original Fairseq PyTorch checkpoint for the Data2Vec Text model into a Hugging Face Transformers PyTorch model, supporting both masked language model and sequence classification configurations.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "description": "This file defines the core modeling components, such as embeddings and self-attention layers, for the Data2VecText model in the Transformers library. It was automatically generated from a modular source file.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/modular_data2vec_text.py",
            "description": "This file implements various PyTorch models for Data2VecText, adapting the Roberta architecture for tasks such as causal language modeling (CLM) and masked language modeling (MLM).",
            "spof": true
          },
          {
            "path": "src/transformers/models/deberta_v2/__init__.py",
            "description": "Initializes the `deberta_v2` package within the transformers library, enabling lazy loading of its configuration, modeling, and tokenization components to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "description": "Implements the PyTorch DeBERTa-v2 model architecture, including its self-attention mechanism and relative position encoding. This file defines core components like `DisentangledSelfAttention` and utility functions for position embeddings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deberta_v2/configuration_deberta_v2.py",
            "description": "This file defines the `DebertaV2Config` class, which stores the configuration parameters for instantiating a DeBERTa-v2 model. It specifies architectural details such as vocabulary size, hidden layer dimensions, and attention head count.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deberta/__init__.py",
            "description": "This `__init__.py` file defines the structure of the DeBERTa model package within the Transformers library. It enables lazy loading of DeBERTa's configuration, modeling, and tokenization components to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deberta/configuration_deberta.py",
            "description": "This file defines the `DebertaConfig` class, which specifies the architectural configuration for the DeBERTa model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deberta/modeling_deberta.py",
            "description": "This file defines the PyTorch model architecture for DeBERTa, including custom layers such as DebertaLayerNorm, DebertaSelfOutput, and the core DisentangledSelfAttention module, along with utility functions for relative positional embeddings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py",
            "description": "This script converts original Dense Passage Retriever (DPR) model checkpoints, such as context encoder, question encoder, or reader, into the PyTorch format compatible with the Hugging Face Transformers library. It adapts the state dictionary to match the Transformers model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/electra/__init__.py",
            "description": "This `__init__.py` file defines the Electra model package, utilizing lazy loading to import its components (tokenizer, configuration, and model classes) only when they are accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/electra/configuration_electra.py",
            "description": "This file defines the `ElectraConfig` class, which specifies the architecture and hyper-parameters for instantiating an ELECTRA model within the Hugging Face Transformers library. It includes parameters such as vocabulary size, hidden layer dimensions, attention heads, and dropout probabilities.",
            "spof": false
          },
          {
            "path": "src/transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py",
            "description": "This script converts an original TensorFlow checkpoint for an ELECTRA model (either generator or discriminator) into a PyTorch model checkpoint. It loads the weights from the TF checkpoint and maps them to a corresponding PyTorch model architecture specified by a configuration file, then saves the PyTorch model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/electra/modeling_electra.py",
            "description": "This file defines the PyTorch model architecture for the ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately) model within the Hugging Face Transformers library. It includes classes for embeddings, self-attention, and cross-attention mechanisms specific to ELECTRA.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie/__init__.py",
            "description": "This `__init__.py` file defines the ERNIE model module for the Hugging Face Transformers library, primarily using lazy loading for its configuration and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie/configuration_ernie.py",
            "description": "This file defines the `ErnieConfig` class, which stores the configuration parameters for the ERNIE model. It specifies architectural details such as vocabulary size, hidden layers, attention heads, and various dropout probabilities for instantiating an ERNIE model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie/modeling_ernie.py",
            "description": "This file defines the ERNIE model architecture components, including embeddings and self-attention mechanisms, within the Hugging Face Transformers library. It is an auto-generated file from a modular source.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie/modular_ernie.py",
            "description": "This file implements the PyTorch ERNIE model, largely by inheriting from and adapting the BERT model architecture. It introduces ERNIE-specific components such as task-type embeddings for pre-training tasks.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5/__init__.py",
            "description": "This `__init__.py` file serves as the module entry point for the `ernie4_5` model within the Transformers library, primarily setting up lazy loading for its configuration and modeling components to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5/configuration_ernie4_5.py",
            "description": "This file defines the `Ernie4_5Config` class, which is used to store and manage the configuration parameters for the Ernie 4.5 model architecture within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flaubert/configuration_flaubert.py",
            "description": "This file defines the `FlaubertConfig` class, which is used to store and manage the architectural configuration parameters for a FlauBERT model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flaubert/__init__.py",
            "description": "This `__init__.py` file implements lazy loading for the Flaubert model components (configuration, modeling, and tokenization) within the Hugging Face Transformers library, improving import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/flaubert/modeling_flaubert.py",
            "description": "This file implements the core PyTorch model architecture for the Flaubert model, including components like multi-head attention, transformer feed-forward networks, and prediction layers, often adapted from the XLM model implementation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fnet/convert_fnet_original_flax_checkpoint_to_pytorch.py",
            "description": "This script converts a pre-trained FNet model checkpoint from Flax format to PyTorch format. It loads Flax model parameters, maps them to a PyTorch FNet model architecture, and saves the converted model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/funnel/configuration_funnel.py",
            "description": "This file defines the `FunnelConfig` class, which is used to store and manage the configuration parameters for the Funnel Transformer model, specifying its architectural details and hyperparameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/funnel/modeling_funnel.py",
            "description": "This file implements the core PyTorch model components for the Funnel Transformer, including its embeddings and specialized attention structure with support for factorized and relative shift attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py",
            "description": "This script converts a pre-trained TensorFlow Funnel Transformer model checkpoint into a PyTorch-compatible model, loading its weights and saving it in PyTorch format.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Anton Vlasjuk",
            "percent": 33
          },
          {
            "name": "Cyril Vallez",
            "percent": 28
          },
          {
            "name": "Arthur",
            "percent": 9
          }
        ]
      },
      "Specialized and Hybrid Model Architectures": {
        "files": [
          {
            "path": "examples/modular-transformers/configuration_my_new_model2.py",
            "description": "This file defines the `MyNewModel2Config` class, which specifies the architecture and configuration parameters for a custom `MyNewModel2` model, inheriting from `PreTrainedConfig`.",
            "spof": false
          },
          {
            "path": "examples/modular-transformers/configuration_my_new_model.py",
            "description": "This file defines the `MyNewModelConfig` class, which specifies the architecture and hyperparameters for a MyNewModel transformer model. It is automatically generated from `modular_my_new_model.py`.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/afmoe.md",
            "description": "This document provides a detailed overview of the AFMoE (Arcee Foundational Mixture of Experts) model, explaining its architecture, key features, and usage examples with the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/bamba.md",
            "description": "This file provides documentation for the Bamba model in the Hugging Face Transformers library, detailing its architecture, usage examples for text generation and quantization, and specific features like padding-free training.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bros.md",
            "description": "This document provides an overview and detailed documentation for the BROS (BERT Relying On Spatiality) model within the Hugging Face Transformers library. It describes its architecture, pre-training objectives, specific task implementations like token classification, and usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/decision_transformer.md",
            "description": "This file provides documentation for the Decision Transformer model within the Hugging Face Transformers library, detailing its overview, key concepts, and associated classes like `DecisionTransformerConfig`, `DecisionTransformerGPT2Model`, and `DecisionTransformerModel`.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/deepseek_v2.md",
            "description": "This file provides documentation for the DeepSeek-V2 model within the Hugging Face Transformers library, detailing its overview, architecture, usage, and API references for its configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/dbrx.md",
            "description": "This file provides documentation for the DBRX large language model, detailing its architecture, training, and offering Python code examples for its usage with the Hugging Face Transformers library. It also includes autodoc references for related classes like DbrxConfig, DbrxModel, and DbrxForCausalLM.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/ernie4_5_moe.md",
            "description": "This file provides documentation for the Ernie 4.5 Moe model, including an overview of its architecture, usage examples for text generation, distributed generation, and quantization, and references to its configuration and model classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/dpr.md",
            "description": "This file provides documentation for the Dense Passage Retrieval (DPR) model within the Hugging Face Transformers library, detailing its architecture, usage, and API references for its components like encoders, readers, and tokenizers.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/exaone_moe.md",
            "description": "This file provides documentation for the EXAONE MoE model, detailing its architecture, key features, and usage examples for integration with the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/falcon_mamba.md",
            "description": "This documentation file provides an overview and usage examples for the FalconMamba large language model within the Hugging Face Transformers library. It details text generation, quantization, and includes API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glm4_moe.md",
            "description": "This file provides documentation for the GLM-4.5, GLM-4.6, and GLM-4.7 language models, detailing their features, improvements, and evaluation results within the Hugging Face Transformers library. It includes an overview of each model's capabilities and links to further technical details.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/granitemoehybrid.md",
            "description": "This file provides documentation for the Hugging Face Transformers implementation of the GraniteMoeHybrid model. It includes an overview, usage examples, details on padding-free training, and autodoc references for its configuration and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glm4_moe_lite.md",
            "description": "This file provides documentation for the GLM-4.7-Flash model, including an overview and autodoc references for its configuration, pre-trained model, base model, and causal language model components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/granitemoe.md",
            "description": "This file provides documentation for the GraniteMoe model within the Hugging Face Transformers library, including an overview, details from its research paper, and usage examples. It also includes autodoc sections for its configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/hunyuan_v1_moe.md",
            "description": "This file provides documentation for the HunYuanMoEV1 model, including its configuration, model definition, and specialized applications for causal language modeling and sequence classification within the Hugging Face Transformers library. It serves as a placeholder for details to be released with the model's official launch.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/jamba.md",
            "description": "This documentation file describes the Jamba model, a hybrid Transformer-Mamba language model, including its architecture, key features, and practical usage examples for text generation and quantization within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/granitemoeshared.md",
            "description": "This documentation file provides an overview and usage instructions for the GraniteMoeShared model in Hugging Face Transformers, including its configuration and specific model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/informer.md",
            "description": "This file provides documentation for the Informer model within the Hugging Face Transformers library, detailing its overview, key features, and API references for its configuration and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/ministral3.md",
            "description": "This file provides documentation and usage examples for the Ministral3 model within the Hugging Face Transformers library, including its features and various classes for different tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/lilt.md",
            "description": "This file provides documentation for the LiLT (Language-independent Layout Transformer) model within the Hugging Face Transformers library. It includes an overview of the model, usage tips, and references to its API classes for various tasks like sequence, token, and question answering classification.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mamba2.md",
            "description": "This file provides documentation for the Mamba 2 model within the Hugging Face Transformers library. It details the model's features, offers usage examples for text generation and quantization, and outlines API references for its configuration, model, and language model head.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mamba.md",
            "description": "This file provides documentation for the Mamba model within the Hugging Face Transformers library, including its features, usage examples for text generation and quantization, and details on its architecture and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mixtral.md",
            "description": "This documentation page provides an overview, architectural details, usage tips, and optimization methods for the Mixtral-8x7B large language model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mistral.md",
            "description": "This file provides comprehensive documentation for the Mistral language model within the Hugging Face Transformers library, detailing its features, usage examples (including chat, `AutoModel`, and quantization), and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/olmoe.md",
            "description": "This file provides documentation for the OLMoE language model, detailing its features, offering text generation examples using the Transformers library, and describing how to apply quantization. It also includes API references for the `OlmoeConfig`, `OlmoeModel`, and `OlmoeForCausalLM` classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/nllb-moe.md",
            "description": "This document provides an overview and usage guide for the NLLB-MOE model within the Hugging Face Transformers library. It details its architecture, generation methods, and links to its associated classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/phimoe.md",
            "description": "This file provides documentation for the `PhiMoE` model in the Hugging Face Transformers library, detailing its overview, usage instructions, and API references for its configuration and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/rwkv.md",
            "description": "This file provides documentation for the RWKV model in the Hugging Face Transformers library, including an overview, usage examples, and a detailed explanation of its recurrent attention mechanism.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/qwen3_moe.md",
            "description": "This file provides documentation for the Qwen3MoE (Mixture of Experts) model within the Hugging Face Transformers library. It includes an overview of the model, usage tips, and references to its configuration and various task-specific implementations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/qwen2_moe.md",
            "description": "This document provides an overview and usage guide for the Qwen2MoE model within the Hugging Face Transformers library. It includes details on its architecture, text generation examples using various methods, and quantization instructions, along with API documentation links for related classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/rag.md",
            "description": "This file provides documentation for the Retrieval-Augmented Generation (RAG) model in Hugging Face Transformers, including its concept, usage examples for text generation and quantization, and API references for its various components like `RagConfig`, `RagTokenizer`, and `RagSequenceForGeneration`.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/reformer.md",
            "description": "This file provides comprehensive documentation for the Reformer model within the Hugging Face Transformers library, detailing its architecture, efficiency improvements, usage tips, and training considerations. It explains key features like Axial Positional Encodings, LSH Self Attention, and Local Self Attention.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/switch_transformers.md",
            "description": "This documentation file introduces Switch Transformers, a sparse T5 model utilizing Mixture-of-Experts. It provides usage examples with Hugging Face Transformers, including quantization, and lists API references for various components of the model.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/xmod.md",
            "description": "This file provides comprehensive documentation for the X-MOD model within the Hugging Face Transformers library, detailing its architecture, usage, fine-tuning, and API references for various tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/xlstm.md",
            "description": "This file provides documentation for the xLSTM model, including an overview of its architecture, its origin, and references to its paper and implementation. It also includes auto-generated documentation for the xLSTM configuration and model classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/zamba.md",
            "description": "This file provides documentation for the Zamba Large Language Model within the Hugging Face Transformers library. It details the model's architecture, usage instructions, inference examples, and related configuration and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/ko/chat_extras.md",
            "description": "This document provides a guide on how to integrate tools and Retrieval-Augmented Generation (RAG) with chat templates in the Hugging Face Transformers library. It explains defining tools, generating tool calls, and using documents for RAG within chat conversations.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/exaone_moe.md",
            "description": "This file provides Korean-language documentation for the EXAONE MoE model within the Hugging Face Transformers library, detailing its architecture, capabilities, usage examples, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/mamba2.md",
            "description": "This file provides Korean language documentation for the Mamba2 model within the Hugging Face Transformers library, explaining its architecture, features, and providing usage examples for generation and fine-tuning.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/dbrx.md",
            "description": "This document provides an overview of the DBRX large language model, including its architecture and pre-training details. It also offers usage examples for text generation with the Transformers library and API documentation for `DbrxConfig`, `DbrxModel`, and `DbrxForCausalLM`.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/deepseek_v3.md",
            "description": "This document provides an overview and documentation for the DeepSeek-V3 model within the Hugging Face Transformers library. It details its architecture, training, usage tips, and associated Python classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/rag.md",
            "description": "This file is a Korean documentation page for the RAG (Retrieval-Augmented Generation) model within the Hugging Face Transformers library. It provides an overview of the RAG model, usage tips, and API references for its related components like `RagConfig`, `RagTokenizer`, and various `Rag` models.",
            "spof": false
          },
          {
            "path": "tests/models/arcee/test_modeling_arcee.py",
            "description": "This file contains unit and integration tests for the PyTorch Arcee model within the Hugging Face Transformers library, ensuring its functionality and correct implementation.",
            "spof": true
          },
          {
            "path": "tests/models/bamba/test_modeling_bamba.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch Bamba model within the Hugging Face Transformers library, including unit tests for its configuration, model functionalities, generation capabilities, and pipeline integration.",
            "spof": false
          },
          {
            "path": "tests/models/afmoe/test_modeling_afmoe.py",
            "description": "This file contains unit tests for the AFMOE (AFM-optimized Expert) model implementations, `AfmoeModel` and `AfmoeForCausalLM`, within the Hugging Face Transformers library. It defines a model tester and a test suite to verify the functionality and correctness of these PyTorch-based models.",
            "spof": true
          },
          {
            "path": "tests/models/big_bird/test_modeling_big_bird.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch implementation of the BigBird model within the Hugging Face Transformers library. It includes a `BigBirdModelTester` class to set up configurations and inputs, and methods to test various BigBird model functionalities like pre-training, masked language modeling, and causal language modeling.",
            "spof": false
          },
          {
            "path": "tests/models/bitnet/test_modeling_bitnet.py",
            "description": "This file contains unit tests and integration tests for the BitNet model implementation in the Hugging Face Transformers library, covering its configuration, core model functionality, and text generation capabilities.",
            "spof": true
          },
          {
            "path": "tests/models/bros/test_modeling_bros.py",
            "description": "This file contains a testing suite for the PyTorch Bros model and its various task-specific heads within the Hugging Face Transformers library. It includes unit tests for model functionality, configuration, and different classification tasks.",
            "spof": false
          },
          {
            "path": "tests/models/cwm/test_configuration_cwm.py",
            "description": "This file contains unit tests for the `CwmConfig` class, verifying its default settings, custom parameter handling, validation logic, serialization, and inheritance properties for the CWM model configuration.",
            "spof": true
          },
          {
            "path": "tests/models/dbrx/test_modeling_dbrx.py",
            "description": "This file contains unit and integration tests for the PyTorch DBRX model within the Hugging Face Transformers library, including a model tester class and specific tests for functionalities like model loading and output logits.",
            "spof": false
          },
          {
            "path": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "description": "This file contains unit and integration tests for the PyTorch Decision Transformer model in the Hugging Face Transformers library. It includes a model tester class, common tests inherited from `ModelTesterMixin`, and an autoregressive prediction integration test.",
            "spof": false
          },
          {
            "path": "tests/models/dpr/test_modeling_dpr.py",
            "description": "This file contains unit tests and integration tests for the DPR (Dense Passage Retriever) models in the Hugging Face Transformers library. It includes tests for DPRContextEncoder, DPRQuestionEncoder, and DPRReader components, checking their configurations, model functionality, and pretrained loading capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "description": "This file contains a testing suite for the PyTorch DeepSeekV2 model, including unit tests for its core functionalities and integration tests for model generation and specific configurations.",
            "spof": false
          },
          {
            "path": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "description": "This file contains the testing suite for the PyTorch Ernie4.5 MoE model, including unit tests for its core functionality and integration tests for features like Flash Attention 2, load balancing loss, and text generation with different model sizes.",
            "spof": true
          },
          {
            "path": "tests/models/exaone_moe/test_modeling_exaone_moe.py",
            "description": "This file contains a testing suite for the PyTorch EXAONE MoE (Mixture of Experts) model, including unit tests for model components and integration tests for functionalities like text generation and logits verification.",
            "spof": true
          },
          {
            "path": "tests/models/evolla/test_modeling_evolla.py",
            "description": "This file contains the testing suite for the PyTorch Evolla model within the Hugging Face Transformers library, ensuring its correct functionality and integration.",
            "spof": true
          },
          {
            "path": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "description": "This file contains unit tests for the FalconMamba model in the Hugging Face Transformers library, verifying its core modeling, generation, and pipeline functionalities.",
            "spof": false
          },
          {
            "path": "src/transformers/models/__init__.py",
            "description": "This `__init__.py` file defines the `transformers.models` package structure. It primarily uses lazy loading (and type checking imports) to expose a large collection of transformer model architectures as submodules, optimizing import times for the library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/afmoe/modular_afmoe.py",
            "description": "This file implements the PyTorch modules for the AFMoE (Aligned Factorization Mixture-of-Experts) model, defining its specialized MoE layers, attention mechanism, and other core components for use within the HuggingFace Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/afmoe/configuration_afmoe.py",
            "description": "This file defines the `AfmoeConfig` class, which is used to store and manage the configuration parameters for the AFMoE (Adaptive Feedforward Mixture of Experts) model, including its architecture, MoE settings, and attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/afmoe/__init__.py",
            "description": "This `__init__.py` file initializes the `afmoe` model package within the Transformers library. It primarily sets up lazy loading for the model's configuration and modeling components to optimize import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/afmoe/modeling_afmoe.py",
            "description": "This file defines the core architectural components and helper classes for the AFMOE (Adaptive Factorized Mixture-of-Experts) model, including its rotary embeddings, RMS normalization, MLP, token choice router, and expert modules.",
            "spof": true
          },
          {
            "path": "src/transformers/models/apertus/configuration_apertus.py",
            "description": "This file defines the `ApertusConfig` class, which is used to store and manage the configuration parameters for the Apertus model within the Hugging Face Transformers library. It specifies architectural details such as vocabulary size, hidden dimensions, and attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/apertus/__init__.py",
            "description": "This `__init__.py` file defines the `apertus` model package, enabling lazy loading of its configuration and modeling components within the transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/arcee/configuration_arcee.py",
            "description": "This file defines the `ArceeConfig` class, which is used to store and manage the configuration parameters for the Arcee model, including architectural details and hyperparameters. It inherits from `PreTrainedConfig` and specifies how to instantiate an Arcee model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bamba/convert_mamba_ssm_checkpoint.py",
            "description": "This script converts checkpoints from the `mamba_ssm` library into the HuggingFace `transformers` format, specifically for the Bamba model. It maps the original state dictionary and configuration to the HuggingFace structure.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bamba/configuration_bamba.py",
            "description": "This file defines the `BambaConfig` class, which is used to store and manage the configuration parameters for the Bamba model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bamba/__init__.py",
            "description": "This `__init__.py` file sets up the Bamba model module within the Hugging Face Transformers library, using a lazy import mechanism to defer loading of its components (configuration, modeling, and processing) until they are explicitly accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bamba/modeling_bamba.py",
            "description": "This file defines the core modeling components for the Bamba model within the Hugging Face Transformers library, including a hybrid dynamic cache for Mamba and attention layers, and Rotary Embeddings. It is automatically generated from `modular_bamba.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bamba/modular_bamba.py",
            "description": "This file implements the PyTorch Bamba model, a modular architecture that combines elements of Mamba and attention mechanisms, within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/autoformer/__init__.py",
            "description": "This `__init__.py` file defines the Autoformer model package, using lazy loading to import its configuration and modeling components. It optimizes startup time by only loading submodules when explicitly accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/autoformer/modeling_autoformer.py",
            "description": "This file implements the PyTorch Autoformer model architecture and its associated components for time series forecasting within the Hugging Face Transformers library. It defines the model's output structures and utility modules for feature embedding and data scaling.",
            "spof": false
          },
          {
            "path": "src/transformers/models/big_bird/modeling_big_bird.py",
            "description": "This file implements the core PyTorch model components for the BigBird transformer, including embeddings, self-attention mechanisms, and block-sparse attention. It defines classes for handling input embeddings and various attention layers specific to the BigBird architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bitnet/configuration_bitnet.py",
            "description": "This file defines the `BitNetConfig` class, which specifies the architecture and hyper-parameters for the BitNet model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bitnet/__init__.py",
            "description": "This `__init__.py` file defines the lazy loading mechanism for the BitNet model, ensuring that its configuration and modeling components are imported only when explicitly accessed, optimizing performance. It sets up the import structure for `configuration_bitnet` and `modeling_bitnet`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bitnet/modeling_bitnet.py",
            "description": "This file implements the core architectural components, such as RMS normalization, MLP, and attention mechanisms, for the BitNet model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bigbird_pegasus/__init__.py",
            "description": "This `__init__.py` file defines the BigBird-Pegasus model package within the Transformers library. It utilizes lazy loading to manage imports of configuration and modeling components, improving initial load times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blt/modular_blt.py",
            "description": "Implements the modular Blt model architecture, inheriting from Mllama, focusing on hash-based token embeddings and patch-based attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bros/convert_bros_to_pytorch.py",
            "description": "This script converts model checkpoints from the original BROS implementation to the Hugging Face Transformers format. It handles state dict key renaming and verification, allowing for saving the converted model and processor locally or pushing them to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cpmant/configuration_cpmant.py",
            "description": "This file defines the configuration class for the CPMAnt model, specifying its architecture parameters like vocabulary size, hidden layers, and attention heads. It allows users to instantiate and customize the CPMAnt model's structure.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cwm/configuration_cwm.py",
            "description": "This file defines the `CwmConfig` class, which specifies the configuration parameters for the Code World Model (CWM) within the Transformers library. It includes settings for model architecture, attention mechanisms, and other hyperparameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cwm/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the CWM model within the Hugging Face Transformers library. It uses lazy loading to import the CWM model's configuration and modeling classes, optimizing import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cwm/modeling_cwm.py",
            "description": "This file implements the core modeling components for the CWM (Convolutional Wavelet Model) architecture, including Rotary Embeddings and the CWM Attention mechanism. It is automatically generated from a modular version of the CWM model implementation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cwm/modular_cwm.py",
            "description": "This file defines the CWM (Code World Model) architecture, including its configuration (`CwmConfig`), attention mechanism, and decoder layers, by extending and adapting components from Llama and Qwen2 models. It implements features such as layer-interleaved sliding-window attention for the CWM.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_v2/__init__.py",
            "description": "This file initializes the `deepseek_v2` module, defining its import structure to enable lazy loading of the Deepseek V2 model's configuration and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "description": "This file defines the configuration class `DeepseekV2Config`, which is used to store and manage the architectural parameters for the DeepSeekV2 model in the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "description": "This file defines the configuration class (`DeepseekV2Config`) for the DeepSeek V2 model, specifying its architectural parameters and default values. It enables the instantiation and customization of DeepSeek V2 models within the HuggingFace Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "description": "This file implements the core architectural components of the DeepseekV2 model for the Hugging Face Transformers library, including its Mixture-of-Experts (MoE) structure, MLP, RMS normalization, and rotary embeddings. It is an automatically generated file derived from a modular version.",
            "spof": false
          },
          {
            "path": "src/transformers/models/decision_transformer/__init__.py",
            "description": "This `__init__.py` file initializes the `decision_transformer` package, enabling lazy loading of its configuration and modeling components. It sets up dynamic imports for `configuration_decision_transformer` and `modeling_decision_transformer` to improve startup performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "description": "This file implements the PyTorch model architecture and attention mechanisms for the DecisionTransformer model, adapting components from the GPT-2 model. It defines the core building blocks for self-attention and cross-attention within the DecisionTransformer.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "description": "This file defines the `DeepseekV3Config` class, which is used to store and manage the configuration parameters for the DeepSeekV3 model within the HuggingFace Transformers library. It specifies architectural details such as hidden sizes, layer counts, and expert parameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "description": "This file defines the architectural components for the DeepseekV3 model, including its RMSNorm, Rotary Embedding, MLP, and Mixture of Experts (MoE) layers. It is an automatically generated file based on a modular source.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "description": "This file implements the core architectural components for the DeepseekV3 model within the Hugging Face Transformers library, including its Mixture-of-Experts (MoE) layers, attention mechanism, and other foundational modules like RMS normalization and rotary embeddings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dbrx/__init__.py",
            "description": "Initializes the DBRX model within the Hugging Face Transformers library, setting up its lazy-loading mechanism. It defines the import structure for DBRX model configuration and modeling components.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dbrx/modeling_dbrx.py",
            "description": "This file defines the DBRX model's core architectural components, including rotary position embeddings and attention mechanisms, within the Hugging Face Transformers library. It is automatically generated from a 'modular' version of the DBRX model definition.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dbrx/modular_dbrx.py",
            "description": "This file implements modular components for the DBRX model, including its attention mechanism and Mixture-of-Experts (MoE) Feed-Forward Network (FFN) components like routers and experts.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dbrx/configuration_dbrx.py",
            "description": "This file defines the configuration classes for the DBRX model, including its attention mechanism, feed-forward network, and overall model parameters within the Hugging Face Transformers library. It provides the architectural blueprint for instantiating and configuring DBRX models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/edgetam/__init__.py",
            "description": "This `__init__.py` file sets up the `edgetam` model package within the `transformers` library, using lazy loading to import its configuration and modeling components efficiently.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dots1/__init__.py",
            "description": "This `__init__.py` file defines the `dots1` package for the Hugging Face Transformers library. It uses lazy loading to import the `dots1` model's configuration and modeling components only when they are accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dots1/configuration_dots1.py",
            "description": "This file defines the `Dots1Config` class, which specifies the architecture and hyperparameters for the `Dots1` model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dots1/modular_dots1.py",
            "description": "This file defines the architectural components for the Dots1 model within the Hugging Face Transformers library. It primarily reuses and adapts existing building blocks from the DeepseekV3 and Qwen3 models, with a custom implementation for its Mixture-of-Experts (MoE) routing.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpr/configuration_dpr.py",
            "description": "This file defines the `DPRConfig` class, which is used to store and manage the configuration for different components of the DPR (Dense Passage Retrieval) model within the Hugging Face Transformers library, such as the `DPRContextEncoder`, `DPRQuestionEncoder`, and `DPRReader`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpr/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the DPR (Dense Passage Retriever) model in the Transformers library. It sets up lazy loading for the DPR model's configuration, modeling, and tokenization components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dpr/modeling_dpr.py",
            "description": "This file implements the PyTorch models for Dense Passage Retrieval (DPR), including components for context encoding, question encoding, and a reader for Open Domain Question Answering.",
            "spof": false
          },
          {
            "path": "src/transformers/models/doge/__init__.py",
            "description": "This `__init__.py` file for the Doge model uses lazy loading to define its import structure. It ensures that submodules like `configuration_doge` and `modeling_doge` are imported only when accessed, optimizing startup performance for the `doge` model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/doge/modular_doge.py",
            "description": "This file defines the `DogeConfig` class, which specifies the architecture and hyper-parameters for the PyTorch Doge small language model within the Hugging Face Transformers library. It includes configurations for both standard and Mixture-of-Experts (MoE) model variants.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "description": "This file defines the architecture and core components (like RMSNorm, MLP, and Rotary Embedding) for the ERNIE 4.5 Mixture-of-Experts (MoE) model within the Hugging Face Transformers library. It is automatically generated from a modular source file.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie4_5_moe/__init__.py",
            "description": "This `__init__.py` file defines the `ernie4_5_moe` model package within the transformers library. It uses lazy loading to import its submodules, `configuration_ernie4_5_moe` and `modeling_ernie4_5_moe`, only when they are accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "description": "This file defines the `Ernie4_5_MoeConfig` class, which stores the configuration parameters for the Ernie 4.5 Mixture-of-Experts (MoE) model. It specifies architectural details and MoE-specific settings for instantiating or loading the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "description": "This file implements the PyTorch model architecture for the Ernie 4.5 Mixture-of-Experts (MoE) model, defining its core components like attention, MLP, and MoE routing logic.",
            "spof": false
          },
          {
            "path": "src/transformers/models/exaone4/__init__.py",
            "description": "This `__init__.py` file defines the `exaone4` model package structure, enabling lazy loading of its configuration and modeling components for improved import performance in the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "description": "This file defines the `FalconMambaConfig` class, which is used to store and manage the configuration parameters for the FalconMamba model architecture within the HuggingFace Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon_mamba/__init__.py",
            "description": "This `__init__.py` file defines the module structure for the Falcon-Mamba model, enabling lazy loading of its configuration and modeling components within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "description": "This file defines the configuration (`FalconMambaConfig`) and caching mechanism (`FalconMambaCache`) for the Falcon-Mamba hybrid model within the Hugging Face Transformers library, extending functionalities from the base Mamba model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "description": "This file implements the core modeling components for the FalconMamba architecture within the Hugging Face Transformers library, including its state caching mechanism and the selective state space mixer.",
            "spof": false
          },
          {
            "path": "src/transformers/models/exaone_moe/__init__.py",
            "description": "This `__init__.py` file initializes the `exaone_moe` model package within the Hugging Face Transformers library. It uses a lazy loading mechanism to import model configuration and core modeling classes only when they are accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/exaone_moe/configuration_exaone_moe.py",
            "description": "This file defines the `ExaoneMoeConfig` class, which serves as the configuration for the Exaone Mixture-of-Experts (MoE) model. It specifies the model's architecture and hyperparameters for instantiation.",
            "spof": true
          },
          {
            "path": "src/transformers/models/exaone_moe/modular_exaone_moe.py",
            "description": "This file defines the `ExaoneMoeConfig` class, which specifies the configuration parameters for the EXAONE Mixture of Experts (MoE) model within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/exaone_moe/modeling_exaone_moe.py",
            "description": "This file implements the core modeling components for the Exaone-Moe model within the Hugging Face Transformers library, including RMS normalization and attention mechanisms. It is automatically generated from 'modular_exaone_moe.py'.",
            "spof": true
          },
          {
            "path": "src/transformers/models/falcon_h1/convert_mamba_ssm_checkpoint.py",
            "description": "This script converts checkpoints from the `mamba_ssm` library into the HuggingFace `transformers` format for the Falcon-H1 model. It remaps model weights and configurations to be compatible with the HuggingFace ecosystem.",
            "spof": true
          },
          {
            "path": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "description": "This file implements the PyTorch model architecture for FalconH1, including its specific hybrid Mamba and attention caching mechanism, rotary embeddings, and attention layer, building upon components from Llama and Jamba models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "description": "This file implements the `FalconHybridMambaAttentionDynamicCache` class, providing a dynamic caching mechanism for the Falcon H1 model. It manages both attention key/value states and Mamba-specific convolutional and SSM states for efficient sequence generation in a hybrid model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "description": "This file defines the `FlexOlmoConfig` class, which is used to store and manage the configuration parameters for the FlexOlmo model, including architectural details and hyper-parameters. It is automatically generated from `modular_flex_olmo.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fnet/configuration_fnet.py",
            "description": "This file defines the `FNetConfig` class, which is used to store and manage the architectural configuration of the FNet model within the HuggingFace Transformers library. It specifies various hyperparameters such as vocabulary size, hidden layer dimensions, and dropout probabilities.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fnet/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the FNet model within the Transformers library. It facilitates lazy loading of the model's configuration, modeling, and tokenization modules.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fnet/modeling_fnet.py",
            "description": "This file implements the PyTorch FNet model architecture, including its embeddings, Fourier Transform layers, and feed-forward components. It defines how the FNet model processes inputs using Fast Fourier Transforms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/funnel/__init__.py",
            "description": "This file serves as the `__init__.py` for the Funnel Transformer model package, setting up lazy loading for its configuration, model, and tokenizer components to improve import performance.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 16
          },
          {
            "name": "Arthur",
            "percent": 11
          },
          {
            "name": "Raushan Turganbay",
            "percent": 9
          }
        ]
      },
      "Decoder-Only Model Architectures": {
        "files": [
          {
            "path": "examples/modular-transformers/modular_super.py",
            "description": "This file defines a `SuperModel` class that extends `LlamaModel` from Hugging Face Transformers, demonstrating how to modify the model's output (specifically scaling logits) within a modular setup.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_test_suffix.py",
            "description": "This file defines modular components for a Llama-like transformer model variant named 'TestSuffixLlama', including RMS normalization, MLP, and multi-head attention with rotary position embeddings. It appears to be an automatically generated test or experimental model implementation.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modular_duplicated_method.py",
            "description": "This file defines a `DuplicatedMethodConfig` class inheriting from `LlamaConfig`. It overrides the `vocab_size` property, setting a fixed value and demonstrating a problematic recursive setter implementation, likely for testing or illustrative purposes.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_my_new_model2.py",
            "description": "This file defines the core architectural components, including RMS normalization, an MLP, and a multi-headed attention mechanism with rotary position embeddings, for a decoder layer within the `MyNewModel2` transformer architecture. It is an auto-generated file from a modular source.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/language_modeling.md",
            "description": "This documentation page explains causal language modeling and provides a step-by-step guide on how to finetune a DistilGPT2 model for this task, including data preprocessing and training instructions.",
            "spof": true
          },
          {
            "path": "docs/source/en/internal/rope_utils.md",
            "description": "This document explains how Rotary Embedding (RoPE) is computed and applied within the Hugging Face Transformers library, detailing supported RoPE types and their configuration in model settings, including per-layer configurations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/apertus.md",
            "description": "This file provides the documentation for the Apertus model within the Hugging Face Transformers library, including an overview, usage examples for text generation, and API references for its configurations and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/cwm.md",
            "description": "This file provides documentation for the Code World Model (CWM) within the Hugging Face Transformers library, detailing its overview, usage examples, and API references for its configuration, pre-trained model, base model, and causal language model.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/cpm.md",
            "description": "This file provides documentation for the CPM (Chinese Pre-trained Language Model), detailing its overview, paper, and associated tokenizers (CpmTokenizer and CpmTokenizerFast) within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/ctrl.md",
            "description": "This file provides comprehensive documentation for the CTRL model within the Hugging Face Transformers library, detailing its overview, usage tips, and API references for its configuration, tokenizer, and various model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/code_llama.md",
            "description": "This file is a documentation page for the CodeLlama model in the Hugging Face Transformers library. It provides an overview of CodeLlama's features, usage examples for code generation and infilling, and details on quantization and tokenizer specifics.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/cpmant.md",
            "description": "This document provides an overview and detailed documentation for the CPMAnt model within the Hugging Face Transformers library, including its configuration, tokenizer, and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bloom.md",
            "description": "This file provides comprehensive documentation for the BLOOM model within the Hugging Face Transformers library, including an overview of its architecture and training, available versions, related resources, and API documentation for its various classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/dialogpt.md",
            "description": "This file provides documentation for the DialoGPT model within the Hugging Face Transformers library. It details the model's origin, training, and offers usage tips for conversational response generation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/cohere2.md",
            "description": "This document provides an overview and usage examples for the Cohere Command R7B (Cohere 2) model within the Hugging Face Transformers library, including text generation and quantization demonstrations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/doge.md",
            "description": "This file provides documentation for the Doge language model within the Hugging Face Transformers library, detailing its architecture, usage, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/cohere.md",
            "description": "This file provides documentation for the Cohere Command-R large language model, detailing its features, usage examples with Hugging Face Transformers (including generation, quantization, and attention visualization), and API references for its configuration, tokenizer, and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/dots1.md",
            "description": "This file provides documentation for the `dots.llm1` model, including an overview, technical report summary, and details on its configuration and model classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/falcon3.md",
            "description": "This file provides documentation for the Falcon3 series of models within the Hugging Face Transformers library, detailing their architecture, training, and available variants.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/falcon_h1.md",
            "description": "This file provides documentation for the FalconH1 model, detailing its overview, contributors, configuration, and usage examples within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/falcon.md",
            "description": "This file is a documentation page for the Falcon family of large language models within the Hugging Face Transformers library. It provides an overview, usage examples for text generation and quantization, and API documentation for Falcon-related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/diffllama.md",
            "description": "This file documents the DiffLlama model within the Hugging Face Transformers library, explaining its architecture, proposed paper, usage, and providing references to its various class implementations for different tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/gpt-sw3.md",
            "description": "This file provides documentation for the GPT-Sw3 model within the Hugging Face Transformers library, including its overview, usage examples, and details on its GPTSw3Tokenizer.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/gemma.md",
            "description": "This file provides documentation for the Gemma language model within the Hugging Face Transformers library, including its architecture, usage examples for text generation and quantization, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/granite.md",
            "description": "This file provides documentation for the Granite language model within the Hugging Face Transformers library, including its description, usage examples (text generation, quantization), and references to its configuration and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/flex_olmo.md",
            "description": "This file provides documentation and usage examples for the FlexOlmo language model within the Hugging Face Transformers library. It describes the model's architecture, how to use it for text generation, and quantization techniques.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/gpt2.md",
            "description": "This documentation file provides an overview of the GPT-2 model, including its architecture, usage examples for text generation and quantization, and auto-generated API references for various GPT-2 classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glm.md",
            "description": "This document provides an overview and usage guide for the GLM-4 large language model within the Hugging Face Transformers library, including its architecture components and example inference code.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/gemma2.md",
            "description": "This file provides documentation for the Gemma 2 language model within the Hugging Face Transformers library, detailing its architecture, usage examples for text generation, quantization, and attention visualization, along with API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/gpt_bigcode.md",
            "description": "This file provides documentation for the GPTBigCode model in the Hugging Face Transformers library. It details the model's overview, implementation differences from GPT2, and usage examples, including integration with Flash Attention 2.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/gpt_oss.md",
            "description": "This file provides documentation for the GptOss model within the Hugging Face Transformers library, detailing its overview, features, and API references for various task-specific implementations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/gpt_neox.md",
            "description": "This document provides an overview, usage examples, and optimization details (Flash Attention 2, SDPA) for the GPT-NeoX model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/helium.md",
            "description": "This file provides comprehensive documentation for the Helium large language model within the Hugging Face Transformers library, detailing its overview, evaluation results, technical specifications, and usage examples.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/gptj.md",
            "description": "This file provides documentation for the GPT-J model within the Hugging Face Transformers library. It includes an overview, usage tips, code examples for text generation, relevant resources, and API references for its various classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glm4.md",
            "description": "This file provides documentation for the GLM-4 model family, detailing its various versions and capabilities. It also documents the associated configuration and model classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/jais2.md",
            "description": "This file provides documentation for the Jais2 model, an Arabic open-weight LLM, detailing its overview, architecture, and specific components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/gpt_neox_japanese.md",
            "description": "This file provides documentation for the GPT-NeoX-Japanese model, detailing its features, usage examples for text generation and quantization, and links to relevant model components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/llama4.md",
            "description": "This file provides documentation and usage examples for the Llama4 model within the Hugging Face Transformers library, detailing its architecture, features, and various inference methods.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/gpt_neo.md",
            "description": "This file provides documentation for the GPT-Neo model within the Hugging Face Transformers library. It describes GPT-Neo, its features, and demonstrates its usage for text generation, including quantization, and lists its various model configurations and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/llama.md",
            "description": "This file is a documentation page for the Llama large language model within the Hugging Face Transformers library. It provides an overview of the Llama model, its architecture, usage examples for text generation and quantization, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/longcat_flash.md",
            "description": "This file provides comprehensive documentation for the `LongCatFlash` model within the Hugging Face Transformers library, detailing its architecture, capabilities, usage examples, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/llama3.md",
            "description": "This file documents the Llama3 model within the Hugging Face Transformers library. It provides an overview of the model, usage tips, and instructions for conversion and integration.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/megatron_gpt2.md",
            "description": "This document provides an overview and usage guide for the MegatronGPT2 model within the Hugging Face Transformers library. It details how to obtain and convert MegatronGPT2 checkpoints for use with Hugging Face's GPT-2 implementation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/nanochat.md",
            "description": "This file is a documentation page for the NanoChat model within the Hugging Face Transformers library. It describes the model's architecture, origin, and provides usage examples for text generation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/ministral.md",
            "description": "This document provides an overview, usage examples, and API references for the Ministral language model within the Hugging Face Transformers library. It describes the model's architecture, including its alternating attention pattern, and demonstrates how to use it for text generation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/minimax.md",
            "description": "This file provides documentation for the MiniMax AI model within the Hugging Face Transformers library, detailing its architecture, usage instructions, and optimization techniques such as Flash Attention and quantization.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/llama2.md",
            "description": "This file is a documentation page for the Llama 2 large language model within the Hugging Face Transformers library. It describes the model's architecture, training, and provides code examples for its usage, including text generation and quantization.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/modernbert-decoder.md",
            "description": "This file provides documentation for the ModernBERT Decoder model within the Hugging Face Transformers library. It explains the model's architecture, features, and offers usage examples for text generation and sequence classification, along with API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/nemotron.md",
            "description": "This file provides documentation for the Nemotron and Minitron models within the Hugging Face Transformers library. It includes details on their architecture, usage, evaluation results, and API references for various tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/opt.md",
            "description": "This documentation file provides an overview and usage examples for the OPT (Open Pre-trained Transformer) models within the Hugging Face Transformers library. It includes details on text generation, quantization, and links to relevant resources and API references for OPT classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mpt.md",
            "description": "This file provides comprehensive documentation for the MPT (MosaicML Pre-trained Transformer) model within the Hugging Face Transformers library, detailing its overview, variants, usage tips, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/olmo2.md",
            "description": "This file provides documentation for the OLMo2 model within the Hugging Face Transformers library, detailing its architecture, usage examples for text generation and quantization, and specific implementation notes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/olmo3.md",
            "description": "This file provides documentation for the OLMo3 model within the Hugging Face Transformers library, including usage examples for text generation, quantization, and references to its configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/olmo.md",
            "description": "This file is a documentation page for the OLMo language model within the Hugging Face Transformers library. It provides an overview of OLMo, demonstrates how to use it for text generation, including quantization techniques, and references its API components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/persimmon.md",
            "description": "This file documents the Persimmon large language model within the Hugging Face Transformers library. It provides an overview of the model, usage tips for conversion and loading, and details its specific configurations and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/phi.md",
            "description": "This documentation file describes the Phi large language model, detailing its characteristics, providing usage examples with Hugging Face Transformers for tasks like text generation and quantization, and referencing its API classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/phi3.md",
            "description": "This file provides documentation for the Phi-3 model within the Hugging Face Transformers library. It includes an overview, usage instructions, code examples, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/qwen3_next.md",
            "description": "This file provides documentation for the Qwen3-Next model series, detailing its architectural innovations, performance benefits, and usage examples. It also includes auto-generated documentation for various Qwen3-Next classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/recurrent_gemma.md",
            "description": "This documentation file provides an overview and detailed information for the RecurrentGemma model, including its architecture, key features, and API references within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/qwen2.md",
            "description": "This file provides documentation for the Qwen2 large language model within the Hugging Face Transformers library. It describes the model's architecture and usage, including code examples for text generation and quantization.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/openai-gpt.md",
            "description": "This document provides comprehensive documentation for the OpenAI GPT model within the Hugging Face Transformers library. It explains the model's capabilities, how to use it for text generation, and details its various components and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/qwen3.md",
            "description": "This file provides documentation for the Qwen3 model within the Hugging Face Transformers library. It includes an overview of the model, usage tips, and auto-generated documentation for its configuration and various model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/starcoder2.md",
            "description": "This file provides comprehensive documentation for the StarCoder2 model within the Hugging Face Transformers library, detailing its overview, licensing, usage examples, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/solar_open.md",
            "description": "This file provides documentation for the SolarOpen language model, including its overview, usage tips, and Python code examples for integration with the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/stablelm.md",
            "description": "This file provides documentation for the StableLM model within the Hugging Face Transformers library, detailing its features, usage, and integration with Flash Attention 2. It includes information on model architecture, inference examples, and API references for various StableLM classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/smollm3.md",
            "description": "This file provides documentation for the SmolLM3 language model, detailing its architecture, capabilities, and providing code examples for usage with Hugging Face Transformers, including text generation and quantization. It also includes API references for various SmolLM3 classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/vaultgemma.md",
            "description": "This file provides documentation for the VaultGemma model, detailing its architecture, training, and offering usage examples for text generation within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/xglm.md",
            "description": "This file provides the documentation for the XGLM model within the Hugging Face Transformers library, detailing its overview, research paper, and API references for its configuration, tokenizer, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/zamba2.md",
            "description": "This file provides documentation for the Zamba2 large language model, detailing its architecture, usage with the Hugging Face Transformers library, and related classes.",
            "spof": false
          },
          {
            "path": "docs/source/ja/tasks/language_modeling.md",
            "description": "This document provides a guide in Japanese on causal language modeling using the Hugging Face Transformers library. It details how to fine-tune a DistilGPT2 model for text generation and perform inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/code_llama.md",
            "description": "This file is a Japanese documentation page for the CodeLlama model within the Transformers library. It provides an overview of CodeLlama, usage examples, and details about its tokenizer.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/bloom.md",
            "description": "This file provides the Japanese documentation for the BLOOM model within the Hugging Face Transformers library, including an overview, resources, and API references for various BLOOM classes.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/cpm.md",
            "description": "This file is a Japanese documentation page for the CPM model in the Hugging Face Transformers library. It provides an overview of the model, its research paper, and mentions its tokenizer classes, noting its architectural similarity to GPT-2.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/ctrl.md",
            "description": "This file provides Japanese documentation for the Hugging Face Transformers CTRL model, including an overview, usage tips, and API references for its configuration, tokenizer, and various model classes.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/dialogpt.md",
            "description": "This file provides Japanese documentation for the DialoGPT model, including an overview, usage tips, and training information. It describes DialoGPT as a GPT2-based model for conversational response generation.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/cpmant.md",
            "description": "This document provides a Japanese overview and documentation for the CPMAnt pre-trained language model, including its configuration, tokenizer, and model classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/llama3.md",
            "description": "This document provides Korean-language documentation for the Llama 3 model within the Hugging Face Transformers library. It includes an overview, usage tips, and resources for working with Llama 3.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/cohere.md",
            "description": "This document provides Korean-language documentation for the Cohere Command-R model within the Hugging Face Transformers library. It includes an overview, usage examples for model loading and generation, and API references for Cohere-related classes.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/code_llama.md",
            "description": "This document provides Korean-language documentation for the Code Llama model within the Hugging Face Transformers library. It describes Code Llama's features, usage examples for code generation and infilling, quantization, and tokenizer details.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/gpt2.md",
            "description": "This file is a Korean-language documentation page for the GPT-2 model within the Hugging Face Transformers library. It provides an overview of GPT-2, usage examples for text generation and quantization, and references to specific model classes and configurations.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/llama2.md",
            "description": "This file is a Korean-language documentation page for the Llama 2 model within the Hugging Face Transformers library, providing an overview, usage tips, and API references for its components like `LlamaConfig`, `LlamaTokenizer`, and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/gpt_neox_japanese.md",
            "description": "This file provides Korean-language documentation for the `GPT-NeoX-Japanese` model, including an overview of its features, usage examples, and API references for its configuration, tokenizer, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/gemma.md",
            "description": "This document provides Korean-language documentation for the Gemma model within the Hugging Face Transformers library, detailing its overview, key features, and referencing various related classes like `GemmaConfig` and `GemmaTokenizer`.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/exaone4.md",
            "description": "This document provides an overview, detailed specifications, and usage examples for the EXAONE 4 large language model within the Hugging Face Transformers library. It covers its architecture, reasoning capabilities, agentic tool use, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/llama4.md",
            "description": "This document provides a Korean-language guide to Meta's Llama 4 model within the Hugging Face Transformers library, detailing its architecture, usage examples, and optimization techniques like attention methods and quantization.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/mistral.md",
            "description": "This document provides a Korean translation of the Hugging Face Transformers documentation for the Mistral language model. It covers its architecture, usage, optimization techniques, and quantization, along with API references.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/lfm2.md",
            "description": "This file provides Korean language documentation for the LFM2 model within the Hugging Face Transformers library, detailing its architecture, usage examples, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/llama.md",
            "description": "This file provides Korean-language documentation for the LLaMA model within the Hugging Face Transformers library, including an overview, usage instructions, resources, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/openai-gpt.md",
            "description": "This document provides a Korean-language overview and detailed documentation for the OpenAI GPT model within the Hugging Face Transformers library. It covers its architecture, usage tips, relevant resources, and API references for its configuration, tokenizer, and various model classes.",
            "spof": false
          },
          {
            "path": "tests/causal_lm_tester.py",
            "description": "This file provides a base testing class (`CausalLMModelTester`) and utility functions for systematically testing Causal Language Models within the Hugging Face Transformers library. It helps verify model architectures, configurations, and their integration with various tasks and pipelines.",
            "spof": false
          },
          {
            "path": "tests/utils/test_modeling_rope_utils.py",
            "description": "This file contains unit tests for the Rotary Positional Embeddings (RoPE) utility functions within the Hugging Face Transformers library. It validates the configuration parameters for various RoPE types (e.g., default, linear, dynamic, YARN) and checks the numerical correctness of their frequency and attention scale computations.",
            "spof": false
          },
          {
            "path": "tests/models/apertus/test_modeling_apertus.py",
            "description": "This file contains the PyTorch testing suite for the Apertus model within the HuggingFace Transformers library. It includes unit tests and integration tests for the model's functionality.",
            "spof": true
          },
          {
            "path": "tests/models/bloom/test_modeling_bloom.py",
            "description": "This file contains unit and integration tests for the Bloom model in the Hugging Face Transformers library. It verifies the model's architecture, attention mechanisms (including past key-values), weight initialization, and other core functionalities, ensuring correct behavior and reproduction of expected outputs.",
            "spof": true
          },
          {
            "path": "tests/models/cohere/test_modeling_cohere.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch Cohere model within the Hugging Face Transformers library, including unit tests for its configuration and core functionalities, and integration tests for specific use cases like 4-bit quantization and batched inference.",
            "spof": false
          },
          {
            "path": "tests/models/cohere2/test_modeling_cohere2.py",
            "description": "This file contains a testing suite for the PyTorch Cohere2 model, including unit tests for model configuration and functionality, as well as integration tests for various data types, pipeline usage, and attention mechanisms.",
            "spof": false
          },
          {
            "path": "tests/models/cwm/test_modeling_cwm.py",
            "description": "This file contains unit and integration tests for the CWM (Causal Waveform Model) in the Hugging Face Transformers library. It verifies the model's configuration, forward pass, sliding window attention, and text generation capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "description": "This file contains the testing suite for the PyTorch ColQwen2 model in the Hugging Face Transformers library. It includes various tests to ensure the correct functionality and integration of the ColQwen2 model.",
            "spof": false
          },
          {
            "path": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch DeepseekV3 model within the HuggingFace Transformers library, covering its modeling, generation, and pipeline functionalities.",
            "spof": false
          },
          {
            "path": "tests/models/ctrl/test_modeling_ctrl.py",
            "description": "This file contains unit tests for the Hugging Face Transformers CTRL model, covering its configuration, core model functionality, and language generation capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/cpmant/test_modeling_cpmant.py",
            "description": "This file contains unit and integration tests for the PyTorch CPMAnt model and its causal language modeling variant within the Hugging Face Transformers library. It includes a model tester, configuration tests, and checks for model inference and text generation capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/dots1/test_modeling_dots1.py",
            "description": "This file contains unit tests and integration tests for the PyTorch Dots1 model and its CausalLM variant within the Hugging Face Transformers library, including a generation test.",
            "spof": false
          },
          {
            "path": "tests/models/diffllama/test_modeling_diffllama.py",
            "description": "This file contains the testing suite for the PyTorch DiffLlama model within the Hugging Face Transformers library. It includes various unit tests for the model's configuration, core functionality, and different downstream tasks like sequence and token classification.",
            "spof": false
          },
          {
            "path": "tests/models/ernie4_5/test_modeling_ernie4_5.py",
            "description": "This file contains unit and integration tests for the PyTorch Ernie4.5 model and its Causal Language Model variant within the HuggingFace transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/blt/test_modeling_blt.py",
            "description": "This file contains unit tests and integration tests for the PyTorch implementation of the BLT model within the Hugging Face Transformers library. It includes a model tester, general model tests, and specific integration tests for functionalities like text generation and attention mechanisms.",
            "spof": true
          },
          {
            "path": "tests/models/exaone4/test_modeling_exaone4.py",
            "description": "This file contains unit tests and integration tests for the `Exaone4` model within the Hugging Face Transformers library. It verifies model logits, text generation capabilities with different attention implementations (eager, SDPA, flash attention), and static cache export functionality.",
            "spof": false
          },
          {
            "path": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "description": "This file contains the PyTorch testing suite for the FalconH1 model within the Hugging Face Transformers library. It includes various tests for the model's configuration, forward pass, and generation capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/falcon/test_modeling_falcon.py",
            "description": "This file contains unit tests for the Falcon model in the HuggingFace Transformers library, covering causal language modeling, various language generation functionalities, and model configurations.",
            "spof": false
          },
          {
            "path": "tests/models/gemma/test_modeling_gemma.py",
            "description": "This file contains unit and integration tests for the PyTorch Gemma model within the Hugging Face Transformers library. It verifies the model's functionality and text generation capabilities across various configurations, including different precisions, quantization, and attention implementations.",
            "spof": false
          },
          {
            "path": "tests/models/flex_olmo/test_modeling_flex_olmo.py",
            "description": "This file contains unit and integration tests for the PyTorch FlexOlmo model within the Hugging Face Transformers library, including tests for model output logits and greedy text generation.",
            "spof": true
          },
          {
            "path": "tests/models/glm/test_modeling_glm.py",
            "description": "This file contains a testing suite for the PyTorch GLM model within the Hugging Face Transformers library. It includes unit tests for the model's functionality and integration tests for various precision types and attention implementations like eager, SDPA, and FlashAttention 2.",
            "spof": false
          },
          {
            "path": "tests/models/gemma2/test_modeling_gemma2.py",
            "description": "This file contains unit and integration tests for the Hugging Face Transformers PyTorch Gemma2 model. It verifies the model's functionality, including different precision settings, pipeline integration, flash attention, and static cache export.",
            "spof": false
          },
          {
            "path": "tests/fixtures/gpt_oss",
            "description": "This directory is designated to hold test fixtures specifically for GPT Open-Source (GPT_OSS) models within the `transformers` library's testing suite. Although currently empty, its presence indicates a planned location for data or configurations used to test these particular models.",
            "spof": false
          },
          {
            "path": "src/transformers/modeling_gguf_pytorch_utils.py",
            "description": "This file provides utility classes and functions for processing and mapping tensors between the GGUF format and Hugging Face Transformers' PyTorch models, including architecture-specific transformations for various models like Llama, Bloom, and GPT2.",
            "spof": false
          },
          {
            "path": "src/transformers/models/apertus/modeling_apertus.py",
            "description": "This file defines the core architectural components, such as MLP, RMS normalization, Rotary Embeddings, and attention mechanisms, for the Apertus model within the Hugging Face Transformers library. It is an automatically generated file based on a modular implementation of the Apertus model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/apertus/modular_apertus.py",
            "description": "This file defines the configuration (`ApertusConfig`) and core architectural components (e.g., MLP, RMSNorm, Attention) for the Apertus model within the Hugging Face Transformers library. It builds upon existing Llama and Nemotron model structures to implement the Apertus transformer architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/arcee/__init__.py",
            "description": "This `__init__.py` file defines the `arcee` model package within the transformers library, setting up lazy loading for its configuration and modeling components. It ensures that the model's modules are only imported when explicitly accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/arcee/modular_arcee.py",
            "description": "This file defines the configuration and various model classes for the Arcee model family, primarily by inheriting and extending components from Llama and Nemotron models. It sets up the architecture for tasks like causal language modeling, sequence classification, question answering, and token classification.",
            "spof": false
          },
          {
            "path": "src/transformers/models/arcee/modeling_arcee.py",
            "description": "This file defines the core architectural components and layers, such as MLP, RMSNorm, Rotary Embeddings, and Attention mechanisms, specifically for the Arcee transformer model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bitnet/modular_bitnet.py",
            "description": "This file implements the PyTorch BitNet model, extending components from the Llama and Gemma architectures with specific modifications like `BitNetRMSNorm` and attention/MLP sub-normalization.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py",
            "description": "This script converts a Megatron-LM BLOOM model checkpoint into a PyTorch-compatible format for use with the Hugging Face Transformers library. It handles tensor parallelism (TP) and optionally shards the converted model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bloom/modeling_bloom.py",
            "description": "This file implements the PyTorch BLOOM model architecture, including its attention mechanism, custom GELU activation function, and related utility functions, as part of the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bloom/__init__.py",
            "description": "This `__init__.py` file defines the public API for the Bloom model within the Transformers library, managing the lazy loading of its configuration, modeling, and tokenization components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bloom/configuration_bloom.py",
            "description": "This file defines the `BloomConfig` class, which is used to store and manage the configuration parameters for the Bloom model architecture within the Hugging Face Transformers library. It specifies various architectural details like vocabulary size, hidden size, and number of layers.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blt/modeling_blt.py",
            "description": "This file defines the core architectural components for the BLT model within the Hugging Face Transformers library, including MLP, RMS normalization, rotary embeddings, and transformer layers. It is an auto-generated version of the modular BLT implementation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cohere/configuration_cohere.py",
            "description": "This file defines the `CohereConfig` class, which is used to store and manage the architectural configuration parameters for the Cohere language model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cohere/__init__.py",
            "description": "This `__init__.py` file defines the `cohere` model package within the Hugging Face Transformers library. It enables lazy loading of Cohere model configurations, modeling components, and tokenization utilities to optimize import times.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cohere/modular_cohere.py",
            "description": "This file implements the PyTorch Cohere model architecture within the Transformers library, adapting components from the Llama model to fit Cohere's specific requirements.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere/modeling_cohere.py",
            "description": "This file defines the core architectural components and utility functions for the Cohere large language model, used in the Hugging Face Transformers library. It includes definitions for layer normalization, rotary embeddings, MLP blocks, and attention mechanisms specific to the Cohere model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/code_llama/__init__.py",
            "description": "This `__init__.py` file defines the module structure for the Code Llama model within the Transformers library. It utilizes lazy loading to manage imports efficiently, only loading submodules like tokenization when they are first accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere2/configuration_cohere2.py",
            "description": "This file defines the `Cohere2Config` class, which is used to store and manage the configuration parameters for the Cohere model architecture, enabling its instantiation with specified arguments.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cohere2/modular_cohere2.py",
            "description": "This file defines the configuration (`Cohere2Config`) for the Cohere2 model, including its architecture parameters. It imports necessary components for building and running a Cohere2 model within the Hugging Face Transformers framework.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cohere2/modeling_cohere2.py",
            "description": "This file defines the core architectural components, such as attention mechanisms, rotary embeddings, and layer normalization, for the Cohere2 model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cohere2/__init__.py",
            "description": "This `__init__.py` file sets up the Cohere2 model module within the Transformers library, enabling lazy loading of its configuration and modeling components to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cpmant/modeling_cpmant.py",
            "description": "This file implements the core PyTorch model architecture and its building blocks (such as attention, layer normalization, and feed-forward networks) for the CPMAnt model within the HuggingFace Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ctrl/configuration_ctrl.py",
            "description": "This file defines the `CTRLConfig` class, which is used to store and manage the configuration parameters for the Salesforce CTRL language model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ctrl/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `ctrl` model package, lazily loading its submodules for configuration, modeling, and tokenization to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ctrl/modeling_ctrl.py",
            "description": "This file implements the PyTorch CTRL (Conditional Transformer Language) model, including its core components like multi-head attention and encoder layers. It defines the model architecture and its forward pass for the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_v3/__init__.py",
            "description": "Initializes the `deepseek_v3` package within the Hugging Face Transformers library, setting up lazy loading for its configuration and model components to improve import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/diffllama/configuration_diffllama.py",
            "description": "This file defines the `DiffLlamaConfig` class, which is used to store and manage the architectural configuration for the DiffLlama model, inheriting from `PreTrainedConfig`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/diffllama/__init__.py",
            "description": "This `__init__.py` file defines the `diffllama` model package within the Hugging Face Transformers library. It sets up lazy loading for the model's configuration and modeling components to improve import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/diffllama/modular_diffllama.py",
            "description": "This file implements the core modular components of the DiffLlama model, including its MLP, Rotary Embedding, and Attention mechanisms, building upon existing Llama and Mistral architectures within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/diffllama/modeling_diffllama.py",
            "description": "This file defines the core architectural components, including the MLP, Rotary Position Embedding, and multi-headed attention mechanism, for the DiffLlama model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts original DialoGPT PyTorch checkpoints by renaming a specific key (`lm_head.decoder.weight` to `lm_head.weight`) to be compatible with the Hugging Face Transformers library. It processes 'small', 'medium', and 'large' DialoGPT models.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dots1/modeling_dots1.py",
            "description": "This file implements core architectural components, such as RMS Normalization, Rotary Positional Embeddings, and attention mechanisms, specifically for the Dots1 Transformer model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/doge/modeling_doge.py",
            "description": "This file defines the core modeling components and architecture for the 'Doge' family of transformer models, including layers like RMSNorm, Rotary Embeddings, and attention mechanisms. It is automatically generated from `modular_doge.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/doge/configuration_doge.py",
            "description": "This file defines the `DogeConfig` class, which specifies the architecture and hyperparameters for the Doge language model. It is automatically generated and should not be edited manually.",
            "spof": false
          },
          {
            "path": "src/transformers/models/doge/convert_doge_weights_to_hf.py",
            "description": "This script converts a Doge model's weights and configuration from their original format (likely in safetensors) into a format compatible with Hugging Face Transformers. It remaps the state dictionary keys and saves the model in the Hugging Face `DogeForCausalLM` format.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5/modular_ernie4_5.py",
            "description": "This file implements the PyTorch Ernie 4.5 model components, including its rotary embeddings, MLP, and attention mechanisms, by adapting and extending modules from Llama and OLMo models.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "description": "This file implements the core modeling components (such as Rotary Position Embedding, MLP, and multi-headed attention) for the Ernie 4.5 model within the Hugging Face Transformers library. It is an auto-generated file from `modular_ernie4_5.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/exaone4/configuration_exaone4.py",
            "description": "This file defines the `Exaone4Config` class, which specifies the architecture and hyper-parameters for the EXAONE 4.0 model, inheriting from `PreTrainedConfig`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/exaone4/modeling_exaone4.py",
            "description": "This file defines the core modeling components for the Exaone4 model in the Hugging Face Transformers library, including its RMS normalization, rotary positional embeddings, and attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon/__init__.py",
            "description": "This `__init__.py` file defines the `falcon` model package for the Hugging Face Transformers library. It uses lazy loading to import the `configuration_falcon` and `modeling_falcon` modules, optimizing import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/falcon/configuration_falcon.py",
            "description": "This file defines the `FalconConfig` class, which is used to store and manage the configuration parameters for the Falcon model architecture within the Hugging Face Transformers library. It specifies hyperparameters like vocabulary size, hidden dimensions, number of layers, and attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon/modeling_falcon.py",
            "description": "This file implements the core PyTorch model architecture for the Falcon large language model, including its attention mechanism, rotary embeddings, and other fundamental layers.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon/convert_custom_code_checkpoint.py",
            "description": "This script converts custom Falcon model checkpoints to a format compatible with the standard Hugging Face Transformers library, improving performance and removing the need for `trust_remote_code=True`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon_h1/configuration_falcon_h1.py",
            "description": "This file defines the configuration class for the FalconH1 model, specifying its architecture and various hyperparameters. It allows for instantiating the FalconH1 model with a predefined set of parameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/falcon_h1/__init__.py",
            "description": "This `__init__.py` file defines the `falcon_h1` model package within the Transformers library. It uses a lazy loading mechanism to import the model's configuration and modeling components, optimizing the initial import time for the package.",
            "spof": true
          },
          {
            "path": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "description": "This file defines the `FlexOlmoConfig` class, which stores the configuration for the FlexOlmo model within the Hugging Face Transformers library. It specifies architectural parameters and integrates components from existing OLMo-based models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flex_olmo/__init__.py",
            "description": "This `__init__.py` file defines the Flex-OLMo model within the Hugging Face Transformers library, using a lazy loading mechanism for its configuration and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "description": "This file defines the core architectural components (like RMS normalization, rotary embeddings, and MLP) for the FlexOlmo model within the HuggingFace Transformers library. It is automatically generated from `modular_flex_olmo.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma/configuration_gemma.py",
            "description": "This file defines the `GemmaConfig` class, which specifies the architecture and hyper-parameters for the Gemma large language model. It inherits from `PreTrainedConfig` and provides default values for various model components.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the Gemma model, organizing and enabling lazy loading of its core components, including configuration, modeling, and tokenization modules.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma/convert_gemma_weights_to_hf.py",
            "description": "This script converts Google's Gemma model weights and tokenizer from their original format to a format compatible with the Hugging Face Transformers library, allowing them to be loaded and used with `GemmaForCausalLM` and `GemmaTokenizer`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma/modular_gemma.py",
            "description": "This file defines the configuration and core architectural components for the Gemma model within the Transformers library, largely adapting and extending modules from the Llama model implementation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma/modeling_gemma.py",
            "description": "This file defines the core architectural components for the Gemma model, including its RMS normalization, MLP block, rotary position embeddings, and attention mechanism. It is an auto-generated file within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py",
            "description": "This script converts Gemma2 model weights and its tokenizer from their original format to the Hugging Face Transformers format, supporting different model sizes (9B, 27B) and allowing local saving or pushing to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma2/modular_gemma2.py",
            "description": "This file defines the configuration class for the Gemma2 model, specifying its architecture and parameters within the Hugging Face Transformers library. It imports necessary components for building the Gemma2 model, including attention mechanisms and core model structures.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma2/__init__.py",
            "description": "This `__init__.py` file defines the `gemma2` model package, enabling lazy loading of its configuration and modeling components. It uses HuggingFace's `_LazyModule` to manage imports efficiently.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma2/configuration_gemma2.py",
            "description": "This file defines the `Gemma2Config` class, which is used to store and manage the configuration parameters for the Gemma2 model architecture within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma2/modeling_gemma2.py",
            "description": "This file defines the architectural components and utilities for the Gemma2 model within the Hugging Face Transformers library. It includes implementations for essential layers like RMSNorm, MLP, and Rotary Embeddings, along with attention-related functions.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 21
          },
          {
            "name": "Arthur",
            "percent": 11
          },
          {
            "name": "Raushan Turganbay",
            "percent": 10
          }
        ]
      },
      "Encoder-Decoder Model Architectures": {
        "files": [
          {
            "path": "docs/source/ar/tasks/summarization.md",
            "description": "This file is an Arabic documentation page for the Hugging Face Transformers library, providing a guide on summarization tasks. It explains how to fine-tune a T5 model for abstractive summarization using the BillSum dataset.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bart.md",
            "description": "This document provides an overview and usage guide for the BART model within the Hugging Face Transformers library, detailing its architecture, capabilities, and code examples for various tasks like mask-filling and generation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/blenderbot-small.md",
            "description": "This file provides the documentation for the Blenderbot Small model, including an overview, usage tips, and API references for its configuration, tokenizers, and model classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/blenderbot.md",
            "description": "This file provides documentation for the Blenderbot model within the Hugging Face Transformers library, including its overview, usage examples, implementation details, and API references for its configuration, tokenizer, and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/barthez.md",
            "description": "This file documents the BARThez model, a French-specific BART model, including its features, usage examples, and API references for its tokenizers.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/bartpho.md",
            "description": "This file provides documentation for the BARTpho model, a large-scale Vietnamese sequence-to-sequence model, within the Hugging Face Transformers library. It includes an overview, usage examples for summarization, and details on its tokenizer.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/bigbird_pegasus.md",
            "description": "This document provides an overview and usage guide for the BigBirdPegasus model, an encoder-decoder transformer for long-input summarization, within the Hugging Face Transformers library. It includes code examples for text summarization, quantization, and details on the model's architecture and configuration classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/byt5.md",
            "description": "This document provides an overview and usage examples for the ByT5 model, a tokenizer-free version of T5 designed for direct UTF-8 byte processing. It includes code snippets for text generation, quantization, and notes on its unique character-level operations, along with auto-documentation for the ByT5Tokenizer.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/flan-ul2.md",
            "description": "This document provides an overview of the FLAN-UL2 model, an encoder-decoder model based on T5 architecture, highlighting its improvements over UL2 and offering guidance on its usage, including low-resource deployment with Hugging Face Transformers.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/flan-t5.md",
            "description": "This file provides documentation for the FLAN-T5 model within the Hugging Face Transformers library. It introduces the model, offers usage examples, lists available variants, and links to further resources.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/fsmt.md",
            "description": "This file provides documentation for the FSMT (FairSeq MachineTranslation) model within the Hugging Face Transformers library, detailing its overview, implementation notes, and API references for its configuration, tokenizer, and models.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/encoder-decoder.md",
            "description": "This documentation file describes Encoder Decoder Models in Hugging Face Transformers, detailing their initialization, usage for tasks like summarization and translation, and fine-tuning. It provides code examples for various methods of interaction, including pipelines and direct model instantiation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/led.md",
            "description": "This file provides documentation for the Longformer-Encoder-Decoder (LED) model within the Hugging Face Transformers library, detailing its features, usage examples for summarization, and advanced topics like quantization and fine-tuning.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/longt5.md",
            "description": "This file provides the documentation for the LongT5 model within the Hugging Face Transformers library. It details the model's overview, architecture, usage tips, and available classes like `LongT5ForConditionalGeneration`.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/madlad-400.md",
            "description": "This file provides documentation for the MADLAD-400 machine translation model within the Hugging Face Transformers library, detailing its overview, capabilities, and usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mbart.md",
            "description": "This file provides documentation for the mBART model within the Hugging Face Transformers library. It includes an overview, usage examples for text translation, and API references for its configuration, tokenizers, and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/marian.md",
            "description": "This file provides documentation for the MarianMT model within the Hugging Face Transformers library. It details the model's characteristics, usage examples for translation, and specific notes on multilingual models and tokenizer handling.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/nllb.md",
            "description": "This documentation file describes the NLLB (No Language Left Behind) multilingual translation model, providing an overview of its features, usage examples for translation and quantization, and details on its tokenizer within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/m2m_100.md",
            "description": "This file provides documentation for the M2M100 multilingual machine translation model, including an overview, usage examples for training and generation, and details on integrating Flash Attention 2 and SDPA for optimized performance.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/myt5.md",
            "description": "This file provides documentation for the MyT5 model, a multilingual language model based on T5 architecture, and its associated MyT5Tokenizer, detailing its underlying 'MYTE' representation and key tokenizer methods.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mvp.md",
            "description": "This file provides documentation for the MVP (Multi-task Supervised Pre-training for Natural Language Generation) model, including an overview, usage tips, code examples, and API references for its configurations and classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mt5.md",
            "description": "This file provides documentation for the mT5 model within the Hugging Face Transformers library. It describes the model's features, offers usage examples for text summarization with and without quantization, and references relevant mT5 classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pegasus_x.md",
            "description": "This file provides documentation for the PEGASUS-X model within the Hugging Face Transformers library. It describes the model's capabilities for long-input summarization and offers usage examples, including quantization techniques.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/plbart.md",
            "description": "This file is a documentation page for the PLBart model, providing an overview, usage examples for code-to-text, text-to-code, and code-to-code tasks, and API references for its configuration, tokenizer, and various model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/prophetnet.md",
            "description": "This file is a documentation page for the ProphetNet model within the Hugging Face Transformers library. It provides an overview of the model, usage tips, and API references for its configuration, tokenizer, and various model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pegasus.md",
            "description": "This file documents the Pegasus model for abstractive summarization within the Hugging Face Transformers library. It provides an overview of the model, code examples for its usage (including summarization and quantization), and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/t5gemma.md",
            "description": "This file provides documentation for the T5Gemma model within the Hugging Face Transformers library. It details the model's architecture, available variants, training objectives, and offers code examples for its usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/ul2.md",
            "description": "This file provides documentation for the UL2 model, detailing its overview, original paper abstract, and usage tips within the Hugging Face Transformers library. It also highlights its architectural similarities to T5v1.1.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/t5gemma2.md",
            "description": "This file documents the T5Gemma 2 model within the Hugging Face Transformers library, detailing its features, architecture, and providing code examples for its usage. It also includes auto-generated API documentation for related T5Gemma 2 classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/t5.md",
            "description": "This file provides documentation for the T5 model, detailing its architecture, capabilities for various NLP tasks, usage examples with the Hugging Face Transformers library, and quantization methods.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/t5v1.1.md",
            "description": "This file provides documentation for the T5v1.1 model, an improved version of the original T5 model, within the Hugging Face Transformers library. It details its features, usage, and available pre-trained variants.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/umt5.md",
            "description": "This file provides documentation for the UMT5 (Unimax mT5) model within the Hugging Face Transformers library, detailing its overview, usage, and differences from mT5.",
            "spof": false
          },
          {
            "path": "docs/source/es/tasks/summarization.md",
            "description": "This file is a Spanish-language documentation page providing a guide on text summarization using the Hugging Face Transformers library. It explains summarization concepts and demonstrates how to fine-tune a T5 model for abstractive summarization with the BillSum dataset.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/summarization.md",
            "description": "This document provides a Japanese guide on summarization tasks using the Hugging Face Transformers library, specifically demonstrating how to fine-tune a T5 model for abstractive summarization on the BillSum dataset.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bartpho.md",
            "description": "This document, written in Japanese, provides an overview, usage examples, and tips for the BARTpho model, a pre-trained sequence-to-sequence model for Vietnamese. It also includes information on the `BartphoTokenizer`.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/blenderbot-small.md",
            "description": "This file provides the Japanese documentation for the Blenderbot Small model within the Transformers library. It includes an overview of the model, its usage with specific checkpoints, and API references for its configuration, tokenizer, and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/blenderbot.md",
            "description": "This file provides Japanese documentation for the Blenderbot model within the Hugging Face Transformers library. It includes an overview, implementation details, usage examples, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/barthez.md",
            "description": "This document provides an overview of the BARThez French sequence-to-sequence model in Japanese, including its origin, key features, and specific tokenizer implementations within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bigbird_pegasus.md",
            "description": "This file provides Japanese-language documentation for the BigBirdPegasus model within the Hugging Face Transformers library, detailing its overview, usage tips, and API references for various BigBirdPegasus classes.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/byt5.md",
            "description": "This file provides Japanese documentation for the ByT5 model within the Hugging Face Transformers library, offering an overview, usage examples, and details about its tokenizer.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bart.md",
            "description": "This document provides a comprehensive Japanese-language guide to the BART (Bidirectional Auto-Regressive Transformer) model, including its overview, implementation details, usage examples, and API references within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/bartpho.md",
            "description": "This document provides an overview, usage examples, and tips for the BARTpho model and its tokenizer within the Hugging Face Transformers library, specifically for Vietnamese language tasks. It details how to use BARTpho for sequence-to-sequence generation and masking tasks.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/bart.md",
            "description": "This file provides Korean-language documentation for the BART (Bidirectional Auto-Regressive Transformers) model within the Hugging Face Transformers library, including an overview, usage tips, implementation notes, and links to resources.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/marian.md",
            "description": "This document provides Korean-language documentation for the MarianMT model within the Hugging Face Transformers library, detailing its implementation, usage, naming conventions, and specific class references.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/encoder-decoder.md",
            "description": "This file provides Korean documentation for the `EncoderDecoderModel` in the Transformers library, explaining its overview, initialization methods (from configurations or pre-trained models), inference, and training processes for sequence-to-sequence tasks.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/barthez.md",
            "description": "This document provides an overview and documentation for the BARThez model and its associated tokenizers (BarthezTokenizer, BarthezTokenizerFast) in Korean. It details the model's origin, its purpose for French sequence-to-sequence tasks, and how it relates to the BART model.",
            "spof": true
          },
          {
            "path": "tests/models/bart/test_modeling_bart.py",
            "description": "This file contains unit tests for the PyTorch implementation of the BART model within the Hugging Face Transformers library. It includes model configuration testing, checks for encoder/decoder components, and tests for specific model heads like sequence classification.",
            "spof": false
          },
          {
            "path": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "description": "This file contains unit tests for the PyTorch implementation of the Blenderbot model in the Hugging Face Transformers library. It tests various functionalities including model configuration, saving/loading, and encoder-decoder behaviors.",
            "spof": false
          },
          {
            "path": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch BigBirdPegasus model, including configuration, generation, and pipeline testing functionalities within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "description": "This file contains unit tests for the PyTorch BlenderbotSmall model within the Hugging Face Transformers library. It verifies the model's configuration, core functionalities, generation capabilities, and pipeline integration.",
            "spof": false
          },
          {
            "path": "tests/models/dia/test_modeling_dia.py",
            "description": "This file contains the testing suite for the PyTorch Dia model in the Hugging Face Transformers library. It includes various tests for model configuration, forward passes, generation capabilities, and standalone encoder/decoder functionality.",
            "spof": true
          },
          {
            "path": "tests/models/fsmt/test_modeling_fsmt.py",
            "description": "This file contains unit tests for the FSMT (Fairseq Machine Translation) model implementation within the Hugging Face Transformers library, covering its configuration, initialization, forward pass, and saving/loading functionalities.",
            "spof": false
          },
          {
            "path": "tests/fixtures/tests_samples/xsum",
            "description": "This empty directory serves as a placeholder for test samples specifically related to the XSum dataset within the `transformers` library's test suite. It is part of the test fixtures used to define data for testing various functionalities, particularly those involving summarization models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bart/__init__.py",
            "description": "This `__init__.py` file serves as the package entry point for the BART model, utilizing lazy loading to import its configuration, tokenizer (from Roberta), and modeling components as needed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts original BART model checkpoints from the Fairseq format to the Hugging Face Transformers PyTorch format, allowing them to be used with the Hugging Face library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bart/configuration_bart.py",
            "description": "This file defines the `BartConfig` class, which is used to store and manage the configuration parameters for instantiating a BART model. It specifies the architecture details and default values for various model components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bart/modeling_bart.py",
            "description": "This file implements the core PyTorch architectural components for the BART (Bidirectional and Auto-Regressive Transformers) model, including custom embedding layers, attention mechanisms, and utility functions for sequence processing within the HuggingFace Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/barthez/__init__.py",
            "description": "This `__init__.py` file defines the `barthez` model package within the transformers library, enabling lazy loading of its components, such as `tokenization_barthez`, to optimize import times. It sets up the module structure using Hugging Face's internal lazy loading mechanism.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py",
            "description": "This file defines the `BigBirdPegasusConfig` class, which is used to store and manage the configuration parameters for the BigBirdPegasus model within the Hugging Face Transformers library. It specifies architectural details such as vocabulary size, layer dimensions, attention heads, and the type of attention mechanism to be used.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "description": "This file implements the PyTorch model architecture and essential components for the BigBirdPegasus transformer model, including attention mechanisms and embeddings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py",
            "description": "This script converts a TensorFlow BigBirdPegasus model checkpoint to a PyTorch model, making it compatible with the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bartpho/__init__.py",
            "description": "This `__init__.py` file defines the `bartpho` package within the `transformers` library. It implements a lazy loading mechanism for the package's submodules, specifically `tokenization_bartpho`, to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "description": "This file implements the PyTorch model architecture and components for the BlenderbotSmall model, including its attention mechanism and positional embeddings, within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blenderbot_small/__init__.py",
            "description": "This `__init__.py` file defines the `blenderbot_small` model package, setting up lazy imports for its configuration, modeling, and tokenization components to optimize loading performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blenderbot_small/configuration_blenderbot_small.py",
            "description": "This file defines the configuration class `BlenderbotSmallConfig` for the BlenderbotSmall model. It stores architectural parameters used to instantiate and configure a BlenderbotSmall model within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blenderbot/__init__.py",
            "description": "This `__init__.py` file defines the `blenderbot` module within the `transformers` library, enabling lazy loading of its core components (configuration, model, and tokenizer) for improved import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blenderbot/configuration_blenderbot.py",
            "description": "This file defines the `BlenderbotConfig` class, which is used to store and manage the configuration parameters for the Blenderbot model architecture within the Hugging Face Transformers library. It specifies hyperparameters like vocabulary size, layer dimensions, dropout rates, and activation functions.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "description": "This file implements the PyTorch model architecture components for the Blenderbot model within the Hugging Face Transformers library, including its attention mechanism, embeddings, and utility functions.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts original Blenderbot PyTorch checkpoints (likely from ParlAI) into the Hugging Face Transformers format, renaming state dictionary keys to match the library's expected structure.",
            "spof": false
          },
          {
            "path": "src/transformers/models/byt5/__init__.py",
            "description": "This `__init__.py` file sets up the `byt5` module for lazy loading within the Hugging Face Transformers library. It defines the module's import structure, particularly for its tokenization components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cpmant/__init__.py",
            "description": "This `__init__.py` file for the `cpmant` model in the `transformers` library sets up lazy loading for its configuration, modeling, and tokenization components to optimize import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cpm/__init__.py",
            "description": "This `__init__.py` file defines the `cpm` package within the transformers library, implementing lazy loading for its submodules. It primarily prepares for the lazy import of the `tokenization_cpm` module.",
            "spof": true
          },
          {
            "path": "src/transformers/models/encoder_decoder/__init__.py",
            "description": "This file defines the `encoder_decoder` model module for the HuggingFace Transformers library, enabling lazy loading of its configuration and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/encoder_decoder/configuration_encoder_decoder.py",
            "description": "This file defines the `EncoderDecoderConfig` class, which serves as the configuration for EncoderDecoder models in the Hugging Face Transformers library. It encapsulates the configurations for both the encoder and decoder components of a sequence-to-sequence model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "description": "This file defines the `EncoderDecoderModel` class, which combines a separate encoder and decoder model into a single architecture for sequence-to-sequence tasks within the Hugging Face Transformers library. It includes methods for initialization, weight management, and handling input/output embeddings for such models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fsmt/__init__.py",
            "description": "This file defines the FSMT model's module structure, enabling lazy loading of its configuration, modeling, and tokenization components for the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fsmt/configuration_fsmt.py",
            "description": "This file defines the configuration classes (`FSMTConfig` and `DecoderConfig`) for the FSMT (Fairseq Machine Translation) model. These classes specify the model's architecture and hyperparameters within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fsmt/modeling_fsmt.py",
            "description": "This file implements the PyTorch FSMT (Fairseq Machine Translation) model, ported from the original WMT19 Fairseq implementation. It includes the model architecture, initialization, and utility functions for preparing inputs and masks, primarily for machine translation tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts a pre-trained FSMT model checkpoint from its original Fairseq format into a format compatible with Hugging Face Transformers, including model weights, configuration, and tokenizer files.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 16
          },
          {
            "name": "Anton Vlasjuk",
            "percent": 16
          },
          {
            "name": "Raushan Turganbay",
            "percent": 13
          }
        ]
      },
      "Protein Language Models": {
        "files": [
          {
            "path": "docs/source/en/model_doc/biogpt.md",
            "description": "This file provides documentation for the BioGPT model in Hugging Face Transformers, detailing its purpose, usage examples for text generation and quantization, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/esm.md",
            "description": "This file provides comprehensive documentation for the ESM family of Transformer protein language models (ESM-1b, ESM-1v, ESM-2, ESMFold) within the Hugging Face Transformers library. It details their functionality, research background, and includes API references for relevant classes and methods.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/evolla.md",
            "description": "This file provides documentation for the Evolla model within the Hugging Face Transformers library, detailing its overview, functionality, and usage examples. It describes Evolla as an 80-billion-parameter protein-language generative model designed to decode the molecular language of proteins.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/biogpt.md",
            "description": "This document provides the Japanese-language documentation for the BioGPT model within the Hugging Face Transformers library. It includes an overview, usage tips, and references to its configuration, tokenizer, and various model classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/biogpt.md",
            "description": "This file provides the Korean documentation for the BioGPT model, detailing its overview, usage tips including SDPA, resources, and API references for its configuration, tokenizer, and various model classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/esm.md",
            "description": "This document provides Korean-language documentation for the ESM (Evolutionary Scale Modeling) family of protein language models, including ESMFold and ESM-2, within the Hugging Face Transformers library. It details their overview, usage tips, and API references for configuration, tokenization, and various model applications like masked language modeling and protein folding.",
            "spof": false
          },
          {
            "path": "tests/models/biogpt/test_modeling_biogpt.py",
            "description": "This file contains the testing suite for the PyTorch BioGPT model, including tests for its configuration, various model types (e.g., CausalLM, TokenClassification), and attention mechanisms.",
            "spof": false
          },
          {
            "path": "tests/models/esm/test_modeling_esm.py",
            "description": "This file contains unit tests for the PyTorch ESM (Evolutionary Scale Modeling) model within the Hugging Face Transformers library. It thoroughly checks the model's functionality, configuration, various task-specific heads (like masked language modeling and token classification), and features such as gradient checkpointing.",
            "spof": false
          },
          {
            "path": "tests/models/esm/test_modeling_esmfold.py",
            "description": "This file contains unit tests and integration tests for the `EsmForProteinFolding` model within the Hugging Face Transformers library. It verifies the model's configuration, structure, and inference capabilities related to protein folding.",
            "spof": false
          },
          {
            "path": "tests/models/esm/test_tokenization_esm.py",
            "description": "This file contains unit tests for the `EsmTokenizer` class, ensuring its tokenization, encoding, padding, and special token handling functionalities work as expected. It verifies various aspects of the tokenizer's behavior, including adding new tokens and special tokens.",
            "spof": false
          },
          {
            "path": "tests/models/evolla/test_processing_evolla.py",
            "description": "This file contains unit tests for the `EvollaProcessor` in the Hugging Face Transformers library. It verifies the processor's functionality in handling various protein sequences and message inputs, ensuring correct tokenization and output formatting.",
            "spof": true
          },
          {
            "path": "src/transformers/models/biogpt/__init__.py",
            "description": "This `__init__.py` file defines the entry point for the BioGPT model in the Hugging Face Transformers library. It sets up lazy loading for the model's configuration, modeling, and tokenization components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/biogpt/modeling_biogpt.py",
            "description": "This file implements key architectural components, including learned positional embeddings, scaled word embeddings, and multi-headed attention, for the BioGPT model within the Transformers library. It is an automatically generated file derived from `modular_biogpt.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/biogpt/configuration_biogpt.py",
            "description": "This file defines the BioGptConfig class, which is used to store and manage the configuration parameters for the BioGPT model within the Hugging Face Transformers library. It specifies the architecture details and default values for instantiating a BioGPT model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/biogpt/modular_biogpt.py",
            "description": "This file defines the PyTorch BioGPT model architecture by inheriting and adapting components from BART and OPT models. It includes modular classes for embeddings, attention mechanisms, and decoder layers, culminating in the `BioGptModel`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/biogpt/convert_biogpt_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts an original PyTorch checkpoint of a BioGPT model into a format compatible with the Hugging Face Transformers library, generating the model configuration, tokenizer files, and PyTorch weights.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/__init__.py",
            "description": "This `__init__.py` file sets up the lazy import structure for the ESM models within the Hugging Face Transformers library. It defines which submodules (like configuration, modeling, and tokenization) belong to the `esm` package.",
            "spof": true
          },
          {
            "path": "src/transformers/models/esm/tokenization_esm.py",
            "description": "This file implements the `EsmTokenizer` class for the ESM (Evolutionary Scale Modeling) model. It handles the tokenization process, including vocabulary loading, token-to-ID conversion, and managing special tokens for sequence preparation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/modeling_esmfold.py",
            "description": "This file defines the architecture and utility components for the ESMFold protein folding model, including custom layers and the output structure for protein folding predictions.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/modeling_esm.py",
            "description": "This file implements the PyTorch modules and helper functions for the ESM (Evolutionary Scale Modeling) model within the Hugging Face Transformers library. It includes components like embeddings, rotary positional embeddings, and a contact prediction head.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/convert_esm.py",
            "description": "This script converts pre-trained ESM (Evolutionary Scale Modeling) checkpoints from the original Facebook ESM library format to the Hugging Face Transformers library format. It supports various ESM models, including ESM-1b, ESM-1v, ESM-2, and ESMFold, adapting their weights and configurations for use within the Transformers ecosystem.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/configuration_esm.py",
            "description": "This file defines the configuration classes for the ESM (Evolutionary Scale Modeling) and ESMFold models, specifying their architectural parameters and hyperparameters within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/protein.py",
            "description": "This file defines a `Protein` dataclass to represent protein structures and provides utilities for converting protein data to and from PDB format, including parsing from ProteinNet strings and generating PDB output.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/loss.py",
            "description": "This file provides utility functions for calculating protein structure prediction metrics, including Predicted Aligned Error (PAE) and TM-score, from model logits. These calculations are essential for evaluating the accuracy of predicted protein structures.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/feats.py",
            "description": "This file contains utility functions for generating and transforming features related to protein structure, including pseudo-beta atom calculations, atom representation conversions, and handling of template and MSA features. It also includes functions for converting torsion angles to rigid body frames and applying these frames to atom positions, likely for protein structure prediction models.",
            "spof": true
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/__init__.py",
            "description": "This `__init__.py` file serves as the public interface for the `openfold_utils` package, consolidating various utilities for protein structure manipulation, feature engineering, geometric transformations, and loss computations. It makes essential components for protein folding models readily accessible.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Anton Vlasjuk",
            "percent": 26
          },
          {
            "name": "Raushan Turganbay",
            "percent": 23
          },
          {
            "name": "Cyril Vallez",
            "percent": 19
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 714,
      "spofCount": 316
    },
    "busFactor": 11,
    "authorCount": 141
  },
  "Utility": {
    "description": "",
    "functions": {
      "General Utility": {
        "files": [
          {
            "path": "utils/modular_integrations.py",
            "description": "This file provides utility functions and `libcst` transformers to convert between relative and absolute Python import statements, facilitating modular code management within a package.",
            "spof": true
          },
          {
            "path": "utils/modular_model_detector.py",
            "description": "This file implements a utility to detect code similarities between different model implementations in the Transformers library. It uses embedding and token-based similarity metrics to identify candidates for modularization.",
            "spof": false
          },
          {
            "path": "docs/source/en/internal/file_utils.md",
            "description": "This documentation file lists and describes general utility functions, enums, namedtuples, and special decorators found in the `utils.py` module of the Transformers library, primarily for those studying the library's internal code.",
            "spof": true
          },
          {
            "path": "docs/source/ja/internal/file_utils.md",
            "description": "This documentation file, written in Japanese, lists and describes the general utility functions found in the `transformers/utils.py` module. It serves as an internal reference for understanding common code within the library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/internal/file_utils.md",
            "description": "This document lists general utility functions from the `utils.py` file within the Transformers library, primarily for internal use or understanding the library's codebase. It covers Enums, namedtuples, special decorators, and other utility functions.",
            "spof": true
          },
          {
            "path": "tests/repo_utils/test_check_docstrings.py",
            "description": "This file contains unit tests for the `check_docstrings` utility module, focusing on functions that generate and manipulate docstring descriptions for function arguments, especially concerning default values.",
            "spof": true
          },
          {
            "path": "tests/utils/test_attention_visualizer.py",
            "description": "This file contains unit tests for the `AttentionMaskVisualizer` utility. It verifies the correct visualization of attention masks for both multimodal (PaliGemma) and text-only (Llama) transformer models.",
            "spof": true
          },
          {
            "path": "tests/utils/test_activations.py",
            "description": "This file contains unit tests for activation functions and the `get_activation` utility in the Hugging Face Transformers library, primarily focusing on different GELU implementations and ensuring proper function retrieval.",
            "spof": true
          },
          {
            "path": "tests/utils/test_import_utils.py",
            "description": "This file contains unit tests for the `clear_import_cache` utility function in the `transformers` library. It verifies that the function correctly removes `transformers` modules from `sys.modules` and that they can be subsequently re-imported.",
            "spof": true
          },
          {
            "path": "tests/utils/test_versions_utils.py",
            "description": "This file contains unit tests for the `require_version` and `require_version_core` utility functions, which are responsible for validating package and Python version requirements within the `transformers` library.",
            "spof": false
          },
          {
            "path": "tests/utils/test_logging.py",
            "description": "This file contains unit tests for the logging utilities of the Transformers library, including verbosity level settings, environment variable overrides for logging, advisory warnings, and progress bar controls.",
            "spof": true
          },
          {
            "path": "tests/utils/test_dynamic_module_utils.py",
            "description": "This file contains unit tests for the `get_imports` utility function, ensuring it correctly identifies and extracts import statements from various Python code structures, including those nested in functions and `try-except` blocks.",
            "spof": true
          },
          {
            "path": "tests/utils/test_skip_decorators.py",
            "description": "This file contains unit tests to validate the correct behavior and stacking order of custom skip decorators (e.g., `@slow`, `@require_torch_accelerator`) in both `unittest` and `pytest` environments, particularly when combined with `parameterized` decorators.",
            "spof": true
          },
          {
            "path": "tests/utils/test_image_utils.py",
            "description": "This file contains unit tests for the image utility functions within the `transformers` library, focusing on image conversion between PIL, NumPy, and PyTorch tensors, as well as functions for handling lists and batches of images.",
            "spof": false
          },
          {
            "path": "tests/utils/test_deprecation.py",
            "description": "This file contains unit tests for the `deprecate_kwarg` decorator in the Transformers library. It verifies that the decorator correctly handles keyword argument renaming, generates appropriate warnings or errors based on versioning, and is compatible with PyTorch's `torch.compile`.",
            "spof": false
          },
          {
            "path": "tests/utils/test_import_structure.py",
            "description": "This file contains unit tests for the `transformers.utils.import_utils` module, verifying the correct definition, spreading, and parsing of import structures, including backend-specific conditions and consistency with `__all__` declarations in model modules.",
            "spof": true
          },
          {
            "path": "tests/utils/test_hf_argparser.py",
            "description": "This file contains unit tests for the `HfArgumentParser` utility in the Transformers library. It verifies that the `HfArgumentParser` correctly converts Python dataclasses into `argparse.ArgumentParser` objects and handles various argument types, defaults, enums, literals, and lists.",
            "spof": false
          },
          {
            "path": "tests/utils/import_structures/import_structure_register_with_comments.py",
            "description": "This file defines classes and functions decorated with `@requires()` for testing the `transformers.utils.import_utils.requires` decorator, including various configurations and comments.",
            "spof": true
          },
          {
            "path": "tests/utils/import_structures/import_structure_raw_register.py",
            "description": "This file defines several classes and functions decorated with the `@requires` decorator, demonstrating its usage for conditional imports based on backend dependencies in the Transformers library. It likely serves as a test or example for the import utility's functionality.",
            "spof": true
          },
          {
            "path": "tests/utils/import_structures/failing_export.py",
            "description": "This file contains a test class designed to fail import if a non-existent backend dependency is specified, testing the `transformers.utils.import_utils.requires` decorator's behavior.",
            "spof": true
          },
          {
            "path": "tests/utils/import_structures/import_structure_raw_register_with_versions.py",
            "description": "This file defines various classes and functions decorated with `@requires`, each demonstrating different version constraints for 'torch' and 'accelerate' backends. It likely serves as a test case for conditional import mechanisms based on package versions.",
            "spof": true
          },
          {
            "path": "tests/utils/import_structures/import_structure_register_with_duplicates.py",
            "description": "This file defines multiple classes and functions decorated with the `@requires` decorator, each specifying duplicate backend requirements (e.g., `('torch', 'torch')`). It likely serves as a test case to verify the `requires` decorator's behavior when encountering redundant backend specifications, particularly within an import structure test context.",
            "spof": true
          },
          {
            "path": "src/transformers/hf_argparser.py",
            "description": "This file provides `HfArgumentParser`, an extension of `argparse.ArgumentParser` that simplifies argument parsing by generating command-line arguments directly from dataclass definitions. It also includes `HfArg` for enhanced dataclass field configuration and utilities for type handling.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/import_utils.py",
            "description": "This file provides utility functions for checking the availability and versions of various Python packages, especially different PyTorch backends (CUDA, MPS, NPU, XPU, MLU, MUSA), and managing environment variables related to these imports.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/chat_parsing_utils.py",
            "description": "Provides utility functions, primarily `recursive_parse`, to extract and transform data from content (often strings) into a structured format based on a given JSON schema. It supports regex-based extraction, JSON parsing, and mapping rules defined within the schema.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/logging.py",
            "description": "This file provides comprehensive logging utilities for the Hugging Face Transformers library, allowing configuration of verbosity levels, handlers, and message formatting. It enables fine-grained control over how logs are produced and displayed within the library.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/deprecation.py",
            "description": "This file provides utilities for managing deprecated keyword arguments in functions and methods. It includes a decorator to warn users, rename arguments, or raise errors based on deprecation rules and version checks.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/pytest_helpers.py",
            "description": "This script parses a pytest JSON report to summarize test outcomes, especially focusing on categorizing and counting test failures by file, class, base test name, and modeling key.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/doc.py",
            "description": "This file provides utility functions for dynamically generating, formatting, and injecting documentation strings into classes and functions, especially within the Transformers library's models and their methods.",
            "spof": false
          },
          {
            "path": "src/transformers/utils/versions.py",
            "description": "This file provides utilities for performing runtime checks of package versions against specified requirements, similar to pip's dependency resolution. It ensures that required dependencies are met at runtime, raising errors if conditions are not satisfied.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/tensor_utils.py",
            "description": "This file provides utility functions for various PyTorch tensor manipulations, including reshaping, aggregation, one-hot encoding, and recursive application of functions to tensors within nested data structures, often used in deep learning models like ESM or OpenFold.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 34
          },
          {
            "name": "Arthur",
            "percent": 11
          },
          {
            "name": "Pablo Montalvo",
            "percent": 9
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 31,
      "spofCount": 19
    },
    "busFactor": 5,
    "authorCount": 18
  },
  "Multimodal Model Library": {
    "description": "An advanced collection of models that process and connect information across multiple modalities like text, images, and audio for tasks such as visual question answering and image captioning.",
    "functions": {
      "Multimodal Model Testing and Validation": {
        "files": [
          {
            "path": "utils/create_dummy_models.py",
            "description": "This utility script helps create dummy model architectures and their associated processors (tokenizers, image processors, feature extractors) within the Hugging Face Transformers library. It identifies model components from configuration classes, primarily for testing and internal development.",
            "spof": false
          },
          {
            "path": "tests/test_tokenization_mistral_common.py",
            "description": "This file contains unit tests for the Mistral Common tokenizer within the Hugging Face Transformers library. It verifies the tokenizer's functionality, including handling of chat completion requests, truncation, padding, and image-related tokenization.",
            "spof": true
          },
          {
            "path": "tests/pipelines/test_pipelines_zero_shot_image_classification.py",
            "description": "This file contains unit tests for the `ZeroShotImageClassificationPipeline` in the Transformers library, verifying its functionality with different models (e.g., CLIP, SigLIP, BLIP-2) and configurations for zero-shot image classification tasks.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_visual_question_answering.py",
            "description": "This file contains unit tests for the visual question answering pipeline in the Hugging Face Transformers library, covering various models, input formats, and functionalities.",
            "spof": false
          },
          {
            "path": "tests/models/aria/test_modeling_aria.py",
            "description": "This file contains a testing suite for the PyTorch Aria model, specifically unit tests and integration tests for `AriaForConditionalGeneration` within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/aria/test_processing_aria.py",
            "description": "This file contains unit tests for the `AriaProcessor` class, verifying its functionalities including image processing (with and without splitting), text processing, and the application of chat templates within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/aria/test_image_processing_aria.py",
            "description": "This file contains unit tests for the `AriaImageProcessor` class, verifying its image processing functionalities such as resizing, normalization, and handling various input formats (e.g., numpy arrays, PIL images).",
            "spof": false
          },
          {
            "path": "tests/models/align/test_modeling_align.py",
            "description": "This file contains the testing suite for the PyTorch ALIGN model, including tests for its vision and text components, ensuring their functionality and adherence to common model testing standards.",
            "spof": false
          },
          {
            "path": "tests/models/align/test_processing_align.py",
            "description": "This file contains unit tests for the `AlignProcessor` class in the Hugging Face Transformers library, specifically verifying its tokenizer and image processing functionalities. It sets up dummy components to test the `AlignProcessor`'s behavior.",
            "spof": true
          },
          {
            "path": "tests/models/altclip/test_processing_altclip.py",
            "description": "This file contains unit tests for the `AltCLIPProcessor` within the Hugging Face Transformers library. It verifies the functionality of the AltCLIP model's processing component.",
            "spof": true
          },
          {
            "path": "tests/models/altclip/test_modeling_altclip.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch AltCLIP model, including tests for its vision and text components, configuration, and common modeling functionalities.",
            "spof": false
          },
          {
            "path": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "description": "This file contains unit tests and integration tests for the `AyaVision` model in the Hugging Face Transformers library, covering its configuration, modeling, and text generation capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/aya_vision/test_processing_aya_vision.py",
            "description": "This file contains unit tests for the `AyaVisionProcessor` in the Hugging Face Transformers library. It verifies the processor's functionality for handling interleaved image and text inputs, including tokenization, image processing, and calculating the number of vision tokens.",
            "spof": true
          },
          {
            "path": "tests/models/blip/test_image_processing_blip.py",
            "description": "This file contains unit tests for the `BlipImageProcessor` and `BlipImageProcessorFast` classes, which are responsible for image pre-processing in the Hugging Face Transformers library. It verifies the functionality and properties of these image processors for the BLIP model.",
            "spof": false
          },
          {
            "path": "tests/models/blip/test_modeling_blip.py",
            "description": "This file contains unit tests for the PyTorch BLIP model implementations within the Hugging Face Transformers library. It specifically tests the BlipVisionModel and BlipTextModel components, along with their configurations.",
            "spof": false
          },
          {
            "path": "tests/models/blip/test_modeling_blip_text.py",
            "description": "This file contains a testing suite for the PyTorch Blip text model, including a model tester class and various unit tests to ensure its correct functionality and configuration.",
            "spof": false
          },
          {
            "path": "tests/models/blip/test_processing_blip.py",
            "description": "This file contains unit tests for the `BlipProcessor` class within the Transformers library, ensuring its correct functionality and integration with its tokenizer component.",
            "spof": true
          },
          {
            "path": "tests/models/audioflamingo3/test_processing_audioflamingo3.py",
            "description": "This file contains unit tests for the `AudioFlamingo3Processor` class, ensuring its correct functionality, including tokenizer integration, saving/loading of pre-trained models, chat template application, and audio transcription request handling.",
            "spof": true
          },
          {
            "path": "tests/models/audioflamingo3/test_modeling_audioflamingo3.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch AudioFlamingo3 model, including unit tests for its configuration and generation capabilities, as well as slow integration tests for validating processor-model alignment.",
            "spof": true
          },
          {
            "path": "tests/models/chameleon/test_processing_chameleon.py",
            "description": "This file contains unit tests for the `ChameleonProcessor` class within the Hugging Face Transformers library. It specifically tests functionalities such as special multi-modal token handling, truncation behavior, and the calculation of vision tokens for the Chameleon model.",
            "spof": true
          },
          {
            "path": "tests/models/chameleon/test_modeling_chameleon.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch implementation of the Chameleon model within the Hugging Face Transformers library. It includes various tests for model configuration, forward pass, generation capabilities, and specific vision-to-sequence functionalities.",
            "spof": false
          },
          {
            "path": "tests/models/chameleon/test_image_processing_chameleon.py",
            "description": "This file contains unit tests for the ChameleonImageProcessor and ChameleonImageProcessorFast classes, verifying their image processing functionalities across various input types and configurations.",
            "spof": true
          },
          {
            "path": "tests/models/bridgetower/test_image_processing_bridgetower.py",
            "description": "This file contains unit tests for the BridgeTower image processing classes (`BridgeTowerImageProcessor` and `BridgeTowerImageProcessorFast`), including tests for their properties and equivalence between the slow and fast implementations, both for single images and batched inputs.",
            "spof": false
          },
          {
            "path": "tests/models/bridgetower/test_processing_bridgetower.py",
            "description": "This file contains unit tests for the `BridgeTowerProcessor` in the Hugging Face Transformers library. It verifies the processor's functionality, focusing on how it handles various image and text processing arguments, including structured and unstructured keyword arguments, and their interaction with default settings.",
            "spof": false
          },
          {
            "path": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch implementation of the BridgeTower model, including its text, vision, and combined functionalities. It verifies the model's configuration, structure, and behaviors for tasks like contrastive learning, image and text retrieval, and masked language modeling.",
            "spof": false
          },
          {
            "path": "tests/models/clip/test_modeling_clip.py",
            "description": "This file contains unit tests for the CLIP Vision model, ensuring its functionality, configuration, and different components (like projection) work as expected within the Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/clip/test_processing_clip.py",
            "description": "This file contains unit tests for the `CLIPProcessor` class, which is used for preprocessing data for the CLIP model in the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/clip/test_image_processing_clip.py",
            "description": "This file contains unit tests for the `CLIPImageProcessor` and `CLIPImageProcessorFast` classes, which are responsible for image preprocessing in the Hugging Face Transformers library for CLIP models.",
            "spof": false
          },
          {
            "path": "tests/models/clip/test_tokenization_clip.py",
            "description": "This file contains unit tests for the `CLIPTokenizer` class from the `transformers` library. It verifies the tokenization and decoding functionalities of the CLIP tokenizer, including pre-trained and custom vocabulary configurations.",
            "spof": true
          },
          {
            "path": "tests/models/aimv2/test_modeling_aimv2.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch implementation of the AIMv2 model in the Hugging Face Transformers library, covering its vision, text, and combined model functionalities and configurations.",
            "spof": true
          },
          {
            "path": "tests/models/clipseg/test_processing_clipseg.py",
            "description": "This file contains unit tests for the `CLIPSegProcessor` in the Hugging Face Transformers library, verifying its functionality for processing both text and visual inputs for the CLIPSeg model.",
            "spof": true
          },
          {
            "path": "tests/models/clipseg/test_modeling_clipseg.py",
            "description": "This file contains a testing suite for the PyTorch CLIPSeg model within the Hugging Face Transformers library. It includes tests for the CLIPSeg vision and text models to ensure their correct functionality and configuration.",
            "spof": false
          },
          {
            "path": "tests/models/chinese_clip/test_processing_chinese_clip.py",
            "description": "This file contains unit tests for the `ChineseCLIPProcessor` in the Hugging Face Transformers library. It defines the setup for the processor's tokenizer and image processor components used in testing.",
            "spof": true
          },
          {
            "path": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "description": "This file contains the testing suite for the PyTorch Chinese-CLIP model within the Hugging Face Transformers library. It includes test classes and methods for validating the text and vision components of the Chinese-CLIP model.",
            "spof": false
          },
          {
            "path": "tests/models/chinese_clip/test_image_processing_chinese_clip.py",
            "description": "This file contains unit tests for the `ChineseCLIPImageProcessor` and `ChineseCLIPImageProcessorFast` classes, verifying their image processing functionalities such as resizing, cropping, and normalization.",
            "spof": false
          },
          {
            "path": "tests/models/clap/test_processing_clap.py",
            "description": "This file contains unit tests for the `ClapProcessor` class within the Hugging Face Transformers library. It verifies the correct functionality of saving/loading, and the integration of the `ClapFeatureExtractor` and a tokenizer for combined audio and text processing.",
            "spof": false
          },
          {
            "path": "tests/models/clap/test_modeling_clap.py",
            "description": "This file contains a testing suite for the PyTorch CLAP model, including tests for both the audio and text components of the model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/clap/test_feature_extraction_clap.py",
            "description": "This file contains unit tests for the `ClapFeatureExtractor` class, ensuring its correct functionality in processing audio data for the CLAP model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/cohere2_vision/test_image_processing_cohere2_vision.py",
            "description": "This file contains unit tests for the Cohere2VisionImageProcessorFast class, verifying its image processing functionalities like resizing, normalization, and handling of various image formats and channel configurations.",
            "spof": false
          },
          {
            "path": "tests/models/cohere2_vision/test_processing_cohere2_vision.py",
            "description": "This file contains unit tests for the `Cohere2VisionProcessor` in the Hugging Face Transformers library. It specifically tests the processor's ability to handle interleaved images and text messages within chat templates, though the tests are currently skipped.",
            "spof": false
          },
          {
            "path": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "description": "This file contains a comprehensive test suite for the PyTorch Cohere2Vision model within the Hugging Face Transformers library, including unit tests for model configuration and functionality, as well as integration tests for forward passes and text generation with a pre-trained model.",
            "spof": true
          },
          {
            "path": "tests/models/clvp/test_processing_clvp.py",
            "description": "This file contains unit tests for the `ClvpProcessor` class, ensuring its `ClvpFeatureExtractor` and `ClvpTokenizer` components work correctly together for processing audio and text inputs, including saving and loading functionalities.",
            "spof": true
          },
          {
            "path": "tests/models/clvp/test_modeling_clvp.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch CLVP (Contrastive Language-Vision Pretraining) model within the Hugging Face Transformers library. It includes tests for the CLVP encoder and decoder components, ensuring their correct functionality and configuration.",
            "spof": false
          },
          {
            "path": "tests/models/clvp/test_tokenization_clvp.py",
            "description": "This file contains unit tests for the `ClvpTokenizer` class within the HuggingFace Transformers library. It verifies various functionalities such as tokenization, special token handling, and padding behavior of the CLVP model's tokenizer.",
            "spof": false
          },
          {
            "path": "tests/models/clvp/test_feature_extraction_clvp.py",
            "description": "This file contains unit and integration tests for the `ClvpFeatureExtractor` class, ensuring its correct functionality for processing audio data into features for the CLVP model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/blip_2/test_modeling_blip_2.py",
            "description": "This file contains unit tests for the PyTorch BLIP-2 model components within the Hugging Face Transformers library, specifically testing the Blip2VisionModel and Blip2QFormerConfig.",
            "spof": false
          },
          {
            "path": "tests/models/blip_2/test_processing_blip_2.py",
            "description": "This file contains unit tests for the `Blip2Processor` class within the Hugging Face Transformers library. It ensures the correct functionality of the processor, including its tokenizer and image processor components, using `unittest` and `ProcessorTesterMixin`.",
            "spof": true
          },
          {
            "path": "tests/models/colqwen2/test_processing_colqwen2.py",
            "description": "This file contains a comprehensive testing suite for the `ColQwen2Processor` in the Hugging Face Transformers library. It verifies the processor's functionalities, including image and text processing, handling of different input types, and correct application of configuration parameters and kwargs.",
            "spof": false
          },
          {
            "path": "tests/models/deepseek_vl_hybrid/test_processing_deepseek_vl_hybrid.py",
            "description": "This file contains unit tests for the `DeepseekVLHybridProcessor` class, including setup for its tokenizer and verification of its processing functionalities, particularly chat templating and image token handling.",
            "spof": false
          },
          {
            "path": "tests/models/deepseek_vl_hybrid/test_image_processing_deepseek_vl_hybrid.py",
            "description": "This file contains unit tests for the `DeepseekVLHybridImageProcessor` and `DeepseekVLHybridImageProcessorFast` classes. It verifies their image processing functionalities, including resizing, normalization, and handling various input formats for high-resolution images.",
            "spof": true
          },
          {
            "path": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "description": "This file contains the testing suite for the PyTorch DeepseekVLHybrid model, ensuring its functionality, configuration, and generation capabilities are working correctly within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/colpali/test_modeling_colpali.py",
            "description": "This file contains unit tests and integration tests for the PyTorch ColPali model, specifically focusing on its retrieval capabilities within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/colpali/test_processing_colpali.py",
            "description": "This file contains unit tests for the `ColPaliProcessor` class, verifying its functionalities for processing images and text queries specific to the ColPali model.",
            "spof": false
          },
          {
            "path": "tests/models/dia/test_processing_dia.py",
            "description": "This file contains unit tests for the `DiaProcessor` class within the Hugging Face Transformers library, verifying its functionality for processing text and audio data, including tokenization, feature extraction, and audio encoding/decoding.",
            "spof": true
          },
          {
            "path": "tests/models/deepseek_vl/test_processing_deepseek_vl.py",
            "description": "This file contains unit tests for the `DeepseekVLProcessor` class, verifying its functionality including tokenizer setup and processor dictionary configuration for the Deepseek-VL model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch DeepseekVL model, including unit tests and integration tests for its various functionalities within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/deepseek_vl/test_image_processing_deepseek_vl.py",
            "description": "This file contains unit tests for the DeepseekVL image processing classes, `DeepseekVLImageProcessor` and `DeepseekVLImageProcessorFast`. It verifies their functionality, configuration, and the equivalence between their slow and fast implementations.",
            "spof": true
          },
          {
            "path": "tests/models/d_fine/test_modeling_d_fine.py",
            "description": "This file contains a testing suite for the PyTorch D-FINE model within the HuggingFace Transformers library. It defines a model tester class to prepare configurations and inputs for unit tests, ensuring the correct functionality of the D-FINE model and its related components.",
            "spof": false
          },
          {
            "path": "tests/models/csm/test_processing_csm.py",
            "description": "This file contains unit tests for the `CsmProcessor` class, verifying its functionality, particularly regarding chat templating and multimodal input processing.",
            "spof": true
          },
          {
            "path": "tests/models/emu3/test_modeling_emu3.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch Emu3 model in the Transformers library, covering both its text-to-text and vision-to-text functionalities.",
            "spof": true
          },
          {
            "path": "tests/models/emu3/test_processing_emu3.py",
            "description": "This file contains unit tests for the `Emu3Processor` class, verifying its functionality for tasks such as image processing, tokenization, and generation within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/donut/test_processing_donut.py",
            "description": "This file contains unit tests for the `DonutProcessor` class from the `transformers` library, specifically verifying its `token2json` method. It ensures the processor correctly converts a tokenized sequence into a structured JSON output.",
            "spof": false
          },
          {
            "path": "tests/models/donut/test_modeling_donut_swin.py",
            "description": "This file contains a testing suite for the PyTorch Donut Swin model within the Hugging Face Transformers library. It includes unit tests for model configuration, forward passes, image classification capabilities, and attention outputs.",
            "spof": false
          },
          {
            "path": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "description": "This file contains unit tests for the `EncoderDecoderModel` in the Hugging Face Transformers library, verifying its functionality, loading, and saving capabilities with various encoder and decoder architectures.",
            "spof": false
          },
          {
            "path": "tests/models/ernie4_5_vl_moe/test_image_processing_ernie4_5_vl_moe.py",
            "description": "This file contains unit tests for the `Ernie4_5_VL_MoeImageProcessor` and `Ernie4_5_VL_MoeImageProcessorFast` classes. It verifies their functionality, including image resizing, normalization, and processing of different image input types.",
            "spof": true
          },
          {
            "path": "tests/models/ernie4_5_vl_moe/test_processing_ernie4_5_vl_moe.py",
            "description": "This file contains unit tests for the `Ernie4_5_VL_MoeProcessor` in the Hugging Face Transformers library. It verifies functionalities such as saving/loading, image processing, tokenizer integration, and chat template application.",
            "spof": true
          },
          {
            "path": "tests/models/ernie4_5_vl_moe/test_video_processing_ernie4_5_vl_moe.py",
            "description": "This file contains unit tests for the `Ernie4_5_VL_MoeVideoProcessor` class, ensuring its video processing functionalities work correctly with various input formats (PIL, NumPy, PyTorch tensors).",
            "spof": true
          },
          {
            "path": "tests/models/ernie4_5_vl_moe/test_modeling_ernie4_5_vl_moe.py",
            "description": "This file contains the unit tests for the PyTorch implementation of the Ernie 4.5 VL MoE model within the Hugging Face Transformers library, including its configuration, modeling, and generation capabilities.",
            "spof": true
          },
          {
            "path": "tests/models/edgetam/test_modeling_edgetam.py",
            "description": "This file contains the testing suite for the PyTorch implementation of the EDGETAM model in the Hugging Face Transformers library. It includes various tester classes and unit tests to ensure the correct functionality and configuration of the EdgeTam model, its prompt encoder, and mask decoder.",
            "spof": true
          },
          {
            "path": "tests/models/gemma3n/test_processing_gemma3n.py",
            "description": "This file contains unit tests for the `Gemma3nProcessor` class, verifying its functionality for the Gemma3n model, including its audio feature extraction capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "description": "This file contains the testing suite for the PyTorch Gemma3n model, including tests for its audio encoder, feature extractor, and text-based causal language model components.",
            "spof": false
          },
          {
            "path": "tests/models/florence2/test_processing_florence2.py",
            "description": "This file contains unit tests for the `Florence2Processor` class, verifying its functionalities including prompt construction, image/text processing, quantization/dequantization of coordinates, and various post-processing methods for tasks like OCR, bounding box parsing, polygon parsing, and phrase grounding.",
            "spof": true
          },
          {
            "path": "tests/models/florence2/test_modeling_florence2.py",
            "description": "This file contains a testing suite for the PyTorch Florence2 model within the Hugging Face Transformers library, including unit tests for model configuration and functionality, as well as integration tests.",
            "spof": true
          },
          {
            "path": "tests/models/gemma3/test_image_processing_gemma3.py",
            "description": "This file contains unit tests for the `Gemma3ImageProcessor` and `Gemma3ImageProcessorFast` classes, verifying their functionality and properties within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/gemma3/test_modeling_gemma3.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch Gemma3 model within the Hugging Face Transformers library. It includes tests for causal language model functionalities, generation, and specific attention and RoPE scaling mechanisms.",
            "spof": false
          },
          {
            "path": "tests/models/gemma3/test_processing_gemma3.py",
            "description": "This file contains unit tests for the `Gemma3Processor` class in the Hugging Face Transformers library. It verifies the processor's functionalities related to multimodal input handling, including image processing, tokenization, pan-and-scan operations, and truncation for text with embedded image tokens.",
            "spof": false
          },
          {
            "path": "tests/models/fast_vlm/test_modeling_fast_vlm.py",
            "description": "This file contains a comprehensive testing suite for the FastVLM model within the Hugging Face Transformers library. It includes unit tests for model configuration, generation capabilities, and integration tests for its functionalities.",
            "spof": true
          },
          {
            "path": "tests/models/flava/test_processing_flava.py",
            "description": "This file contains unit tests for the `FlavaProcessor` in the Hugging Face Transformers library, specifically validating its image processing and tokenization functionalities.",
            "spof": true
          },
          {
            "path": "tests/models/flava/test_modeling_flava.py",
            "description": "This file contains unit tests for the `FlavaImageModel` within the Hugging Face Transformers library, ensuring its correct functionality and adherence to expected behaviors. It defines a tester class to prepare configurations and inputs, and then runs various tests inherited from `ModelTesterMixin`.",
            "spof": false
          },
          {
            "path": "tests/models/flava/test_image_processing_flava.py",
            "description": "This file contains unit tests for the `FlavaImageProcessor` and `FlavaImageProcessorFast` classes, verifying their functionality and configuration for image preprocessing in the FLAVA model. It tests various image processing properties and methods for both standard and codebook-specific operations.",
            "spof": true
          },
          {
            "path": "tests/models/git/test_modeling_git.py",
            "description": "This file contains unit tests for the Hugging Face Transformers library's Generative Image-to-text (GIT) model, specifically focusing on its vision components and overall model functionality.",
            "spof": false
          },
          {
            "path": "tests/models/git/test_processing_git.py",
            "description": "This file contains unit tests for the `GitProcessor` within the `transformers` library. It uses `ProcessorTesterMixin` to ensure proper functionality and integration of the Git model's processing component, specifically requiring vision capabilities.",
            "spof": true
          },
          {
            "path": "tests/models/fuyu/test_image_processing_fuyu.py",
            "description": "This file contains unit tests for the `FuyuImageProcessor` and `FuyuImageProcessorFast` classes within the Transformers library, ensuring their image processing functionalities work correctly across various input formats (PIL, NumPy, PyTorch) and verifying the equivalence between the slow and fast implementations.",
            "spof": true
          },
          {
            "path": "tests/models/fuyu/test_processing_fuyu.py",
            "description": "This file contains unit tests for the `FuyuProcessor` in the Hugging Face Transformers library. It verifies the correct processing of text, images, and multimodal inputs, including scenarios with and without images/text and with multiple images.",
            "spof": false
          },
          {
            "path": "tests/models/fuyu/test_modeling_fuyu.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch implementation of the Fuyu model within the Hugging Face Transformers library. It includes unit tests for model functionality, generation capabilities, and pipeline integration.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Yoni Gozlan",
            "percent": 13
          },
          {
            "name": "Raushan Turganbay",
            "percent": 11
          },
          {
            "name": "Anton Vlasjuk",
            "percent": 10
          }
        ]
      },
      "Multimodal Data Processing": {
        "files": [
          {
            "path": "utils/test_module/custom_image_processing.py",
            "description": "This file defines a custom image processor, `CustomImageProcessor`, which inherits from `CLIPImageProcessor`. It likely serves as a placeholder or a starting point for specialized image processing logic building upon the base `CLIPImageProcessor` within the `transformers` library.",
            "spof": true
          },
          {
            "path": "utils/test_module/custom_video_processing.py",
            "description": "This file defines a custom video processor, `CustomVideoProcessor`, by inheriting from `LlavaOnevisionVideoProcessor` from the `transformers` library, likely for specialized video processing tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/processors.md",
            "description": "This documentation file explains the concept of 'Processors' in the Transformers library, which are used to handle and preprocess multimodal inputs (e.g., text, images, audio) for models. It details how to load and use `ProcessorMixin` and `AutoProcessor` classes for various multimodal tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/processors.md",
            "description": "This document explains the concept of 'Processors' in the Transformers library, distinguishing between multi-modal processors for current models and deprecated processors used for tasks like GLUE, XNLI, and SQuAD, providing details and examples for each category.",
            "spof": false
          },
          {
            "path": "docs/source/zh/main_classes/processors.md",
            "description": "This document explains the concept of 'processors' in the Transformers library, distinguishing between current multimodal processors for handling diverse data types and older, now-deprecated processors used for benchmarks like GLUE, XNLI, and SQuAD. It details the classes and utilities for each type, especially for the deprecated ones.",
            "spof": true
          },
          {
            "path": "docs/source/ko/main_classes/feature_extractor.md",
            "description": "This document, written in Korean, explains the concept of feature extractors in the Hugging Face Transformers library, detailing their role in preparing input features for audio and vision models. It also provides documentation for various feature extraction-related classes like `FeatureExtractionMixin`, `SequenceFeatureExtractor`, and `ImageFeatureExtractionMixin`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/altclip/processing_altclip.py",
            "description": "This file defines the `AltCLIPProcessor` class, which serves as an image/text processor for the AltCLIP model. It combines an image processor and a tokenizer into a single utility class for unified data processing.",
            "spof": false
          },
          {
            "path": "src/transformers/models/align/processing_align.py",
            "description": "This file defines the `AlignProcessor` class, which serves as an image/text processor for the ALIGN model. It combines an image processor and a tokenizer into a single processing unit for multimodal tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aria/image_processing_aria.py",
            "description": "This file defines the `AriaImageProcessor` class, which handles image preprocessing, including resizing, normalization, and splitting, for the Aria vision model within the Hugging Face Transformers library. It was automatically generated from `modular_aria.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aria/processing_aria.py",
            "description": "This file defines the `AriaProcessor` class, which combines an image processor and a tokenizer to prepare multimodal (text and image) inputs for the Aria model, including handling image splitting and tokenization for multimodal integration.",
            "spof": false
          },
          {
            "path": "src/transformers/models/audioflamingo3/processing_audioflamingo3.py",
            "description": "This file defines the `AudioFlamingo3Processor`, which integrates an AudioFlamingo3 feature extractor and tokenizer into a single interface. It prepares both audio waveforms and text sequences for the AudioFlamingo3 model by performing feature extraction, tokenization, and managing audio token expansion.",
            "spof": true
          },
          {
            "path": "src/transformers/models/auto/processing_auto.py",
            "description": "This file defines the `AutoProcessor` class, which automatically loads and instantiates the appropriate processor (combining tokenizers, feature extractors, and image/video processors) for a given pretrained model based on its configuration. It provides a unified interface for model-specific processing components.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "description": "This file implements the `AyaVisionProcessor`, a multimodal processor that combines an image processor and a tokenizer to prepare image and text inputs for the AyaVision model. It handles embedding structured image tokens into text prompts and generating multimodal token type IDs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip_2/processing_blip_2.py",
            "description": "This file defines the `Blip2Processor` class, which combines an image processor and a tokenizer to prepare data for the BLIP-2 model. It handles the specific tokenization requirements for BLIP-2, including the insertion of image tokens into text sequences.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip/image_processing_blip.py",
            "description": "This file defines the `BlipImageProcessor` class, which is responsible for performing image preprocessing operations such as resizing, rescaling, and normalization for the BLIP model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip/image_processing_blip_fast.py",
            "description": "This file defines the `BlipImageProcessorFast` class, which provides a fast implementation for pre-processing images specifically for the BLIP model. It handles operations like resizing, rescaling, normalization, and color conversion.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blip/processing_blip.py",
            "description": "This file defines the `BlipProcessor` class, which combines an image processor and a tokenizer to prepare both image and text inputs for the BLIP model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "description": "This file implements the `BridgeTowerImageProcessorFast` class, a fast image processor designed for the BridgeTower model. It provides optimized methods for image preprocessing steps such as resizing, center cropping, padding, rescaling, and normalization.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "description": "This file defines the `BridgeTowerProcessor` class, which serves as a multimodal processor for the BridgeTower model. It combines an image processor and a tokenizer to handle both visual and text inputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "description": "This file defines the `BridgeTowerImageProcessor` class, which handles the preprocessing of images for the BridgeTower model. It includes methods for operations like resizing, scaling, normalization, center cropping, and padding images.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bros/processing_bros.py",
            "description": "This file defines the `BrosProcessor` class, which is responsible for preparing inputs for the Bros model, including handling tokenization and other pre-processing steps.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "description": "This file defines the `ChineseCLIPProcessor` class, which serves as a combined image and text processor for Chinese-CLIP models within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "description": "This file defines the `ChineseCLIPImageProcessor` class, which handles the preprocessing of images for the Chinese-CLIP model, including operations like resizing, cropping, rescaling, and normalization.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chameleon/image_processing_chameleon_fast.py",
            "description": "This file defines the `ChameleonImageProcessorFast` class, which provides optimized image preprocessing functionalities for the Chameleon model. It includes methods for operations like RGB conversion, resizing, and normalization, designed for faster execution using `torchvision.transforms.v2`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "description": "This file implements the `ChameleonImageProcessor` class, which handles all image preprocessing steps for the Chameleon model, including resizing, cropping, rescaling, and normalization.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chameleon/processing_chameleon.py",
            "description": "This file defines the `ChameleonProcessor` class, which is responsible for preprocessing both text and image inputs for the Chameleon model by combining a tokenizer and an image processor.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clipseg/processing_clipseg.py",
            "description": "This file defines the `CLIPSegProcessor` class, which handles the preprocessing of both text and image inputs (including visual prompts) for the CLIPSeg model, preparing them for consumption by the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clap/processing_clap.py",
            "description": "This file defines the `ClapProcessor` class, which serves as an audio/text processor for the CLAP model within the Transformers library. It combines a feature extractor for audio and a tokenizer for text into a single processing pipeline.",
            "spof": true
          },
          {
            "path": "src/transformers/models/clip/processing_clip.py",
            "description": "This file defines the `CLIPProcessor` class, which combines an image processor and a tokenizer to handle both image and text inputs for the CLIP model. It facilitates unified preprocessing for multimodal tasks.",
            "spof": true
          },
          {
            "path": "src/transformers/models/clip/tokenization_clip.py",
            "description": "This file implements the `CLIPTokenizer` class, providing byte-level Byte-Pair-Encoding tokenization for CLIP models. It leverages HuggingFace's `tokenizers` library for its backend operations.",
            "spof": true
          },
          {
            "path": "src/transformers/models/clip/image_processing_clip_fast.py",
            "description": "This file defines the `CLIPImageProcessorFast` class, providing a fast and optimized implementation for image preprocessing specifically for the CLIP model. It handles operations like resizing, cropping, rescaling, and normalization with predefined parameters.",
            "spof": true
          },
          {
            "path": "src/transformers/models/clip/image_processing_clip.py",
            "description": "This file defines the `CLIPImageProcessor` class, which handles the image preprocessing steps for the CLIP model, including resizing, cropping, scaling, normalization, and RGB conversion.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clvp/tokenization_clvp.py",
            "description": "This file implements the byte-level Byte-Pair-Encoding (BPE) tokenizer for the CLVP model, `ClvpTokenizer`. It handles tokenization, encoding, and decoding of text for the CLVP architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clvp/processing_clvp.py",
            "description": "This file defines the `ClvpProcessor` class, which handles the preprocessing of audio and text inputs for the CLVP (Contrastive Learning for Visual and Prompt) model by combining a feature extractor and a tokenizer.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clvp/number_normalizer.py",
            "description": "This file defines an `EnglishNormalizer` class responsible for converting English text into a standardized format. It handles tasks such as spelling out numbers, expanding common abbreviations, converting text to ASCII, and normalizing whitespace, intended for use with models like CLVP.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cohere2_vision/processing_cohere2_vision.py",
            "description": "This file defines the `Cohere2VisionProcessor`, a multimodal processor that combines an image processor and a tokenizer to prepare both image and text inputs for the Cohere2Vision model. It handles tasks like inserting image tokens into text and managing image processing.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "description": "This file defines a fast image processor (`Cohere2VisionImageProcessorFast`) for the Cohere2 Vision model, implementing image preprocessing steps like resizing, normalization, and crucially, an advanced mechanism for cropping images into optimized grids of patches based on configurable tile sizes and counts.",
            "spof": true
          },
          {
            "path": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "description": "This file defines the `ColQwen2Processor` class, which is responsible for pre-processing both image and text inputs for the ColQwen2 model, preparing them for multimodal tasks. It handles tasks like image resizing, tokenization, and formatting inputs into a `BatchFeature`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "description": "This file defines the ColQwen2Processor, which handles multimodal data preprocessing (images and text) for the ColQwen2 model, including tokenization and image feature extraction. It also includes configuration classes for model processing arguments and output.",
            "spof": false
          },
          {
            "path": "src/transformers/models/colpali/modular_colpali.py",
            "description": "This file defines the `ColPaliProcessor` class, which handles the preprocessing of images and text for the ColPali model by combining an image processor and a tokenizer. It prepares inputs into a format suitable for model inference and includes functionality for retrieval scoring.",
            "spof": false
          },
          {
            "path": "src/transformers/models/colpali/processing_colpali.py",
            "description": "This file defines the `ColPaliProcessor`, which combines an image processor and a tokenizer to prepare multimodal inputs (images and/or text) for the ColPali model, including handling special tokens and input formatting.",
            "spof": false
          },
          {
            "path": "src/transformers/models/csm/processing_csm.py",
            "description": "This file defines the `CsmProcessor` class, which handles the preprocessing of both text and audio inputs for the CSM model. It tokenizes text, extracts audio features, computes encoded audio lengths, and prepares combined multimodal inputs for the model, including generating labels for training.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "description": "This file defines the `DeepseekVLImageProcessor` class, responsible for handling image preprocessing steps such as resizing, scaling, and normalization for the Deepseek-VL model. It was automatically generated from `modular_deepseek_vl.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "description": "This file defines the `DeepseekVLProcessor` class, which integrates an image processor and a tokenizer to prepare data for the Deepseek-VL model. It handles both textual and visual inputs, including the management of special image tokens within the text.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "description": "This file defines a fast image processor for the Deepseek-VL model, designed for efficient preprocessing of images. It handles operations such as resizing, padding to a square, rescaling, and normalization, optimized for performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "description": "This file defines the `DeepseekVLHybridProcessor` for the Deepseek-VL-Hybrid model, which handles both text tokenization and image preprocessing. It prepares combined text and image inputs into a format suitable for the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "description": "This file implements a fast image processor for the Deepseek-VL-Hybrid model, handling image resizing, padding, rescaling, and normalization for both standard and high-resolution inputs. It is an auto-generated optimized version of the DeepseekVLHybrid image processing logic.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "description": "This file defines the `DeepseekVLHybridImageProcessor` class, which handles the preprocessing of images for the DeepseekVL-Hybrid model. It includes functionalities like resizing, rescaling, normalization, and managing both standard and high-resolution image inputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dia/tokenization_dia.py",
            "description": "This file implements the `DiaTokenizer` for the Dia model, which uses a raw bytes UTF-8 encoding scheme with support for special tokens like `[S1]` and `[S2]`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/donut/processing_donut.py",
            "description": "This file defines the `DonutProcessor` class, which combines an image processor and a tokenizer to prepare data for the Donut model. It handles both image and text input, and includes a utility to convert generated token sequences into a structured JSON format.",
            "spof": false
          },
          {
            "path": "src/transformers/models/donut/image_processing_donut.py",
            "description": "This file implements the `DonutImageProcessor` class, which handles the preprocessing of images for the Donut model, including resizing, padding, normalization, and aligning the long axis of the image.",
            "spof": false
          },
          {
            "path": "src/transformers/models/emu3/image_processing_emu3.py",
            "description": "This file defines the `Emu3ImageProcessor` class, which handles the preprocessing of images for the Emu3 model, including dynamic resizing, scaling, normalization, and format conversion.",
            "spof": false
          },
          {
            "path": "src/transformers/models/emu3/processing_emu3.py",
            "description": "This file defines the `Emu3Processor` for the Emu3 model, which handles multimodal input processing (images and text). It prepares inputs for tasks such as image generation from text or text generation from images, managing image placeholders and tokenization.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/image_processing_ernie4_5_vl_moe.py",
            "description": "This file defines the `Ernie4_5_VL_MoeImageProcessor` class, which handles the preprocessing of images for the Ernie 4.5 VL MoE model. It includes functionalities like dynamic resizing, rescaling, normalization, and RGB conversion for input images.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/processing_ernie4_5_vl_moe.py",
            "description": "This file defines the `Ernie4_5_VL_MoeProcessor` class, which combines an image processor, a video processor, and a tokenizer to prepare multi-modal inputs (images, videos, and text) for the Ernie 4.5 VL MoE model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/video_processing_ernie4_5_vl_moe.py",
            "description": "This file defines the `Ernie4_5_VL_MoeVideoProcessor` class, which handles video preprocessing for the Ernie 4.5 VL Mixture-of-Experts model. It manages operations such as dynamic video resizing, frame sampling, normalization, and loading of video processor configurations and fonts.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/image_processing_ernie4_5_vl_moe_fast.py",
            "description": "This file provides a fast image processor (`Ernie4_5_VL_MoeImageProcessorFast`) for the Ernie 4.5 VL-MoE model, handling image preprocessing steps such as smart resizing, normalization, and conversion into a patched format optimized for model input. It's an auto-generated file derived from a modular source.",
            "spof": true
          },
          {
            "path": "src/transformers/models/evolla/processing_evolla.py",
            "description": "This file defines the `EvollaProcessor` class, which handles the pre-processing of both protein sequences and text messages for the EVOLLA model. It tokenizes protein data (amino acid and Foldseek sequences) and conversational text messages into a unified input format.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flava/processing_flava.py",
            "description": "This file defines the `FlavaProcessor` class, which serves as a combined image and text processor for the FLAVA model, integrating an image processor and a tokenizer.",
            "spof": true
          },
          {
            "path": "src/transformers/models/flava/image_processing_flava_fast.py",
            "description": "This file defines the `FlavaImageProcessorFast` class, which is a fast image processor specifically designed for the FLAVA model. It handles image preprocessing, including resizing, cropping, normalization, and also includes functionalities for image masking and codebook pixel manipulation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/florence2/processing_florence2.py",
            "description": "This file defines the `Florence2Processor` class, which handles the preparation of multimodal inputs (images and text) for the Florence-2 model, including prompt construction and data tokenization.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "description": "This file defines the `FuyuImageProcessor` class, which handles all image preprocessing steps for the Fuyu model, including resizing, padding, normalization, and creating image patch metadata for tokenization. It also includes utility classes for Fuyu-specific batch features and image kwargs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fuyu/processing_fuyu.py",
            "description": "This file defines the Fuyu model's processor, handling text and image tokenization, including special tokens for bounding boxes and points, and coordinate transformations for input preparation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fuyu/image_processing_fuyu_fast.py",
            "description": "This file implements a fast image processor (`FuyuImageProcessorFast`) for the Fuyu model, handling image preprocessing steps such as resizing, padding, normalization, and patch creation for efficient model input.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "description": "This file defines the `Gemma3nProcessor` class, which is responsible for pre-processing multimodal inputs (text, images, and audio) for the Gemma3n model, preparing them for the model's forward pass.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "description": "This file defines the `Gemma3ImageProcessor` class, which handles image preprocessing for the Gemma3 model, including operations like resizing, normalization, and a specialized pan-and-scan cropping method.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "description": "This file defines the `Gemma3ImageProcessorFast` class, a fast image processor for the Gemma3 model within the HuggingFace Transformers library. It handles image preprocessing steps such as resizing, pan-and-scan, rescaling, and normalization for efficient input preparation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3/processing_gemma3.py",
            "description": "This file defines the `Gemma3Processor` class, which is a multimodal processor designed to prepare both image and text inputs for the Gemma3 model. It handles tasks such as image preprocessing, tokenization, and the integration of image tokens within the text sequence, including support for pan-and-scan image cropping.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Raushan Turganbay",
            "percent": 17
          },
          {
            "name": "Yoni Gozlan",
            "percent": 16
          },
          {
            "name": "Anton Vlasjuk",
            "percent": 11
          }
        ]
      },
      "Multimodal Model Architectures": {
        "files": [
          {
            "path": "examples/modular-transformers/modular_multimodal2.py",
            "description": "This file redefines and renames components of the `CLIPVisionModel` to consistently use a 'Multimodal2Vision' prefix, addressing inconsistencies in the original CLIP naming for modular integration. It subclasses various CLIP components to enforce this naming convention.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_multimodal2.py",
            "description": "This file defines the vision-related components for a Multimodal2 model, including the attention mechanism, MLP, encoder layers, and embedding logic for its transformer-based vision encoder.",
            "spof": true
          },
          {
            "path": "examples/modular-transformers/modeling_new_task_model.py",
            "description": "This file defines the core components and logic for the `NewTaskModel` within the Transformers library, including its output structures, a multimodal projector, and specialized attention masking for handling combined image and text inputs.",
            "spof": false
          },
          {
            "path": "src/transformers/loss/loss_grounding_dino.py",
            "description": "This file defines the loss functions and a specialized Hungarian matcher for the GroundingDino object detection model within the HuggingFace Transformers library. It includes implementations for sigmoid focal loss, a custom matcher for GroundingDino, and a comprehensive loss computation class.",
            "spof": true
          },
          {
            "path": "src/transformers/models/altclip/configuration_altclip.py",
            "description": "This file defines the configuration classes for the AltCLIP model, specifically `AltCLIPTextConfig` and `AltCLIPVisionConfig`, inheriting from `PreTrainedConfig` to specify parameters for the text and vision components of the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/altclip/__init__.py",
            "description": "This file serves as the package initializer for the `altclip` model within the transformers library, implementing lazy loading of its configuration, modeling, and processing components. It ensures these submodules are only loaded when explicitly accessed, optimizing import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/altclip/modeling_altclip.py",
            "description": "This file defines the PyTorch model architecture for AltCLIP, including its components like embeddings, attention mechanisms, and the contrastive loss function used for training.",
            "spof": false
          },
          {
            "path": "src/transformers/models/align/__init__.py",
            "description": "Initializes the `align` model package, enabling lazy loading of its configuration, modeling, and processing components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/align/configuration_align.py",
            "description": "This file defines the configuration classes for the ALIGN model's text and vision components, specifying their architectural parameters for instantiation and use within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/align/modeling_align.py",
            "description": "This file implements the PyTorch model architecture for the ALIGN (vision-text) model, including its core components, output structures, and loss functions. It defines layers and utilities specific to the ALIGN vision and text models within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aimv2/configuration_aimv2.py",
            "description": "This file defines the configuration classes (`Aimv2VisionConfig` and `Aimv2TextConfig`) for the vision and text encoders of the AIMv2 model, inheriting from `PreTrainedConfig`. It is automatically generated from `modular_aimv2.py`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/aimv2/modeling_aimv2.py",
            "description": "This file defines the core components and architecture for the Aimv2 model, including its output structure, normalization, MLP, and embedding layers for both vision and text, used within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aimv2/__init__.py",
            "description": "This `__init__.py` file defines the `aimv2` model's public API within the transformers library. It uses lazy loading to import the configuration and modeling components of the AIM-V2 model only when they are first accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/aimv2/modular_aimv2.py",
            "description": "This file provides the PyTorch implementation and configuration classes for the AIMv2 model within the Hugging Face Transformers library, extending configurations from the Siglip model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/aria/configuration_aria.py",
            "description": "This file defines configuration classes for the Aria model, including `AriaTextConfig` for its text component and `AriaConfig` for the overall model, which encompasses both vision and text configurations, along with parameters for image token handling and projector mapping.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aria/__init__.py",
            "description": "This `__init__.py` file for the `aria` model within the Transformers library enables lazy loading of its components (configuration, image processing, modeling, and processing modules) to optimize import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/aria/modeling_aria.py",
            "description": "This file defines core model components for the Aria model in the Transformers library, including text processing layers (RMSNorm, SharedExpertsMLP) and a projector for integrating vision features into the language model's embedding space via cross-attention and MLPs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aria/modular_aria.py",
            "description": "This file defines configurations and utilities for the multimodal Aria model, including its text component (`AriaTextConfig`) which extends Llama and incorporates a Mixture of Experts (MoE) architecture, and the overarching `AriaConfig` for both vision and text.",
            "spof": false
          },
          {
            "path": "src/transformers/models/audioflamingo3/__init__.py",
            "description": "This `__init__.py` file serves as the package entry point for the `audioflamingo3` model within the Hugging Face Transformers library. It sets up lazy loading for its configuration, modeling, and processing components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/audioflamingo3/configuration_audioflamingo3.py",
            "description": "This file defines the configuration classes for the AudioFlamingo3 model and its encoder component within the Hugging Face Transformers library. It specifies architectural parameters and integrates configurations for both audio and text backbones.",
            "spof": true
          },
          {
            "path": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "description": "This file defines the modeling architecture for the AudioFlamingo3 model, including attention mechanisms and encoder layers, within the Hugging Face Transformers library. It is automatically generated from a modular source file.",
            "spof": true
          },
          {
            "path": "src/transformers/models/audioflamingo3/modular_audioflamingo3.py",
            "description": "This file implements the AudioFlamingo3 model for conditional generation, integrating a Whisper-based audio encoder, a multi-modal projector, and a Qwen2 language model for tasks like audio transcription.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aya_vision/configuration_aya_vision.py",
            "description": "This file defines the `AyaVisionConfig` class, which is used to store and manage the configuration for the AyaVision model, including its vision and text backbone settings. It specifies parameters necessary for instantiating and configuring an AyaVision model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aya_vision/__init__.py",
            "description": "This `__init__.py` file defines the public API for the Aya Vision model within the Transformers library. It utilizes lazy loading for its sub-modules (configuration, modeling, and processing) to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "description": "This file defines the core architecture and components for the Aya Vision multimodal model within the Hugging Face Transformers library, integrating a vision backbone, a language model, and a multimodal projector. It is an auto-generated implementation of the Aya Vision model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "description": "This file implements the PyTorch AyaVision model, extending LLaVA components to handle multimodal projection and integrate image features into a language model for conditional generation tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip_2/__init__.py",
            "description": "This `__init__.py` file defines the `blip_2` package structure within the `transformers` library, enabling lazy loading of its core components like configuration, modeling, and processing utilities.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blip_2/configuration_blip_2.py",
            "description": "This file defines the configuration classes for the BLIP-2 model components, including the vision encoder, the Q-Former, and the overall BLIP-2 model. It specifies architectural hyperparameters and default values for these models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip_2/modeling_blip_2.py",
            "description": "This file implements the PyTorch BLIP-2 model architecture, including its vision and Q-Former components, along with various output classes for conditional generation and image-text matching tasks within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip/configuration_blip.py",
            "description": "This file defines the configuration classes (`BlipTextConfig` and `BlipVisionConfig`) for the BLIP model's text and vision components, specifying their architectural parameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip/__init__.py",
            "description": "This `__init__.py` file serves as the package entry point for the BLIP model, orchestrating the lazy loading of its various components such as configuration, image processing, and model definitions within the `transformers` library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blip/modeling_blip_text.py",
            "description": "This file implements the core text modeling components for the BLIP model within the Hugging Face Transformers library. It defines essential modules like text embeddings, self-attention, and self-output layers for the text encoder.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip/modeling_blip.py",
            "description": "This file implements the core PyTorch BLIP (Bootstrapping Language-Image Pre-training) model, defining its architecture, loss functions, and output structures within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "description": "This file defines the configuration classes for the BridgeTower multi-modal model, including separate configurations for its vision and text components, as well as a combined configuration for the full model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bridgetower/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `bridgetower` model within the `transformers` library, using lazy imports to manage its sub-modules like configuration, image processing, and modeling.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "description": "This file implements the core PyTorch BridgeTower multi-modal model architecture, including its vision and transformer components, and defines the data structures for its various outputs such as for contrastive learning.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blt/configuration_blt.py",
            "description": "This file defines the configuration classes for various components (local encoder, local decoder, global transformer, and patcher) of the BLT model within the HuggingFace Transformers library. These classes specify model parameters such as hidden size, number of layers, attention heads, and more.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bros/configuration_bros.py",
            "description": "This file defines the `BrosConfig` class, which is used to store and manage the configuration parameters for the Bros model within the Hugging Face Transformers library. It specifies architectural details like vocabulary size, hidden layer dimensions, and attention head count.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bros/modeling_bros.py",
            "description": "This file implements the core PyTorch modules for the Bros (BBOX-aware RoBERTa) model, including positional embeddings for bounding boxes, text embeddings, and the self-attention mechanism, within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "description": "This file defines the configuration classes for the text and vision components of the Chinese-CLIP model, specifying architecture parameters and default values for instantiating models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chinese_clip/__init__.py",
            "description": "This `__init__.py` file defines the module structure for the `chinese_clip` model within the Transformers library. It enables lazy loading of various components like configuration, feature extraction, image processing, and modeling to optimize import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "description": "This file implements the core PyTorch model architecture, including embeddings and loss functions, for the Chinese-CLIP model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chameleon/configuration_chameleon.py",
            "description": "This file defines configuration classes for the Chameleon model within the Hugging Face Transformers library, specifically `ChameleonVQVAEConfig` for the VQ-VAE component and `ChameleonConfig` for the overall model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chameleon/__init__.py",
            "description": "This file implements a lazy import mechanism for the 'Chameleon' model within the HuggingFace Transformers library. It defines the module's structure and components like configuration, image processing, and modeling, which are loaded only when accessed.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chameleon/modeling_chameleon.py",
            "description": "This file implements the core PyTorch model architecture and various building blocks (e.g., RMSNorm, Rotary Embedding, MLP, LayerNorm) for the Chameleon model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clipseg/configuration_clipseg.py",
            "description": "This file defines the configuration classes for the CLIPSeg model, including separate configurations for its text and vision components (`CLIPSegTextConfig`, `CLIPSegVisionConfig`), and an overarching `CLIPSegConfig`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clipseg/__init__.py",
            "description": "This `__init__.py` file defines the `clipseg` package within the `transformers` library. It uses lazy loading to import ClipSeg model components like configuration, modeling, and processing classes only when they are first accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/clipseg/modeling_clipseg.py",
            "description": "This file implements the core PyTorch model architecture, embeddings, and loss functions for the CLIPSeg model within the Hugging Face Transformers library. It defines various output classes and essential model components for image and text processing.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clap/configuration_clap.py",
            "description": "This file defines the configuration classes for the CLAP (Contrastive Language-Audio Pre-training) model, specifically for its text and audio encoders. These classes (`ClapTextConfig` and `ClapAudioConfig`) specify the architectural parameters and hyperparameters required to instantiate the CLAP model components.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clap/__init__.py",
            "description": "This `__init__.py` file sets up lazy loading for the CLAP model components within the Transformers library. It defines the import structure for configuration, feature extraction, modeling, and processing modules, delaying their import until actually needed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/clap/modeling_clap.py",
            "description": "This file implements the PyTorch CLAP (Contrastive Language-Audio Pre-training) model, defining its core architecture, utility functions, and output structures within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clip/configuration_clip.py",
            "description": "This file defines the configuration classes (`CLIPTextConfig` and `CLIPVisionConfig`) for the text and vision components of the CLIP model. These classes specify the architectural parameters used to instantiate CLIP text and vision encoders within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clip/modeling_clip.py",
            "description": "This file implements the core components, loss functions, and embedding layers for the PyTorch CLIP (Contrastive Language-Image Pre-training) model. It defines classes for vision and text embeddings, as well as output structures for the CLIP model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clip/__init__.py",
            "description": "This `__init__.py` file defines the package structure for the CLIP model within the Hugging Face Transformers library. It uses lazy loading to import core CLIP components like configuration, modeling, and tokenization, improving startup performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/clvp/modeling_clvp.py",
            "description": "This file implements the PyTorch CLVP (Contrastive Learning for Visual-Text Pretraining or similar) model, defining its architecture components, loss functions, and data structures for various outputs within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clvp/configuration_clvp.py",
            "description": "This file defines the configuration classes (`ClvpEncoderConfig` and `ClvpDecoderConfig`) for the CLVP (Conditional Latent Video Prediction) model's encoder and decoder components within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clvp/__init__.py",
            "description": "This `__init__.py` file serves as the package entry point for the CLVP model in the Hugging Face Transformers library. It defines the module's import structure and enables lazy loading of its various components, such as configuration, modeling, and tokenization, to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere2_vision/configuration_cohere2_vision.py",
            "description": "This file defines the `Cohere2VisionConfig` class, which is used to store and manage the configuration parameters for the Cohere2 Vision model, including its vision and text backbones, within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere2_vision/__init__.py",
            "description": "This `__init__.py` file defines the import structure for the `cohere2_vision` model within the `transformers` library. It uses a lazy loading mechanism to import model components like configuration, image processing, and modeling classes, optimizing startup time.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "description": "This file defines the Cohere2Vision multi-modal model architecture, including its multi-modal projector, model outputs, and the main model integrating vision and language components. It is an auto-generated file from a modular source.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "description": "This file implements the PyTorch Cohere2Vision multi-modal model, defining its architecture for processing and integrating vision and language features, built upon an AyaVision base model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/colqwen2/configuration_colqwen2.py",
            "description": "This file defines the `ColQwen2Config` class, which is a configuration class for the `ColQwen2` model designed for efficient document retrieval using vision-language models. It manages parameters such as VLM backbone configuration, embedding dimension, and initializer range.",
            "spof": false
          },
          {
            "path": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "description": "This file defines the ColQwen2 model for document retrieval, leveraging Vision-Language Models (VLMs) to create multi-vector embeddings from document images and text. It is automatically generated from `modular_colqwen2.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/colqwen2/__init__.py",
            "description": "This `__init__.py` file defines the `colqwen2` model's module structure within the Hugging Face Transformers library. It utilizes lazy loading to import the model's configuration, modeling, and processing components efficiently.",
            "spof": true
          },
          {
            "path": "src/transformers/models/colpali/configuration_colpali.py",
            "description": "This file defines the `ColPaliConfig` class, which is used to configure the ColPali model for retrieval by specifying configurations for its Vision-Language Model (VLM) backbone and text backbone.",
            "spof": false
          },
          {
            "path": "src/transformers/models/colpali/__init__.py",
            "description": "This `__init__.py` file defines the `colpali` model package within the `transformers` library, using lazy loading for its components (configuration, modeling, and processing) to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/colpali/modeling_colpali.py",
            "description": "This file implements the PyTorch ColPali model, an architecture that leverages vision-language models to construct efficient multi-vector embeddings from document images and text for retrieval purposes. It defines the base model, the retrieval-specific model, and its output structure.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/__init__.py",
            "description": "This `__init__.py` file defines the data2vec package, enabling lazy loading of its audio, text, and vision configurations and models within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/data2vec/configuration_data2vec_text.py",
            "description": "This file defines the `Data2VecTextConfig` class, which is used to store and manage the configuration parameters for the Data2VecText model architecture in the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "description": "This file defines the architectural components for the Data2VecAudio model, including convolutional layers, positional embeddings, feature encoders, and multi-headed attention, as part of the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/decision_transformer/configuration_decision_transformer.py",
            "description": "This file defines the `DecisionTransformerConfig` class, which is used to store and manage the configuration parameters for a `DecisionTransformerModel` within the Hugging Face Transformers library. It specifies architectural details for a Decision Transformer model, including parameters for a GPT-2 like backbone.",
            "spof": false
          },
          {
            "path": "src/transformers/models/csm/configuration_csm.py",
            "description": "This file defines the configuration classes for the CSM (Context-aware Speech Model) models, including `CsmDepthDecoderConfig` and `CsmConfig`, which specify their architecture and hyperparameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/csm/__init__.py",
            "description": "This file serves as the package entry point for the `CSM` model within the `transformers` library. It uses a lazy import mechanism to load the model's configuration, core model, and processing utilities only when they are accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/csm/modular_csm.py",
            "description": "This file implements the modular components and depth decoder for the CSM (Codebook Sequence Model) within the Hugging Face Transformers library. It defines the core model architecture, including attention, MLP, and decoder layers, and manages the forward pass logic for the depth decoder, potentially for multi-modal applications like audio-text processing.",
            "spof": false
          },
          {
            "path": "src/transformers/models/csm/modeling_csm.py",
            "description": "This file defines core components such as RMSNorm, Rotary Positional Embeddings, and an MLP block, along with the output structure, for the CSM model within the HuggingFace Transformers library. It appears to be an auto-generated implementation of a CSM transformer architecture, potentially with a depth decoder.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl/__init__.py",
            "description": "This `__init__.py` file defines the public API for the `deepseek_vl` model within the `transformers` library, utilizing lazy loading for its submodules to optimize import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "description": "This file defines the DeepseekVL model architecture, configuration, image processing, and a combined processor for the Hugging Face Transformers library. It leverages existing components from other models like Idefics and Janus.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "description": "This file implements the core model architecture for the Deepseek-VL (Vision-Language) model, integrating vision and language components. It defines model outputs, an aligner module, and the main model class for handling multimodal inputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl/configuration_deepseek_vl.py",
            "description": "This file defines the `DeepseekVLConfig` class, which is used to store and manage the configuration for the DeepseekVL model, including its text and vision backbone settings. It is automatically generated from a modular file and should not be edited directly.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/configuration_deepseek_vl_hybrid.py",
            "description": "This file defines the `DeepseekVLHybridConfig` class, which is used to store and manage the configuration parameters for the Deepseek VL Hybrid model. It specifies the architecture, including the text, vision, and high-resolution vision backbones, and their respective default settings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/__init__.py",
            "description": "This `__init__.py` file defines the module structure for the Deepseek VL Hybrid model components, enabling lazy loading of its configuration, image processing, modeling, and general processing classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "description": "This file defines the configuration and core architectural components for a hybrid Deepseek-VL model. It integrates a high-resolution vision backbone (e.g., SAM) with standard text and vision backbones for multimodal processing.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "description": "This file defines the core modeling components and output structures for the DeepseekVLHybrid model within the Hugging Face Transformers library. It includes definitions for specific architectural layers like DeepseekVLSamVisionNeck and DeepseekVLHybridAligner.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dia/configuration_dia.py",
            "description": "This file defines the configuration classes for the Dia model, including separate configurations for its encoder and decoder components, inheriting from `PreTrainedConfig`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dia/modeling_dia.py",
            "description": "This file defines the core modeling architecture and components for the Dia model within the Hugging Face Transformers library, including its pre-trained model, various layers (embedding, MLP, RMSNorm), and rotary positional embeddings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dia/modular_dia.py",
            "description": "This file defines the modular components and architecture for the PyTorch implementation of the Dia model, including its multi-channel embeddings, attention mechanisms, encoder layers, and overall encoder structure.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dia/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the 'DIA' model within the Transformers library, enabling lazy loading of its various components such as configuration, modeling, and tokenization. It defines the module's import structure for efficient resource management.",
            "spof": true
          },
          {
            "path": "src/transformers/models/donut/configuration_donut_swin.py",
            "description": "This file defines the `DonutSwinConfig` class, which is used to store the configuration parameters for the Donut Swin Transformer model. It specifies architectural details such as image size, patch size, embedding dimensions, and layer depths for model instantiation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/donut/__init__.py",
            "description": "This `__init__.py` file defines the Donut model package for the Hugging Face Transformers library. It uses lazy loading to manage imports of its submodules, including configuration, feature extraction, image processing, and model implementations, making them accessible when needed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/donut/modeling_donut_swin.py",
            "description": "This file implements the PyTorch Donut Swin Transformer model, adapting the Swin Transformer architecture by removing the final layer normalization and defining specific output types for the Donut model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/edgetam/configuration_edgetam.py",
            "description": "This file defines configuration classes for the EdgeTAM model's vision encoder, prompt encoder, and mask decoder components. It specifies architecture details and default parameters for instantiating these model parts.",
            "spof": true
          },
          {
            "path": "src/transformers/models/emu3/configuration_emu3.py",
            "description": "This file defines the configuration classes for the Emu3 VQ-VAE and text models, specifying their architectural parameters for use within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/emu3/__init__.py",
            "description": "This `__init__.py` file serves as the package entry point for the `emu3` model within the `transformers` library. It uses lazy loading to define the import structure for its submodules, including configuration, image processing, modeling, and general processing components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/emu3/modeling_emu3.py",
            "description": "This file defines the core components and architecture for the Emu3 model within the Hugging Face Transformers library, including attention mechanisms, normalization layers, and multi-layer perceptrons. It is an auto-generated file from a modular implementation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/emu3/modular_emu3.py",
            "description": "This file defines modular components for the Emu3 model, including its VQ-VAE (Vector Quantized Variational AutoEncoder) architecture for processing visual tokens and specialized decoder layers derived from Llama.",
            "spof": false
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/configuration_ernie4_5_vl_moe.py",
            "description": "This file defines the configuration classes for the vision and text components of the Ernie4.5-VL Moe model, specifying their architectural parameters. It is automatically generated from a modular source file.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/__init__.py",
            "description": "This `__init__.py` file defines the module structure for `ernie4_5_vl_moe` within the `transformers` library. It uses lazy loading to import various components like configuration, image processing, modeling, and video processing for the Ernie 4.5 VL MoE model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/modeling_ernie4_5_vl_moe.py",
            "description": "This file defines core components for the Ernie 4.5 VL MoE (Mixture of Experts) model, including rotary positional embeddings and multi-headed attention mechanisms for its text processing part. It is automatically generated from a modular source.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/modular_ernie4_5_vl_moe.py",
            "description": "This file defines the configuration classes for the vision and text components of the Ernie4.5-VL Mixture-of-Experts (Moe) model within the HuggingFace Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/exaone4/modular_exaone4.py",
            "description": "This file defines the `Exaone4Config` class, which serves as the configuration for the EXAONE 4.0 model architecture within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/evolla/modeling_evolla.py",
            "description": "This file implements the modeling components, including embeddings and rotary position embeddings, for the EvollaSaProt model within the Hugging Face Transformers library. It is automatically generated from `modular_evolla.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/evolla/__init__.py",
            "description": "This `__init__.py` file defines the `evolla` model package, enabling lazy loading of its configuration, modeling, and processing components for the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/evolla/configuration_evolla.py",
            "description": "This file defines the configuration classes for the Evolla model and its SaProt protein encoder component within the Hugging Face Transformers library. It specifies architectural parameters and default values for both parts of the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/evolla/modular_evolla.py",
            "description": "This file implements modular components for the Evolla 'SaProt' model within the Hugging Face Transformers library, primarily defining a protein encoder that adapts elements from ESM and Llama architectures, including custom rotary embeddings and attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fast_vlm/configuration_fast_vlm.py",
            "description": "This file defines the `FastVlmConfig` class, which stores the configuration for the FastVLM multimodal model, including its vision and text backbone configurations. It is automatically generated from `modular_fast_vlm.py`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fast_vlm/modular_fast_vlm.py",
            "description": "This file defines the FastVLM multimodal model for the Hugging Face Transformers library. It includes the configuration, model architecture, and multimodal projector for the FastVLM model, extending the Llava model structure.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fast_vlm/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `fast_vlm` model within the `transformers` library. It uses lazy loading to import the model's configuration and modeling components, optimizing startup performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fast_vlm/modeling_fast_vlm.py",
            "description": "This file defines the FastVLM model architecture, integrating a vision backbone and a language model via a multimodal projector. It handles the processing of both image pixel values and text input IDs for multimodal tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flava/configuration_flava.py",
            "description": "This file defines the configuration classes (`FlavaImageConfig`, `FlavaTextConfig`) for the image and text components of the FLAVA model. These configurations specify architectural parameters for instantiating FLAVA models within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flava/__init__.py",
            "description": "This `__init__.py` file defines the `flava` model module, enabling lazy loading of its various components such as configuration, feature extraction, image processing, and model definitions within the HuggingFace Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/flava/modeling_flava.py",
            "description": "This file defines the PyTorch FLAVA (Foundational Language And Vision Alignment) model, including its output structures for various embeddings and pretraining losses (MIM, MLM, ITM, contrastive, MMM).",
            "spof": false
          },
          {
            "path": "src/transformers/models/florence2/modular_florence2.py",
            "description": "This file defines the configuration classes for the Florence-2 model within the Hugging Face Transformers library. It includes configurations for both the vision component (`Florence2VisionConfig`) and the overall multimodal model (`Florence2Config`).",
            "spof": true
          },
          {
            "path": "src/transformers/models/florence2/configuration_florence2.py",
            "description": "This file defines configuration classes for the Florence-2 model, including `Florence2VisionConfig` for the visual components and `Florence2Config` for the overall model architecture, combining text and vision configurations.",
            "spof": true
          },
          {
            "path": "src/transformers/models/florence2/__init__.py",
            "description": "This `__init__.py` file defines the import structure for the `florence2` model, enabling lazy loading of its configuration, modeling, and processing components within the Hugging Face Transformers library. This approach helps optimize startup performance by only loading modules when they are explicitly accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/florence2/modeling_florence2.py",
            "description": "This file defines the core modeling components and architecture for the Florence2 vision model within the HuggingFace Transformers library. It is an auto-generated file from a modular source.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fuyu/configuration_fuyu.py",
            "description": "This file defines the `FuyuConfig` class, which is used to store and manage the architectural configuration for the Fuyu model within the Hugging Face Transformers library. It specifies various parameters for initializing a Fuyu model instance.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fuyu/modeling_fuyu.py",
            "description": "This file implements the PyTorch Fuyu model, which integrates a vision backbone and a language model to process multimodal inputs, including text and image patches.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fuyu/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the Fuyu model package within the Transformers library. It defines the module's import structure, enabling lazy loading of Fuyu-related components like configurations, image processing, and model implementations.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma3n/__init__.py",
            "description": "This file serves as the package initializer for the Gemma3n model within the Hugging Face Transformers library. It sets up lazy imports for its various components, such as configuration, feature extraction, modeling, and processing modules.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "description": "This file defines the configuration class for the Gemma3nText model within the Hugging Face Transformers library, specifying its architecture and parameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "description": "This file defines the modeling architecture for the Gemma3n model, including its output structures and core components like RMS normalization and audio-specific relative position embeddings, generated from a modular source.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "description": "This file defines the configuration class, `Gemma3nTextConfig`, for the Gemma3n text model within the Hugging Face Transformers library. It specifies various architectural parameters and hyperparameters for building and instantiating a Gemma3nTextModel.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3/__init__.py",
            "description": "This `__init__.py` file defines the entry point for the `gemma3` model within the `transformers` library, implementing lazy loading for its submodules such as configuration, modeling, and image processing components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma3/configuration_gemma3.py",
            "description": "This file defines the configuration class for the Gemma3Text model, specifying its architecture and various hyperparameters. It is automatically generated from `modular_gemma3.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3/modular_gemma3.py",
            "description": "This file defines the `Gemma3TextConfig` class, which is used to store and manage the configuration parameters for the Gemma 3 text model architecture within the Hugging Face Transformers library. It extends configurations from Gemma 2 and other related models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/gemma3/modeling_gemma3.py",
            "description": "This file defines the core architectural components and model structure for the Gemma3 model within the Hugging Face Transformers library, automatically generated from its modular counterpart.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Raushan Turganbay",
            "percent": 19
          },
          {
            "name": "Cyril Vallez",
            "percent": 19
          },
          {
            "name": "Anton Vlasjuk",
            "percent": 10
          }
        ]
      },
      "Multimodal Model Documentation and Examples": {
        "files": [
          {
            "path": "examples/pytorch/contrastive-image-text/run_clip.py",
            "description": "This script trains CLIP-like dual encoder models using text and vision encoders, supporting various languages by allowing the use of pre-trained text encoders in the desired language.",
            "spof": false
          },
          {
            "path": "docs/source/en/chat_content_patterns.md",
            "description": "This document outlines the various patterns and formats for structuring chat messages for Hugging Face Transformers models, covering text, tools, multimodal inputs (image, video, audio), batched inference, and multi-turn conversations.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/document_question_answering.md",
            "description": "This file is a documentation guide for Document Question Answering using the Hugging Face Transformers library. It specifically demonstrates how to fine-tune and use the LayoutLMv2 model for this task.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/any_to_any.md",
            "description": "This documentation file introduces multimodal (any-to-any) models in the Hugging Face Transformers library, explaining their capabilities and demonstrating how to use them for inference with various input and output modalities, including text, audio, and video.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/image_text_to_text.md",
            "description": "This document provides a guide on using image-text-to-text models (VLMs) with Hugging Face Transformers, covering inference, pipeline usage, streaming, and optimization techniques.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/image_captioning.md",
            "description": "This document provides a guide on how to fine-tune an image captioning model using the Hugging Face Transformers library. It covers data preprocessing, model loading, evaluation, training, and inference steps.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/video_text_to_text.md",
            "description": "This document provides an overview of video-text-to-text models (video language models) within the Hugging Face Transformers library. It demonstrates how to perform inference with an instruction-tuned video LM, including examples of single and interleaved video inputs.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/idefics.md",
            "description": "This documentation file provides a guide on how to use the IDEFICS multimodal model within the Hugging Face Transformers library for various image-text tasks, including loading, quantization, and examples for tasks like image captioning and visual question answering.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/visual_document_retrieval.md",
            "description": "This document explains visual document retrieval, demonstrating how to use the ColPali model from Hugging Face Transformers to index and retrieve multimodal documents (images and text) using a UFO sightings dataset. It provides code examples for setting up the model, processing data, generating embeddings, and performing retrieval queries.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/visual_question_answering.md",
            "description": "This document serves as a guide and tutorial for Visual Question Answering (VQA) using the Hugging Face Transformers library. It explains VQA, its applications, and provides instructions for fine-tuning models like ViLT and performing inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/internal/time_series_utils.md",
            "description": "This file documents internal utility functions and classes used for time series based models within the Hugging Face Transformers library, focusing on distributional output classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/altclip.md",
            "description": "This file provides documentation for the AltCLIP model within the Hugging Face Transformers library, detailing its architecture, usage with code examples (including quantization), and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/align.md",
            "description": "This file provides documentation for the ALIGN model within the Hugging Face Transformers library. It explains the model's architecture and capabilities, offering code examples for zero-shot image classification and image-text similarity tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/chinese_clip.md",
            "description": "This file provides documentation for the Chinese-CLIP model within the Hugging Face Transformers library, including an overview, usage examples, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/clip.md",
            "description": "This file provides documentation for the CLIP (Contrastive Language-Image Pre-training) model within the Hugging Face Transformers library. It explains the model's functionality, provides usage examples for zero-shot image classification, and lists its various components and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/blip-2.md",
            "description": "This file provides the documentation for the BLIP-2 model within the Hugging Face Transformers library, including an overview, architecture, usage tips, and references to its various components and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/aria.md",
            "description": "This file provides documentation for the Aria multimodal mixture-of-experts (MoE) model within the Hugging Face Transformers library. It explains the model's capabilities, provides usage examples for text generation from images, and details quantization techniques.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/colpali.md",
            "description": "This documentation file introduces the ColPali model for visual document retrieval, explaining its functionality and providing Python code examples for its usage, including basic retrieval and quantized inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/aya_vision.md",
            "description": "This file provides documentation and usage examples for the Aya Vision multimodal vision-language model within the Hugging Face Transformers library. It covers tasks like image-to-text generation, quantization, and handling multiple or batched inputs.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/blip.md",
            "description": "This file provides documentation for the BLIP (Bootstrapped Language-Image Pretraining) model within the Hugging Face Transformers library. It includes an overview of the model, usage examples for visual question answering, and API references for its various components and configurations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/audioflamingo3.md",
            "description": "This file provides documentation for the Audio Flamingo 3 model, an open large audio-language model, detailing its overview, features, paper reference, and comprehensive usage examples for inference and training within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/cohere2_vision.md",
            "description": "This file is a documentation page for the Cohere Command A Vision multimodal model within the Hugging Face Transformers library. It provides an overview, usage examples, and API references for the model's components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bridgetower.md",
            "description": "This document provides a comprehensive overview and usage guide for the BridgeTower model within the Hugging Face Transformers library. It details the model's architecture, its applications in contrastive learning, image-text retrieval, and masked language modeling, along with practical code examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/chameleon.md",
            "description": "This file provides documentation for the Chameleon model within the Hugging Face Transformers library. It includes an overview of the model, usage examples for inference with single and multiple images, and tips for model optimization.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/clap.md",
            "description": "This documentation file provides an overview of the CLAP (Contrastive Language-Audio Pretraining) model, its features, and how to use it within the Hugging Face Transformers library. It includes code examples for extracting text embeddings and references to various CLAP-related classes for detailed API documentation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/aimv2.md",
            "description": "This file provides comprehensive documentation for the Hugging Face Transformers' AIMv2 model, detailing its overview, capabilities, usage examples, and API references for its configurations and core components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/clipseg.md",
            "description": "This file provides comprehensive documentation for the CLIPSeg model within the Hugging Face Transformers library, detailing its overview, usage, and API reference for various components like `CLIPSegConfig`, `CLIPSegProcessor`, and `CLIPSegForImageSegmentation`.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/colqwen2.md",
            "description": "This file documents the ColQwen2 model, a vision-based system for document retrieval that processes documents as images to extract multi-vector embeddings. It provides usage examples, quantization instructions, and details about its architecture and related components like ColQwen2.5.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/data2vec.md",
            "description": "This file provides comprehensive documentation for the Data2Vec model within the Hugging Face Transformers library, detailing its overview, usage tips, and API references for its audio, text, and vision implementations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/emu3.md",
            "description": "This file provides documentation for the Emu3 multimodal large language model, detailing its capabilities for text and image generation, usage examples within the Hugging Face Transformers library, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/deepseek_vl.md",
            "description": "This file provides documentation for the DeepseekVL vision-language model within the Hugging Face Transformers library, including its architecture, usage examples, quantization, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/donut.md",
            "description": "This file documents the Donut (Document Understanding Transformer) model within the Hugging Face Transformers library. It explains the model's capabilities, architecture, and provides usage examples for tasks like document question answering, classification, and parsing, along with details on quantization and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/deepseek_vl_hybrid.md",
            "description": "This file provides documentation and usage examples for the DeepseekVLHybrid vision-language model in the Hugging Face Transformers library. It details how to use the model for image-text tasks, including setup, inference, and quantization, and includes directives for generating API reference documentation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/deplot.md",
            "description": "This file provides documentation for the DePlot model within the Hugging Face Transformers library, detailing its purpose (plot-to-table translation for visual language reasoning), underlying architecture, and offering usage examples for inference and fine-tuning.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/fast_vlm.md",
            "description": "This file provides documentation for the FastVLM model, including its overview, features, usage examples, and specific implementation details within the Hugging Face Transformers library. It details how to use FastVLM for single and batched inference, highlighting its efficiency as a vision-language model.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/glm46v.md",
            "description": "This file provides documentation for the GLM-4.6V vision-language model, including its overview, associated classes like `Config`, `ImageProcessor`, `VideoProcessor`, and `Processor`, and model implementations for Hugging Face Transformers.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/flava.md",
            "description": "This file provides comprehensive documentation for the FLAVA (Foundational Language And Vision Alignment) model within the Hugging Face Transformers library. It details the model's overview, paper, and lists various related configurations, processors, and model classes with their key methods.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/gemma3.md",
            "description": "This document provides detailed information and usage examples for the Gemma 3 multimodal model within the Hugging Face Transformers library. It covers the model's architecture, capabilities like image-to-text generation and quantization, and includes API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/gemma3n.md",
            "description": "This file provides documentation and usage examples for the Gemma3n multimodal model within the Hugging Face Transformers library. It details the model's architecture, new features, and how to use it for various tasks including image-to-text generation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/git.md",
            "description": "This document provides an overview and detailed documentation for the Generative Image-to-text Transformer (GIT) model within the Hugging Face Transformers library, including its architecture, usage, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glm4v.md",
            "description": "This document provides an overview, usage examples, and API reference for the GLM-V vision-language model within the Hugging Face Transformers library. It details how to use GLM-V for image and video processing tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/glm_image.md",
            "description": "This file provides documentation and usage examples for the GlmImage model within the Hugging Face Transformers library, detailing its architecture, capabilities, and associated classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/ernie4_5_vl_moe.md",
            "description": "This file provides documentation for the Ernie 4.5 VL MoE model within the Hugging Face Transformers library. It details the model's architecture, usage examples for image-to-text and video-to-text generation, and includes autodocs for its associated classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/exaone4.md",
            "description": "This file provides documentation for the EXAONE 4 model, detailing its features, architectural changes, usage examples for different modes (non-reasoning, reasoning, and agentic tool use), and API references for its integration within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/idefics3.md",
            "description": "This file is a documentation page for the Idefics3 model within the Hugging Face Transformers library, detailing its architecture, usage, and providing API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/instructblipvideo.md",
            "description": "This file provides documentation for the InstructBlipVideo model, detailing its overview, architecture, usage tips, and API references for its various components within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/florence2.md",
            "description": "This file provides documentation for the Florence-2 model within the Hugging Face Transformers library. It details the model's capabilities for various vision and vision-language tasks, and includes code examples for inference and quantization.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/idefics2.md",
            "description": "This document provides an overview and usage guide for the Idefics2 multimodal model within the Hugging Face Transformers library. It includes details on its architecture, usage tips, and optimization techniques like Flash Attention and quantization.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glm4v_moe.md",
            "description": "This file provides documentation for the Glm4vMoe model, including an overview, its scientific paper abstract, supported models, and detailed API references for its various components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/internvl.md",
            "description": "This documentation file describes the InternVL family of Visual Language Models, covering its architecture, performance, and providing usage examples within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/llava.md",
            "description": "This document provides an overview, usage tips, and detailed examples for using the LLaVa multimodal model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/instructblip.md",
            "description": "This file provides documentation for the InstructBLIP model within the Hugging Face Transformers library, detailing its overview, architecture, usage tips, and API reference for its various components and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/lfm2_vl.md",
            "description": "This file provides documentation for the LFM2-VL vision-language model within the Hugging Face Transformers library. It includes an overview, architecture details, a usage example, and API references for its components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/llava_next.md",
            "description": "This file provides documentation for the LLaVA-NeXT model within the Hugging Face Transformers library, detailing its features, improvements, and offering Python code examples for image-to-text generation, quantization, and multi-image inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/janus.md",
            "description": "This file provides documentation and usage examples for the Janus model, a vision-language model capable of both understanding and generating text and images, within the Hugging Face Transformers library. It covers its overview, inference examples, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/idefics.md",
            "description": "This file provides documentation for the IDEFICS model within the Hugging Face Transformers library, including an overview, its research origins, and API references for its various components like configuration, model, and processors.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/groupvit.md",
            "description": "This file provides documentation for the GroupViT model within the Hugging Face Transformers library. It includes an overview of the model, its capabilities (like zero-shot semantic segmentation), usage tips, and references to relevant configurations and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/metaclip_2.md",
            "description": "This document provides an overview and usage guide for the MetaCLIP 2 model within the Hugging Face Transformers library. It includes details on its capabilities, code examples for zero-shot image classification, and API references for its various components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/grounding-dino.md",
            "description": "This file provides comprehensive documentation for the Grounding DINO model within the Hugging Face Transformers library. It covers an overview of the model, usage examples for zero-shot object detection, integration with SAM, and API references for related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/llava_onevision.md",
            "description": "This file documents the LLaVA-OneVision model within the Hugging Face Transformers library, detailing its architecture, features, and providing usage examples for single and multi-image inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/kosmos-2.md",
            "description": "This file provides documentation for the KOSMOS-2 multimodal large language model within the Hugging Face Transformers library. It includes an overview, capabilities, usage examples, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/matcha.md",
            "description": "This file is a documentation page for the MatCha model within the Hugging Face Transformers library. It provides an overview of the model, its capabilities in visual language processing (especially with charts and math reasoning), available checkpoints, and usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/kosmos2_5.md",
            "description": "This documentation file describes the KOSMOS-2.5 multimodal literate model, explaining its capabilities for processing text-intensive images and providing code examples for its usage within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/markuplm.md",
            "description": "This file provides documentation for the MarkupLM model within the Hugging Face Transformers library, detailing its overview, architecture, and usage with its dedicated processor for tasks like web page classification and question answering.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/lxmert.md",
            "description": "This file provides comprehensive documentation for the LXMERT model within the Hugging Face Transformers library, detailing its architecture, pre-training objectives, usage tips, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/fuyu.md",
            "description": "This file is a documentation page for the Fuyu multimodal model within the Hugging Face Transformers library. It provides an overview of the model, usage instructions, conversion tips, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mm-grounding-dino.md",
            "description": "This file provides documentation for the MM Grounding DINO model, including its features, usage examples with the Hugging Face Transformers library, and performance benchmarks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/mllama.md",
            "description": "This file provides documentation for the Mllama (Llama 3.2-Vision) multimodal large language model within the Hugging Face Transformers library. It includes an overview, usage tips, code examples, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/llava_next_video.md",
            "description": "This file provides documentation for the LLaVA-NeXT-Video model in the Hugging Face Transformers library. It details the model's overview, capabilities, usage tips, and includes code examples for both single and mixed-media inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/ovis2.md",
            "description": "This file provides documentation for the Ovis2 multi-modal large language model within the Hugging Face Transformers library. It includes an overview, architecture, usage examples, and API references for its components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/owlv2.md",
            "description": "This file provides documentation for the OWLv2 model in the Hugging Face Transformers library, detailing its architecture, capabilities, and providing usage examples for zero-shot object detection.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pix2struct.md",
            "description": "This file is a documentation page for the Pix2Struct model within the Hugging Face Transformers library. It provides an overview of the model, its capabilities, usage tips, and details its various components and configurations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/paddleocr_vl.md",
            "description": "This file is a documentation page for the PaddleOCR-VL model within the Hugging Face Transformers library, providing an overview of its features, architecture, and usage examples for document parsing and element-level recognition.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/nougat.md",
            "description": "This file provides documentation for the Nougat model within the Hugging Face Transformers library. It details the model's architecture, purpose (converting scientific PDFs to markdown), usage, and includes code examples for inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/patchtst.md",
            "description": "This file provides comprehensive documentation for the PatchTST model within the Hugging Face Transformers library, detailing its overview, architecture, usage tips, and links to relevant classes for various time series tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pe_audio_video.md",
            "description": "This file is a documentation draft for the PE Audio Video (Perception Encoder Audio-Video) model within the Hugging Face Transformers library. It outlines its overview, usage, and auto-generates documentation for its associated classes like processors, configurations, and models.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/perceiver.md",
            "description": "This file provides documentation for the Perceiver and Perceiver IO models, detailing their architecture, functionality, and how they are implemented within the Hugging Face Transformers library, including available classes and resources.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pixtral.md",
            "description": "This file provides documentation for the Pixtral multimodal model within the Hugging Face Transformers library. It details the model's capabilities, architecture, and offers Python code examples for its usage, including quantization.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/paligemma.md",
            "description": "This file provides documentation and usage examples for the PaliGemma and PaliGemma 2 vision-language models within the Hugging Face Transformers library, including code snippets for text generation, quantization, and attention visualization. It also serves as an API reference for PaliGemma components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pe_audio.md",
            "description": "This file provides documentation for the PE Audio (Perception Encoder Audio) model within the Hugging Face Transformers library. It describes the model's overview, its capability to embed audio and text into a shared space for cross-modal retrieval, and details its various components and usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/perception_lm.md",
            "description": "This file provides documentation for the PerceptionLM model within the Hugging Face Transformers library. It includes an overview of the model, its paper, and details related to its configuration, processors, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/qwen3_vl.md",
            "description": "This file provides documentation and usage examples for the Qwen3-VL multimodal vision-language model series within the Hugging Face Transformers library. It details its architecture, capabilities, and includes code snippets for model usage and references to its various components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/mistral3.md",
            "description": "This file provides documentation for the Mistral 3 model within the Hugging Face Transformers library, detailing its features, usage examples for text and image-to-text generation, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/qwen2_5_omni.md",
            "description": "This file provides comprehensive documentation for the Qwen2.5-Omni multimodal model within the Hugging Face Transformers library. It details the model's features, capabilities, and offers various code examples for its usage with different input modalities (text, image, audio, video) and output types.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/qwen2_vl.md",
            "description": "This file provides comprehensive documentation for the Qwen2-VL multimodal model within the Hugging Face Transformers library, detailing its features, usage examples for inference with images and videos, and tips for optimal configuration.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/siglip2.md",
            "description": "This document provides an overview and usage guide for the SigLIP2 model, including examples for zero-shot image classification, text embeddings, and quantization. It details its two variants, NaFlex and FixRes, and explains important preprocessing steps for optimal performance.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/phi4_multimodal.md",
            "description": "This documentation file describes the Phi4 Multimodal model, outlining its capabilities for processing text, image, and audio inputs. It provides code examples for its usage with the Hugging Face Transformers library and references for related API components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/qwen3_vl_moe.md",
            "description": "This file provides documentation for the Qwen3-VL-Moe multimodal vision-language model within the Hugging Face Transformers library. It details the model's capabilities, architectural innovations, and includes usage examples and API references for its various components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/qwen2_5_vl.md",
            "description": "This file provides documentation for the Qwen2.5-VL multimodal vision-language model, including its features, usage examples with the Hugging Face Transformers library, quantization details, and handling of various input types like images and videos.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/smolvlm.md",
            "description": "This file provides documentation for the SmolVLM model, detailing its overview, usage tips, and code examples for both single and batch media inference. It also lists the relevant classes and methods for the model and its processors.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "description": "This file provides documentation for the SeamlessM4T-v2 model in the Hugging Face Transformers library. It details the model's capabilities for various multilingual speech and text translation tasks, explains its architecture, and offers usage examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/seamless_m4t.md",
            "description": "This file provides documentation for the SeamlessM4T model in Hugging Face Transformers, detailing its capabilities for massively multilingual and multimodal machine translation, usage examples, and architecture.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "description": "This document provides an overview, usage notes, and code examples for the Qwen3-Omni-MOE model, a unified multiple modalities model, within the Hugging Face Transformers library. It details how to perform inference with various media types using Python.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/prompt_depth_anything.md",
            "description": "This file provides documentation for the 'Prompt Depth Anything' model within the Hugging Face Transformers library. It includes an overview, usage examples, and API references for the model and its associated components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/tvp.md",
            "description": "This file documents the TVP (Text-Visual Prompting) model, explaining its architecture, purpose for temporal video grounding, and providing usage examples within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/sam3.md",
            "description": "This file documents the SAM3 (Segment Anything Model 3) model, explaining its Promptable Concept Segmentation capabilities. It provides various usage examples with the Hugging Face Transformers library, demonstrating text, box, and combined prompting for segmentation tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/video_llava.md",
            "description": "This file provides comprehensive documentation for the Video-LLaVA model within the Hugging Face Transformers library, covering its overview, usage examples for both image and video inputs, and optimization techniques like quantization and Flash Attention 2.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vipllava.md",
            "description": "This file provides documentation for the VipLlava model, including its overview, usage tips, and API references for its configuration, model, and conditional generation classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/siglip.md",
            "description": "This document provides comprehensive documentation for the SigLIP image-text model within the Hugging Face Transformers library. It explains its features, usage examples, quantization, and details its various components and classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/udop.md",
            "description": "This file documents the UDOP model within the Hugging Face Transformers library, providing an overview of its architecture, usage tips, and references to its API components. It describes UDOP's capabilities in unifying vision, text, and layout for universal document processing tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/sam3_tracker_video.md",
            "description": "This file provides documentation and usage examples for the SAM3 Tracker Video model in the Hugging Face Transformers library. It explains how to perform promptable visual segmentation (PVS) and object tracking across video frames, including single-object, multi-object, refinement, and streaming inference scenarios.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/visual_bert.md",
            "description": "This documentation file describes the VisualBERT model, a vision-and-language model in the Hugging Face Transformers library. It explains its architecture, usage, and provides an example for visual question answering, along with references to its various specialized classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/video_llama_3.md",
            "description": "This file documents the VideoLLaMA3 multimodal foundation model within the Hugging Face Transformers library. It provides an overview of the model, its architecture, usage examples for inference with images and videos, and references to its various components and configurations.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/vision-text-dual-encoder.md",
            "description": "This file documents the `VisionTextDualEncoder` model within the Hugging Face Transformers library, explaining its architecture, functionality, and use cases for vision-text tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/xclip.md",
            "description": "This file provides the documentation for the X-CLIP model within the Hugging Face Transformers library. It includes an overview of the model, its architecture, usage tips, and details on associated classes like XCLIPProcessor and XCLIPModel.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/voxtral.md",
            "description": "This file provides documentation for the Voxtral model within the Hugging Face Transformers library. It details the model's features, capabilities, and offers various Python code examples for its usage with different audio and text interaction modes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vilt.md",
            "description": "This file provides documentation for the ViLT (Vision-and-Language Transformer) model within the Hugging Face Transformers library. It includes an overview of the model, usage tips, and API references for its various components and tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vision-encoder-decoder.md",
            "description": "This documentation page describes the Hugging Face Transformers `VisionEncoderDecoderModel`, detailing its overview, initialization methods, inference procedures, and training considerations for image-to-text tasks.",
            "spof": true
          },
          {
            "path": "docs/source/es/tasks/image_captioning.md",
            "description": "This document provides a guide on how to fine-tune an image captioning model using the Hugging Face Transformers library. It covers data preprocessing, model training, and inference, exemplified with a Pok√©mon dataset.",
            "spof": true
          },
          {
            "path": "docs/source/ja/main_classes/processors.md",
            "description": "This document provides a Japanese explanation of processors in the Transformers library, differentiating between current multi-modal processors for models like Wav2Vec2 and CLIP, and older, deprecated processors used for tasks like GLUE, XNLI, and SQuAD.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/idefics.md",
            "description": "This file is a Japanese documentation guide on using the IDEFICS multi-modal model within the Hugging Face Transformers library for various image-related tasks such as captioning, visual question answering, and few-shot prompting.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/image_captioning.md",
            "description": "This document, written in Japanese, provides a guide on how to fine-tune an image captioning model using the Hugging Face Transformers library and then use the fine-tuned model for inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/document_question_answering.md",
            "description": "This documentation page, written in Japanese, provides a guide on performing Document Question Answering using Hugging Face Transformers. It details how to fine-tune a LayoutLMv2 model on the DocVQA dataset and preprocess data for this task.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/visual_question_answering.md",
            "description": "This document provides a guide in Japanese on how to perform Visual Question Answering (VQA) using the Hugging Face Transformers library, covering fine-tuning models like ViLT and performing inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/blip-2.md",
            "description": "This document provides Japanese-language documentation for the BLIP-2 model in the Hugging Face Transformers library, including an overview, usage tips, resources, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/altclip.md",
            "description": "This file is a Japanese documentation page for the AltCLIP model within the Hugging Face Transformers library, explaining its capabilities, how it extends CLIP with multilingual understanding, and providing usage examples.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bridgetower.md",
            "description": "This file is a Japanese documentation page for the BridgeTower model within the Hugging Face Transformers library. It provides an overview of the model, its architecture, usage examples for various tasks, and references to relevant classes and methods.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/align.md",
            "description": "This file provides the Japanese documentation for the ALIGN model, including its overview, how to use it for image-text similarity, and references to its configurations and model classes within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/blip.md",
            "description": "This file provides the Japanese documentation for the BLIP model within the Hugging Face Transformers library. It includes an overview of the model, its capabilities, the research paper summary, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/clip.md",
            "description": "This file provides Japanese-language documentation for the CLIP model within the Hugging Face Transformers library. It includes an overview, usage examples, resources, and API references for various CLIP components.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/clipseg.md",
            "description": "This file provides Japanese-language documentation for the CLIPSeg model, covering its overview, usage tips, and API reference for various components like configurations, processors, and models for image segmentation.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/decision_transformer.md",
            "description": "This file provides Japanese documentation for the Decision Transformer model, including an overview, its scientific paper summary, and references to its configuration and model classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/data2vec.md",
            "description": "This file provides Japanese-language documentation for the Data2Vec model, detailing its overview, usage tips, and resources for its Audio, Text, and Vision implementations within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/clvp.md",
            "description": "This document provides an overview, usage guide, and API reference for the CLVP (Contrastive Language-Voice Pretrained Transformer) model, translated into Japanese, as part of the Hugging Face Transformers documentation.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bros.md",
            "description": "This document provides a detailed overview and usage guide for the BROS (BERT Relying On Spatality) model, covering its architecture, pre-training objectives, different task-specific heads, and tips for working with bounding box information in Japanese.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/clap.md",
            "description": "This file provides Japanese-language documentation for the CLAP (Contrastive Language-Audio Pretraining) model within the Hugging Face Transformers library. It includes an overview, the original paper's summary, and API documentation for various CLAP-related classes and methods.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/deplot.md",
            "description": "This file is the Japanese documentation for DePlot, a model built on the Pix2Struct architecture designed for one-shot visual language reasoning through plot-to-table translation. It explains the model's overview, provides usage examples, and details fine-tuning instructions.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/clipseg.md",
            "description": "This file is a Korean documentation page for the CLIPSeg model within the Hugging Face Transformers library. It provides an overview of the model, usage tips, resources, and API references for its various components.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/blip-2.md",
            "description": "This file provides Korean documentation for the BLIP-2 model within the Hugging Face Transformers library, detailing its overview, usage, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/paligemma.md",
            "description": "This file provides Korean-language documentation for the PaliGemma model, including an overview, usage tips, inference examples, fine-tuning guidance, and links to relevant resources. It also documents the `PaliGemmaConfig`, `PaliGemmaProcessor`, and `PaliGemmaForConditionalGeneration` classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/chameleon.md",
            "description": "This file provides Korean-language documentation for the Chameleon mixed-modal foundation model within the Hugging Face Transformers library. It describes the model's capabilities, offers usage examples for single and multi-image inference, and details optimization techniques like quantization and Flash Attention 2.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/blip.md",
            "description": "This file provides Korean-language documentation for the BLIP (Bootstrapping Language-Image Pre-training) model, detailing its overview, applications, and how to use its various components (e.g., configurations, processors, and specific model classes) within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/gemma3.md",
            "description": "This file is a Korean documentation page for the Gemma 3 multimodal model within the Hugging Face Transformers library, detailing its architecture, features, usage examples for text generation and quantization, and API references for related classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/altclip.md",
            "description": "This file provides documentation for the AltCLIP model, detailing its overview, architecture, usage tips, and API references within the Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/smolvlm.md",
            "description": "This document provides a Korean-language guide to the SmolVLM (SmolVLM2) model within the Hugging Face Transformers library, detailing its features, usage tips, and providing code examples for image and video inference.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/qwen2_vl.md",
            "description": "This document provides a Korean-language overview, usage examples, and tips for the Qwen2-VL multimodal model within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/siglip.md",
            "description": "This file provides Korean language documentation for the SigLIP model within the Hugging Face Transformers library, covering its overview, usage tips, code examples, and integration with optimization techniques like Flash Attention 2 and SDPA.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/grounding-dino.md",
            "description": "This file provides Korean language documentation for the Grounding DINO model, covering its overview, usage tips for zero-shot object detection, and its integration with the Segment Anything (SAM) model. It also lists related resources and API references for the GroundingDino components within the Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/clip.md",
            "description": "This document provides a Korean-language overview of the CLIP model, including its architecture, usage with code examples, and performance optimization tips using Flash Attention 2 and SDPA. It serves as a comprehensive guide for integrating and utilizing CLIP within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/gemma3n.md",
            "description": "This document provides Korean-language documentation for the Gemma3n multimodal model, detailing its architecture, new techniques, supported modalities (vision, audio, text), and usage examples with the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/tvp.md",
            "description": "This file documents the Text-Visual Prompting (TVP) model for Temporal Video Grounding in Korean, providing an overview of the model, usage examples, and API references for its components.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/xclip.md",
            "description": "This file provides the Korean-language documentation for the X-CLIP model, including an overview, architectural details, relevant resources, and API references for its components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/idefics.md",
            "description": "This file is a Korean documentation page detailing how to use the Hugging Face Transformers library's IDEFICS model for various image-text tasks, including image captioning, visual question answering, and few-shot prompting. It covers loading the model, including quantized versions, and provides code examples for each task.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/image_captioning.md",
            "description": "This document is a Korean-language guide on image captioning, demonstrating how to fine-tune a model using the Hugging Face Transformers library with the Pokemon BLIP Captions dataset and perform inference.",
            "spof": false
          },
          {
            "path": "docs/source/ko/tasks/document_question_answering.md",
            "description": "This document is a Korean-language guide explaining Document Question Answering (DQA), specifically detailing how to fine-tune a LayoutLMv2 model on the DocVQA dataset and prepare data for inference.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/visual_question_answering.md",
            "description": "This document provides a guide on Visual Question Answering (VQA) using the Hugging Face Transformers library, covering fine-tuning ViLT models and performing inference.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Yoni Gozlan",
            "percent": 8
          },
          {
            "name": "Raushan Turganbay",
            "percent": 6
          },
          {
            "name": "Yuxuan Zhang",
            "percent": 5
          }
        ]
      },
      "Task-Specific Inference Pipelines": {
        "files": [
          {
            "path": "src/transformers/pipelines/any_to_any.py",
            "description": "This file defines the `AnyToAnyPipeline` class, which is a multimodal generation pipeline in the Transformers library. It generates text outputs from various input modalities such as text, images, videos, and audio, and supports conversational models for chat-based interactions.",
            "spof": true
          },
          {
            "path": "src/transformers/pipelines/zero_shot_image_classification.py",
            "description": "This file implements the ZeroShotImageClassificationPipeline, which uses models like CLIP to classify images against a set of user-provided candidate labels without needing explicit training on those labels. It provides methods for preprocessing images and text, running model inference, and postprocessing results into scored labels.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/visual_question_answering.py",
            "description": "This file implements the Visual Question Answering (VQA) pipeline for the Hugging Face Transformers library. It enables users to ask questions about images using pre-trained models, handling input processing, model inference, and result post-processing.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Raushan Turganbay",
            "percent": 75
          },
          {
            "name": "Cyril Vallez",
            "percent": 12
          },
          {
            "name": "Matt",
            "percent": 4
          }
        ]
      },
      "Model Weight Conversion Utilities": {
        "files": [
          {
            "path": "src/transformers/models/align/convert_align_tf_to_hf.py",
            "description": "This script converts the weights of the ALIGN model from its original TensorFlow implementation to a Hugging Face PyTorch compatible format. It includes functions for preparing configuration, processing images, and renaming model keys to match the Hugging Face architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/aimv2/convert_aimv2_original_pytorch_to_hf.py",
            "description": "This script converts original PyTorch AIMV2 model weights and image processor configurations into the Hugging Face Transformers format, handling state dictionary key mapping and structural adjustments.",
            "spof": true
          },
          {
            "path": "src/transformers/models/aria/convert_aria_weights_to_hf.py",
            "description": "This script converts the weights of an Aria model from its original format to the Hugging Face Transformers format. It then pushes the converted model and its processor to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "src/transformers/models/audioflamingo3/convert_audioflamingo3_to_hf.py",
            "description": "This script converts original AudioFlamingo3 model checkpoints from NVIDIA into the Hugging Face Transformers format. It handles the conversion of both the processor (tokenizer and feature extractor) and the model weights, with an option to push the converted assets to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "src/transformers/models/blip_2/convert_blip_2_original_to_pytorch.py",
            "description": "This script converts BLIP-2 model checkpoints from the original LAVIS repository to the Hugging Face Transformers format. It handles various BLIP-2 configurations (e.g., OPT-based, T5-based) and their corresponding weight mappings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py",
            "description": "This script converts pre-trained BLIP (Bootstrapping Language-Image Pre-training) models from their original PyTorch format to the Hugging Face Transformers format, handling models for conditional generation, image-text retrieval, and question answering.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chinese_clip/convert_chinese_clip_original_pytorch_to_hf.py",
            "description": "This script converts an original PyTorch Chinese-CLIP model checkpoint to the Hugging Face Transformers format, mapping the weights from the original structure to the Hugging Face model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "description": "This script converts pre-trained Chameleon model weights from their original format into a format compatible with the Hugging Face Transformers library. It handles loading and mapping sharded or unsharded model checkpoints to create a Hugging Face model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py",
            "description": "This script converts CLIPSeg model checkpoints from their original PyTorch format to the Hugging Face Transformers format. It handles renaming keys, restructuring the state dictionary, and verifying the conversion before saving or pushing to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clap/convert_clap_original_pytorch_to_hf.py",
            "description": "This script converts an original PyTorch CLAP model checkpoint from the `laion_clap` library into a Hugging Face `ClapModel` format, adapting its configuration and state dictionary.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py",
            "description": "This script converts an original OpenAI PyTorch CLIP model checkpoint into a Hugging Face Transformers compatible format, transferring weights and verifying output consistency.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clvp/convert_clvp_to_hf.py",
            "description": "This script converts pre-trained CLVP (Contrastive Learning for Video Pre-training) model weights from their original format into a Hugging Face Transformers-compatible format. It handles downloading checkpoints, remapping parameter keys for both encoder and decoder components, and then saving the converted model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/colqwen2/convert_colqwen2_weights_to_hf.py",
            "description": "This script converts ColQwen2 model weights from their original format to the Hugging Face Transformers format. It loads original weights, renames them, configures a new HF model, and then saves or pushes the converted model to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "src/transformers/models/colpali/convert_colpali_weights_to_hf.py",
            "description": "This script converts the ColPali model weights from its original format to the Hugging Face Transformers model format, allowing it to be used within the Hugging Face ecosystem.",
            "spof": false
          },
          {
            "path": "src/transformers/models/csm/convert_csm.py",
            "description": "This script converts a pre-trained CSM (Conditional Spoken Model) checkpoint into a Hugging Face Transformers-compatible format, including its model weights, tokenizer, and processor, handling parameter renaming and ROPE permutation.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_vl/convert_deepseek_vl_weights_to_hf.py",
            "description": "This file is responsible for converting original Deepseek-VL model weights into a format compatible with Hugging Face Transformers. It handles key remapping, QKV weight splitting, and saving the converted model configuration and weights.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deepseek_vl_hybrid/convert_deepseek_vl_hybrid_weights_to_hf.py",
            "description": "This script converts original Deepseek-VL-Hybrid model weights to the Hugging Face Transformers format. It remaps parameter names to align with the Hugging Face architecture for both high and low-resolution vision models and the language model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dia/convert_dia_to_hf.py",
            "description": "This script converts a Dia model from the Nari Labs format into the Hugging Face Transformers format, handling weight reshaping, renaming, and combining embeddings. It also includes functionality to convert the associated preprocessor (tokenizer and feature extractor).",
            "spof": true
          },
          {
            "path": "src/transformers/models/donut/convert_donut_to_pytorch.py",
            "description": "This script converts checkpoints from the original `donut-python` library into a format compatible with Hugging Face Transformers. It reconstructs the model using Hugging Face components, adapts the state dictionary, and includes verification steps.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dit/convert_dit_unilm_to_pytorch.py",
            "description": "This script converts Document Image Transformer (DiT) model checkpoints from the UNILM repository to the Hugging Face Transformers' Beit model format, enabling their use within the Transformers library. It renames model keys and handles model loading, saving, and optional pushing to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "description": "This script converts original Emu3 model weights and tokenizers (including Tiktoken for the LLM and VQ-GAN for image processing) into the Hugging Face Transformers format. It allows for loading and using Emu3 models with the Hugging Face library after conversion.",
            "spof": true
          },
          {
            "path": "src/transformers/models/ernie4_5_vl_moe/convert_ernie4_5_vl_moe_to_hf.py",
            "description": "This script converts the Ernie 4.5 VL model configuration and processor components (vision config, text config, and tokenizer) into the Hugging Face Transformers format, preparing it for use within the Hugging Face ecosystem.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fast_vlm/convert_fastvlm_weights_to_hf.py",
            "description": "This script converts original FastVLM model weights from their native format to a Hugging Face Transformers compatible format. It loads the original state dictionary, re-maps keys to match the Hugging Face model architecture, and then pushes the converted model and its processor to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "src/transformers/models/flava/convert_dalle_to_flava_codebook.py",
            "description": "This script converts a pre-trained DALL-E image encoder checkpoint into a compatible Hugging Face FlavaImageCodebook model checkpoint. It includes functions to rename state dictionary keys to match the Hugging Face model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flava/convert_flava_original_pytorch_to_hf.py",
            "description": "This script converts an original PyTorch FLAVA model checkpoint and its DALL-E codebook checkpoint into the Hugging Face Transformers format. It remaps state dictionary keys and saves the converted model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/florence2/convert_florence2_original_pytorch_to_hf.py",
            "description": "This script converts an original PyTorch implementation of the Florence-2 model to the Hugging Face Transformers format. It includes functions for converting model configurations and renaming various vision tower layer weights.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fuyu/convert_fuyu_model_weights_to_hf.py",
            "description": "This script converts Fuyu model weights from their original format (Adept's `.pt` files) into a Hugging Face Transformers compatible format, including the model and tokenizer.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma3n/convert_gemma3n_weights.py",
            "description": "This script is a utility to convert Gemma 3 model weights from the Orbax checkpoint format to the Hugging Face Transformers format, supporting various model components like audio and vision encoders.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma3/convert_gemma3_weights.py",
            "description": "This utility script converts Gemma3 model checkpoints from the Orbax format to the Hugging Face Transformers format. It supports various Gemma3 model variants and can include a vision encoder during the conversion process.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Ryan Mullins",
            "percent": 15
          },
          {
            "name": "Armaghan Shakir",
            "percent": 13
          },
          {
            "name": "Anton Vlasjuk",
            "percent": 10
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 457,
      "spofCount": 201
    },
    "busFactor": 16,
    "authorCount": 96
  },
  "Audio & Speech Model Library": {
    "description": "A suite of models for audio-centric tasks including automatic speech recognition (ASR), audio classification, and text-to-speech (TTS) synthesis across numerous languages.",
    "functions": {
      "Audio Feature Extraction and Utilities": {
        "files": [
          {
            "path": "utils/test_module/custom_feature_extraction.py",
            "description": "This file defines a custom feature extractor, `CustomFeatureExtractor`, which inherits directly from `Wav2Vec2FeatureExtractor` without adding new functionality. It likely serves as a placeholder or example within a test module for extending existing feature extractors.",
            "spof": true
          },
          {
            "path": "docs/source/en/feature_extractors.md",
            "description": "This document explains Hugging Face Transformers feature extractors, detailing how they preprocess audio data for models. It covers loading, usage, and functionalities such as padding, truncation, and resampling.",
            "spof": true
          },
          {
            "path": "docs/source/en/internal/audio_utils.md",
            "description": "This document lists and describes internal utility functions used by audio feature extractors in the `transformers` library to compute audio features using algorithms like STFT and log mel spectrogram. It serves as documentation for developers studying the audio processors' code.",
            "spof": false
          },
          {
            "path": "docs/source/zh/internal/audio_utils.md",
            "description": "This document lists utility functions for audio `FeatureExtractors` to compute special features from raw audio using common algorithms like Short Time Fourier Transform or log mel spectrogram. It is primarily useful for those examining the library's audio processor code.",
            "spof": true
          },
          {
            "path": "docs/source/ja/internal/audio_utils.md",
            "description": "This file documents utility functions for audio processing within `FeatureExtractor`, covering transformations like Short-Time Fourier Transform and log-mel spectrogram. It lists specific functions for audio conversions used internally by the library's audio processors.",
            "spof": true
          },
          {
            "path": "docs/source/ko/internal/audio_utils.md",
            "description": "This document provides Korean-language documentation for internal utility functions used by audio `FeatureExtractor`s in the Transformers library, detailing how they compute features like STFT or log mel spectrogram from raw audio.",
            "spof": true
          },
          {
            "path": "tests/utils/test_audio_utils.py",
            "description": "This file contains unit tests for the audio utility functions in the `transformers` library, verifying their correctness and behavior for various audio transformations like Mel scale conversions and filter bank generation.",
            "spof": false
          },
          {
            "path": "tests/models/audio_spectrogram_transformer/test_feature_extraction_audio_spectrogram_transformer.py",
            "description": "This file contains unit tests for the `ASTFeatureExtractor` class, ensuring its functionality for audio feature extraction, including integration tests and edge cases like torch double-precision padding and saving/loading configurations.",
            "spof": false
          },
          {
            "path": "tests/models/dia/test_feature_extraction_dia.py",
            "description": "This file contains unit tests for the `DiaFeatureExtractor` in the Hugging Face Transformers library, ensuring its feature extraction capabilities, padding, truncation, and integration work correctly.",
            "spof": true
          },
          {
            "path": "tests/models/gemma3n/test_feature_extraction_gemma3n.py",
            "description": "This file contains unit tests for the `Gemma3nAudioFeatureExtractor` class, verifying its functionality, serialization, and correct feature extraction from audio inputs.",
            "spof": true
          },
          {
            "path": "src/transformers/audio_utils.py",
            "description": "This file contains audio processing utilities, primarily for loading audio from various sources (files, URLs) and formats (numpy arrays, torch tensors), resampling, and converting audio data. It aims to provide a unified way to handle audio inputs for feature extraction and model inference across different frameworks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py",
            "description": "This file defines the `ASTFeatureExtractor` class, which is responsible for extracting, padding, and normalizing mel-filter bank features from raw audio input for the Audio Spectrogram Transformer (AST) model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clap/feature_extraction_clap.py",
            "description": "This file defines the `ClapFeatureExtractor` class, which extracts mel-filter bank features from raw audio input for the CLAP model. It handles processes like Short Time Fourier Transform (STFT), mel spectrogram computation, and various audio truncation/padding strategies.",
            "spof": false
          },
          {
            "path": "src/transformers/models/clvp/feature_extraction_clvp.py",
            "description": "This file defines the `ClvpFeatureExtractor` class, which is responsible for extracting log-mel-spectrogram features from raw audio input for the CLVP model within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dia/feature_extraction_dia.py",
            "description": "This file defines the `DiaFeatureExtractor` class, which is responsible for processing raw audio data into features suitable for the Dia model. It handles operations like padding, truncation, and converting stereo audio to mono if required.",
            "spof": true
          },
          {
            "path": "src/transformers/models/gemma3n/feature_extraction_gemma3n.py",
            "description": "This file implements the `Gemma3nAudioFeatureExtractor` class, which is responsible for extracting audio features for Universal Speech Models (like Gemma3n). It includes methods for creating Mel-frequency filter banks and performing signal processing steps like pre-emphasis and dithering.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Ryan Mullins",
            "percent": 22
          },
          {
            "name": "Jaeyong Sung",
            "percent": 15
          },
          {
            "name": "Cyril Vallez",
            "percent": 12
          }
        ]
      },
      "Audio Classification and Analysis": {
        "files": [
          {
            "path": "examples/pytorch/audio-classification/README.md",
            "description": "This README provides examples and instructions for fine-tuning Wav2Vec2 and similar pre-trained models for audio classification tasks using PyTorch, including keyword spotting and language identification on both single and multi-GPU setups. It also details how to share fine-tuned models on the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/audio_classification.md",
            "description": "This document provides a comprehensive guide on how to perform audio classification using the Hugging Face Transformers library, detailing steps from data preprocessing and model fine-tuning to evaluation and inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/audio-spectrogram-transformer.md",
            "description": "This file is a documentation page for the Audio Spectrogram Transformer (AST) model, detailing its overview, usage tips, and API references within the Hugging Face Transformers library for audio classification tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pop2piano.md",
            "description": "This file provides documentation for the Pop2Piano model within the Hugging Face Transformers library, including its overview, working principles, usage instructions, and code examples for generating piano covers from audio.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/qwen2_audio.md",
            "description": "This file provides documentation for the Qwen2-Audio model, detailing its overview, capabilities, and offering Python code examples for various inference modes like voice chat and audio analysis.",
            "spof": false
          },
          {
            "path": "docs/source/es/tasks/audio_classification.md",
            "description": "This document is a Spanish-language guide on how to fine-tune a Wav2Vec2 model for audio classification using the Hugging Face `transformers` library. It covers data loading, preprocessing, training, and evaluation for speaker intent classification.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/audio_classification.md",
            "description": "This document provides a Japanese-language guide on how to perform audio classification using the Hugging Face Transformers library. It details fine-tuning a Wav2Vec2 model on the MInDS-14 dataset for speaker intent classification and then performing inference with the trained model.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/audio-spectrogram-transformer.md",
            "description": "This document provides a Japanese overview of the Audio Spectrogram Transformer (AST) model, including its architecture, usage tips for fine-tuning, and references within the Hugging Face Transformers library for audio classification.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_audio_classification.py",
            "description": "This file contains unit tests for the Audio Classification Pipeline in the Hugging Face Transformers library, covering various functionalities like small and large model inference, different input types, and `top_k` behavior.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_zero_shot_audio_classification.py",
            "description": "This file contains unit tests for the zero-shot audio classification pipeline within the Hugging Face Transformers library. It verifies the functionality of the pipeline using both small and large models with various inputs and assertions on the output.",
            "spof": false
          },
          {
            "path": "tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py",
            "description": "This file contains the testing suite for the PyTorch Audio Spectrogram Transformer (AST) model, including unit tests for its configuration, model components, and an integration test for audio classification with a pre-trained model.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/zero_shot_audio_classification.py",
            "description": "Implements the `ZeroShotAudioClassificationPipeline` for performing zero-shot classification on audio inputs. It allows users to classify audio based on a set of provided candidate labels without explicit training for those labels.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/audio_classification.py",
            "description": "This file implements the audio classification pipeline for the Hugging Face Transformers library, enabling classification of various audio inputs (files, bytes, arrays) using `AutoModelForAudioClassification` models. It includes utilities for reading audio with ffmpeg and preprocessing audio data such as resampling.",
            "spof": false
          },
          {
            "path": "src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py",
            "description": "This file defines the `ASTConfig` class, which is used to store and manage the configuration parameters for the Audio Spectrogram Transformer (AST) model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/audio_spectrogram_transformer/__init__.py",
            "description": "This file serves as the `__init__.py` for the `audio_spectrogram_transformer` model, lazily importing its configuration, feature extraction, and modeling components to optimize startup performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "description": "This file implements the PyTorch Audio Spectrogram Transformer (AST) model, including its core components like embeddings, attention mechanisms, and intermediate layers.",
            "spof": false
          },
          {
            "path": "src/transformers/models/audio_spectrogram_transformer/convert_audio_spectrogram_transformer_original_to_pytorch.py",
            "description": "This script converts pre-trained Audio Spectrogram Transformer (AST) model checkpoints from their original repository format to be compatible with the Hugging Face Transformers library. It renames and adjusts weights, then verifies the converted model's output on dummy inputs.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Pavel Iakubovskii",
            "percent": 21
          },
          {
            "name": "Matthew Hernandez",
            "percent": 18
          },
          {
            "name": "Cyril Vallez",
            "percent": 8
          }
        ]
      },
      "Automatic Speech Recognition": {
        "files": [
          {
            "path": "examples/pytorch/speech-recognition/README.md",
            "description": "This README provides detailed instructions and examples for fine-tuning Automatic Speech Recognition (ASR) models using PyTorch within the Hugging Face Transformers library, covering CTC and Sequence-to-Sequence approaches with various datasets and training configurations.",
            "spof": false
          },
          {
            "path": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "description": "This script is designed for fine-tuning a Hugging Face Transformers CTC (Connectionist Temporal Classification) model for automatic speech recognition tasks. It handles model configuration, data preparation, and training arguments for the fine-tuning process.",
            "spof": false
          },
          {
            "path": "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
            "description": "This script is designed for fine-tuning Hugging Face Transformers' sequence-to-sequence models for speech recognition and speech translation tasks. It supports various models like Whisper, Wav2Vec2, and HuBERT, handling data loading, preprocessing, and training.",
            "spof": false
          },
          {
            "path": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "description": "This script is designed for fine-tuning a Hugging Face Transformers CTC (Connectionist Temporal Classification) adapter model specifically for automatic speech recognition tasks. It handles model configuration, data loading, preprocessing, and training arguments.",
            "spof": false
          },
          {
            "path": "examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py",
            "description": "This script pre-trains a Wav2Vec2 model on unlabeled audio data using PyTorch and the Hugging Face `accelerate` library, without relying on the `Trainer` class. It includes arguments for dataset configuration, model parameters, and training specifics.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/asr.md",
            "description": "This document provides a guide on performing Automatic Speech Recognition (ASR) using Hugging Face Transformers. It details the process of fine-tuning a Wav2Vec2 model on the MInDS-14 dataset, covering data preprocessing, evaluation setup, and training procedures.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glmasr.md",
            "description": "This file provides documentation for the GlmAsr model within the Hugging Face Transformers library, including an overview, capabilities, usage examples for speech recognition, and API references for its configurations, processor, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/hubert.md",
            "description": "This document provides an overview and usage guide for the HuBERT model in Hugging Face Transformers, including examples for automatic speech recognition and quantization.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/kyutai_speech_to_text.md",
            "description": "This file provides documentation for the Kyutai Speech-To-Text model within the Hugging Face Transformers library. It includes an overview, usage examples for inference, and references to its API components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/lasr.md",
            "description": "This file is a documentation page for the LASR (Large Audio Speech Recognition) model within the Hugging Face Transformers library. It provides an overview, usage examples, and API references for its components like the tokenizer, feature extractor, processor, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/granite_speech.md",
            "description": "This documentation file provides an overview of the Granite Speech model, detailing its architecture, usage tips, and practical code examples for speech transcription and chat-based interactions. It also includes autodoc references for related configuration, processor, feature extractor, and model classes within the Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/mms.md",
            "description": "This document describes the Hugging Face Transformers implementation of the MMS (Massively Multilingual Speech) project, covering its Automatic Speech Recognition (ASR) and Speech Synthesis (TTS) models with usage examples.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/moonshine_streaming.md",
            "description": "This documentation file introduces the Moonshine Streaming speech recognition model, detailing its architecture, features, and use cases for real-time audio transcription. It includes code examples for using the model with Hugging Face Transformers and references for its various components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/moonshine.md",
            "description": "This file provides documentation for the Moonshine speech recognition model, detailing its architecture, features, and usage examples within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/parakeet.md",
            "description": "This file provides documentation for the Parakeet Automatic Speech Recognition (ASR) model within the Hugging Face Transformers library, detailing its architecture, usage examples, and specific components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/speech-encoder-decoder.md",
            "description": "This file provides documentation for the `SpeechEncoderDecoderModel` in the Hugging Face Transformers library, detailing its purpose for speech-to-text tasks, initialization from various configurations, inference methods, and training procedures.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/sew.md",
            "description": "This document provides an overview, usage tips, and API documentation for the SEW (Squeezed and Efficient Wav2Vec) model within the Hugging Face Transformers library. It details the model's architecture, performance, and associated classes like SEWConfig, SEWModel, SEWForCTC, and SEWForSequenceClassification.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/sew-d.md",
            "description": "This file provides documentation for the SEW-D (Squeezed and Efficient Wav2Vec with Disentangled attention) model within the Hugging Face Transformers library. It includes an overview, usage tips, and API references for its related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/unispeech.md",
            "description": "This file is a documentation page for the UniSpeech model in the Hugging Face Transformers library, providing an overview, usage guidelines, and API references for its various implementations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/speech_to_text.md",
            "description": "This file is a documentation page for the Hugging Face Transformers Speech2Text model. It provides an overview of the model, explains its capabilities for ASR and speech translation, and includes code examples for inference using the model and its associated processing classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/unispeech-sat.md",
            "description": "This file provides comprehensive documentation for the UniSpeech-SAT model within the Hugging Face Transformers library, including its overview, usage tips, and API references. It serves as a model card detailing the model's architecture, capabilities, and how to use it for various speech-related tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/wav2vec2.md",
            "description": "This file provides comprehensive documentation for the Wav2Vec2 model within the Hugging Face Transformers library, including its overview, usage tips, integration with Flash Attention 2, related resources, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/wav2vec2_phoneme.md",
            "description": "This file provides documentation for the Wav2Vec2Phoneme model, a variant of Wav2Vec2 specialized for zero-shot cross-lingual phoneme recognition. It details the model's overview, usage tips, and its associated CTCTokenizer.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/xls_r.md",
            "description": "This file provides documentation for the XLS-R model, including an overview of its research paper, key features, and usage tips for implementing it with Hugging Face Transformers.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/whisper.md",
            "description": "This file provides documentation for the Whisper model within the Hugging Face Transformers library, including an overview, usage examples for speech-to-text transcription, and API references for its various components like `WhisperConfig`, `WhisperTokenizer`, and `WhisperForConditionalGeneration`.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/wav2vec2-conformer.md",
            "description": "This file provides documentation for the Wav2Vec2-Conformer model in the Hugging Face Transformers library. It details the model's overview, usage tips, and lists its various classes and functionalities for tasks like automatic speech recognition and audio classification.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/wavlm.md",
            "description": "This file provides documentation for the WavLM model within the Hugging Face Transformers library, detailing its overview, usage tips, and API references for various WavLM-related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/wav2vec2-bert.md",
            "description": "This file provides documentation for the Wav2Vec2-BERT model within the Hugging Face Transformers library. It details the model's origin, pre-training, architectural features, and usage for various downstream tasks like ASR and audio classification.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/xlsr_wav2vec2.md",
            "description": "This document provides a detailed description and usage guide for the XLSR-Wav2Vec2 model within the Hugging Face Transformers library, including its origins, architecture, and application tips.",
            "spof": false
          },
          {
            "path": "docs/source/es/tasks/asr.md",
            "description": "This file is a Spanish-language guide detailing how to fine-tune a Wav2Vec2 model for Automatic Speech Recognition (ASR) using the Hugging Face Transformers library and the MInDS-14 dataset. It covers data preprocessing, model training setup, and evaluation with WER.",
            "spof": true
          },
          {
            "path": "docs/source/zh/tasks/asr.md",
            "description": "This document is a Chinese-language guide demonstrating how to fine-tune a Wav2Vec2 model for Automatic Speech Recognition (ASR) using the Hugging Face Transformers library, and subsequently perform inference with the trained model.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/whisper.md",
            "description": "This file provides the Korean documentation for the Hugging Face Transformers library's implementation of the Whisper model, including an overview, usage tips, and API references for its configuration, tokenizers, feature extractor, processor, and various model classes.",
            "spof": false
          },
          {
            "path": "docs/source/ko/tasks/asr.md",
            "description": "This file is a Korean-language tutorial that guides users through fine-tuning a Wav2Vec2 model for Automatic Speech Recognition (ASR) using the Hugging Face Transformers and Datasets libraries. It demonstrates the process with the MInDS-14 dataset, covering preprocessing, model training, and evaluation.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "description": "This file contains unit tests for the Automatic Speech Recognition (ASR) pipeline in the Transformers library, covering various models, functionalities, and edge cases like different data types and timestamp handling.",
            "spof": false
          },
          {
            "path": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch Data2VecAudio model within the Hugging Face Transformers library. It includes tests for the base model, its adapters, and different task-specific heads like CTC and sequence classification.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/automatic_speech_recognition.py",
            "description": "This file defines the Automatic Speech Recognition (ASR) pipeline for the Hugging Face Transformers library, enabling transcription of audio into text. It handles audio processing, chunking, and uses various models and tokenizers for speech-to-text conversion.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/configuration_data2vec_audio.py",
            "description": "This file defines the `Data2VecAudioConfig` class, which is used to store and manage the configuration parameters for the Data2VecAudio model architecture within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "description": "This file implements the PyTorch Data2Vec Audio model and its various task-specific heads, such as for CTC and sequence classification, largely adapting components from the Wav2Vec2 architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts an original Fairseq PyTorch checkpoint of the Data2Vec Audio model into a Hugging Face Transformers PyTorch model. It maps the weights and validates the conversion by comparing outputs from both models.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Joao Gante",
            "percent": 12
          },
          {
            "name": "Cyril Vallez",
            "percent": 10
          },
          {
            "name": "eustlb",
            "percent": 10
          }
        ]
      },
      "Text-to-Speech Synthesis": {
        "files": [
          {
            "path": "docs/source/en/tasks/text-to-speech.md",
            "description": "This document provides a guide on using and fine-tuning Text-to-Speech (TTS) models from the Hugging Face Transformers library, with code examples for inference and a detailed walkthrough for fine-tuning SpeechT5 on the VoxPopuli dataset.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/clvp.md",
            "description": "This document provides an overview, usage tips, and API references for the CLVP (Contrastive Language-Voice Pretrained Transformer) model, which is an integral part of the Tortoise TTS system within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bark.md",
            "description": "This file provides documentation for the Bark text-to-speech model within the Hugging Face Transformers library, detailing its architecture, usage, and optimization techniques. It serves as a guide for users to understand and implement Bark for multilingual speech generation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/csm.md",
            "description": "This file documents the Csm (Conversational Speech Model) within the Hugging Face Transformers library. It provides an overview of the model's architecture and offers detailed usage examples for speech generation, including contextual and batched inference, performance optimization, and mentions training capabilities.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/dia.md",
            "description": "This file provides comprehensive documentation for the Dia text-to-speech model, including an overview of its architecture, usage examples for generation and training, and API references for its various components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/fastspeech2_conformer.md",
            "description": "This file provides documentation for the FastSpeech2Conformer model within the Hugging Face Transformers library. It includes an overview, model architecture details, usage examples for inference, and references to its configurations, tokenizer, and associated models.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/speecht5.md",
            "description": "This file provides comprehensive documentation for the SpeechT5 model within the Hugging Face Transformers library, detailing its overview, architecture, and usage instructions for various components like configuration, tokenizers, feature extractors, and different model applications.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/univnet.md",
            "description": "This file provides documentation for the UnivNet model within the Hugging Face Transformers library, detailing its overview, usage, and specific class references like `UnivNetConfig`, `UnivNetFeatureExtractor`, and `UnivNetModel`.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vits.md",
            "description": "This documentation file describes the VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model, including its features, usage examples with the Hugging Face Transformers library, and notes on specific considerations like reproducibility and handling non-Roman alphabets.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/text-to-speech.md",
            "description": "This document provides a Japanese-language guide on performing Text-to-Speech (TTS) tasks using Hugging Face Transformers, covering both inference with pre-trained models and fine-tuning SpeechT5 with a focus on data preprocessing and speaker embeddings.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bark.md",
            "description": "This file provides Japanese documentation for the Bark text-to-speech model, including an overview of its components, optimization techniques, usage examples, and API references.",
            "spof": true
          },
          {
            "path": "tests/pipelines/test_pipelines_text_to_audio.py",
            "description": "This file contains unit tests for the Text-to-Audio pipelines in the Hugging Face Transformers library, covering various models like SpeechT5, MusicGen, SeamlessM4T, Bark, and VITS.",
            "spof": false
          },
          {
            "path": "tests/models/bark/test_processing_bark.py",
            "description": "This file contains unit tests for the `BarkProcessor` class, verifying its functionality including saving/loading pretrained processors, handling speaker embeddings, and tokenization within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/bark/test_modeling_bark.py",
            "description": "This file contains a testing suite for the PyTorch Bark model within the Hugging Face Transformers library. It includes various test classes and functions to verify the functionality of Bark's semantic, coarse, and fine models.",
            "spof": false
          },
          {
            "path": "tests/models/csm/test_modeling_csm.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch ConversationalSpeechModel (CSM) within the Hugging Face Transformers library. It includes tests for the model's configuration, conditional generation capabilities, and various utility functions.",
            "spof": true
          },
          {
            "path": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "description": "This file contains a testing suite for the PyTorch FastSpeech2Conformer model, including a model tester, configuration tests, and various functional tests for the model's outputs and behaviors.",
            "spof": false
          },
          {
            "path": "tests/models/fastspeech2_conformer/test_tokenization_fastspeech2_conformer.py",
            "description": "This file contains unit tests for the FastSpeech2Conformer tokenizer, verifying its functionality including token conversion, vocabulary handling, and encoding/decoding processes.",
            "spof": true
          },
          {
            "path": "tests/fixtures/parakeet",
            "description": "This directory is designated to hold test fixtures for the 'Parakeet' component or related functionalities within the `transformers` library's testing suite. Although currently empty, it serves as a placeholder for any future test-specific data or configurations.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/text_to_audio.py",
            "description": "This file defines the `TextToAudioPipeline` class, which facilitates text-to-audio generation using various models. It handles the complete workflow from text input to audio waveform output, including preprocessing and vocoder integration.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bark/configuration_bark.py",
            "description": "This file defines the configuration classes for the Bark model and its various sub-models (semantic, coarse acoustics, fine acoustics) within the Hugging Face Transformers library. It specifies the architecture parameters for these components.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bark/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `bark` model within the transformers library, defining its public API. It uses lazy loading to import the model's configuration, modeling, and processing components only when they are accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bark/modeling_bark.py",
            "description": "This file implements the PyTorch self-attention mechanism for the BARK model, including a standard attention layer (`BarkSelfAttention`) and an optimized Flash Attention version (`BarkSelfFlashAttention2`). It defines how attention is computed and integrated into the BARK model architecture within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bark/processing_bark.py",
            "description": "This file defines the `BarkProcessor` class, which handles the preprocessing and postprocessing of data for the Bark text-to-speech model. It integrates a tokenizer and manages speaker-specific embeddings for generating speech.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bark/convert_suno_to_hf.py",
            "description": "This script converts original Suno Bark model checkpoints (semantic, coarse, or fine acoustic) into the Hugging Face Transformers format, allowing them to be used within the Hugging Face ecosystem.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dia/processing_dia.py",
            "description": "This file defines the `DiaProcessor` class, which is responsible for preparing text and audio inputs for the Dia model. It handles processing for both text-to-speech generation and training, including audio encoding, padding, and attention mask generation.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
            "description": "This file defines the configuration class for the FastSpeech2Conformer model, inheriting from `PreTrainedConfig`. It specifies various architectural parameters and hyperparameters for the FastSpeech2Conformer model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fastspeech2_conformer/tokenization_fastspeech2_conformer.py",
            "description": "This file defines the `FastSpeech2ConformerTokenizer` class, which handles tokenization for the FastSpeech2Conformer model by performing text preprocessing, phonemization using `g2p_en`, and conversion between phonemes and their corresponding IDs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "description": "This file implements the core PyTorch modules for the FastSpeech2Conformer model, including the length regulator, duration predictor, and speech decoder postnet. It defines model output classes for both the base model and a version integrated with HiFi-GAN.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fastspeech2_conformer/convert_model_with_hifigan.py",
            "description": "This script converts an original FastSpeech2Conformer checkpoint, including its HiFi-GAN vocoder, into a Hugging Face Transformers compatible format, allowing it to be saved locally or pushed to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/fastspeech2_conformer/convert_hifigan.py",
            "description": "This script converts an original HiFi-GAN checkpoint, specifically designed for FastSpeech2Conformer, into a format compatible with Hugging Face Transformers. It loads weights, remaps configuration parameters from a YAML file, and saves the converted model.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fastspeech2_conformer/__init__.py",
            "description": "This `__init__.py` file serves as the package initializer for the `fastspeech2_conformer` model within the Hugging Face Transformers library. It sets up lazy imports for the model's configuration, modeling, and tokenization components to optimize loading performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/fastspeech2_conformer/convert_fastspeech2_conformer_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts an original PyTorch checkpoint of a FastSpeech2Conformer model, along with its configuration and tokenizer vocabulary, into a format compatible with the Hugging Face Transformers library.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Cyril Vallez",
            "percent": 16
          },
          {
            "name": "eustlb",
            "percent": 13
          },
          {
            "name": "Jaeyong Sung",
            "percent": 9
          }
        ]
      },
      "Neural Audio Codec Models": {
        "files": [
          {
            "path": "docs/source/en/model_doc/dac.md",
            "description": "This document provides an overview and usage guide for the DAC (Descript Audio Codec) model within the Hugging Face Transformers library. It details its purpose for high-fidelity audio compression, its architecture, and practical examples for encoding and decoding audio.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/encodec.md",
            "description": "This file provides the documentation for the EnCodec neural audio codec model within the Hugging Face Transformers library, including an overview, usage examples, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mimi.md",
            "description": "This file provides documentation for the Mimi neural audio codec model within the Hugging Face Transformers library, detailing its features, usage, and API reference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vibevoice_acoustic_tokenizer.md",
            "description": "This file provides documentation for the VibeVoice Acoustic Tokenizer model, detailing its overview, architecture, and offering code examples for audio encoding, decoding, and streaming using the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/xcodec.md",
            "description": "This file documents the X-Codec model, an audio codec that integrates semantic information for improved audio language models. It provides an overview, usage examples, and API references for the model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/dac/test_feature_extraction_dac.py",
            "description": "This file contains unit tests for the `DacFeatureExtractor` class, ensuring its feature extraction, padding, truncation, and integration functionalities work correctly within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/encodec/test_feature_extraction_encodec.py",
            "description": "This file contains unit tests for the `EncodecFeatureExtractor` class, ensuring its proper functionality for feature extraction from audio data, including handling of padding, truncation, and different audio formats.",
            "spof": false
          },
          {
            "path": "tests/models/encodec/test_modeling_encodec.py",
            "description": "This file contains unit tests for the PyTorch Encodec model within the Hugging Face Transformers library, ensuring its functionality, configuration, and various behaviors are correct.",
            "spof": false
          },
          {
            "path": "tests/fixtures/audioflamingo3",
            "description": "This directory is intended to house test fixtures specifically for components related to the 'audioflamingo3' model within the Transformers library's test suite. Its purpose is to provide stable, pre-defined data or configurations required for testing the functionality of the audioflamingo3 model.",
            "spof": false
          },
          {
            "path": "tests/fixtures/vibevoice",
            "description": "This directory is intended to contain test fixtures specifically for the 'vibevoice' component or functionality within the Transformers library's testing suite. As it is currently empty, it serves as a placeholder for future test data or configurations related to vibevoice.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dac/__init__.py",
            "description": "Initializes the `dac` model package, setting up lazy imports for its configuration, feature extraction, and modeling components.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dac/feature_extraction_dac.py",
            "description": "This file defines the `DacFeatureExtractor` class, which is responsible for preprocessing audio data for the DAC model, including handling sampling rates, padding, truncation, and conversion to appropriate tensor formats.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dac/configuration_dac.py",
            "description": "This file defines the `DacConfig` class, which is used to store and manage the configuration parameters for the DAC (Descript Audio Codec) model, including its architecture, codebook details, and training loss weights.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dac/modeling_dac.py",
            "description": "This file defines the core architectural components, such as vector quantization, residual units, and encoder blocks, as well as the output classes for the DAC (Discrete Audio Codec) model within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dac/convert_dac_checkpoint.py",
            "description": "This script converts pre-trained Descript Audio Codec (DAC) model checkpoints into the Hugging Face Transformers format. It handles mapping original weights, applying and removing weight normalization, and saving the converted model and feature extractor.",
            "spof": false
          },
          {
            "path": "src/transformers/models/encodec/configuration_encodec.py",
            "description": "This file defines the `EncodecConfig` class, which is used to store and manage the configuration parameters for the EnCodec audio compression model. It sets various architectural and audio processing parameters for instantiating an EnCodec model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/encodec/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `encodec` model sub-package, defining its import structure and enabling lazy loading of its configuration, feature extraction, and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/encodec/modeling_encodec.py",
            "description": "This file defines the core PyTorch model architecture components, such as convolutional layers, LSTM, and residual blocks, for the EnCodec model within the Hugging Face Transformers library. It also includes data structures for model inputs and outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/encodec/feature_extraction_encodec.py",
            "description": "This file defines the `EncodecFeatureExtractor` class, which prepares raw audio for the EnCodec model by handling feature extraction, sampling rate validation, padding, truncation, and optional audio chunking.",
            "spof": false
          },
          {
            "path": "src/transformers/models/encodec/convert_encodec_checkpoint_to_pytorch.py",
            "description": "This script converts pre-trained EnCodec model checkpoints, typically from the original EnCodec implementation, into a format compatible with the Hugging Face Transformers library. It maps the original state dictionary keys to the corresponding keys in the Hugging Face EnCodec model structure.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Eric Bezzam",
            "percent": 37
          },
          {
            "name": "Yih-Dar",
            "percent": 22
          },
          {
            "name": "Cyril Vallez",
            "percent": 11
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 124,
      "spofCount": 38
    },
    "busFactor": 12,
    "authorCount": 54
  },
  "Computer Vision Model Library": {
    "description": "A powerful set of models for visual tasks such as image classification, object detection, and semantic segmentation, enabling advanced visual understanding capabilities.",
    "functions": {
      "Image and Video Data Processing": {
        "files": [
          {
            "path": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "description": "This file defines the `ImgprocModelImageProcessor` class, which handles various image preprocessing steps like resizing, rescaling, normalization, and color conversion. It is an automatically generated file from a modular source.",
            "spof": false
          },
          {
            "path": "examples/modular-transformers/modular_new_imgproc_model.py",
            "description": "This file defines a custom image processor, `ImgprocModelImageProcessor`, that extends the `BlipImageProcessor` from the Transformers library. It adds a `new_image_processing_method` which demonstrates a simple pixel value manipulation by dividing them by two.",
            "spof": true
          },
          {
            "path": "docs/source/en/video_processors.md",
            "description": "This document explains the concept and usage of `VideoProcessor` in the Hugging Face Transformers library, detailing its role in preparing and post-processing video data for models. It highlights the benefits of `AutoVideoProcessor` and its fast, GPU-accelerated capabilities for efficient video processing.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/video_processor.md",
            "description": "This document describes the `Video Processor` in the Transformers library, a utility designed to prepare input features for video models and handle post-processing of their outputs. It covers functionalities like video decoding, frame sampling, transformations, and introduces \"fast video processors\" for efficient, GPU-accelerated processing.",
            "spof": true
          },
          {
            "path": "docs/source/en/internal/image_processing_utils.md",
            "description": "This document lists and describes utility functions and functional transformations used by image processors within the library, primarily intended for developers studying the image processing codebase.",
            "spof": false
          },
          {
            "path": "docs/source/zh/internal/image_processing_utils.md",
            "description": "This Chinese documentation page lists utility functions and the ImageProcessingMixin used by image processors in the Hugging Face Transformers library, primarily for developers studying the image processor code.",
            "spof": true
          },
          {
            "path": "docs/source/ja/internal/image_processing_utils.md",
            "description": "This file documents utility functions and image transformations used internally by the image processors in the Transformers library, primarily for developers studying the library's code.",
            "spof": true
          },
          {
            "path": "tests/test_image_processing_common.py",
            "description": "This file provides common utilities and a mixin class for testing image processors within the Hugging Face Transformers library. It includes methods to prepare dummy image/video data and compare the functionality and performance of 'slow' and 'fast' image processing implementations.",
            "spof": false
          },
          {
            "path": "tests/test_image_transforms.py",
            "description": "This file contains unit tests for various image transformation utility functions provided by the `transformers` library, such as resizing, cropping, channel reordering, and type conversions to and from PIL Image objects.",
            "spof": false
          },
          {
            "path": "tests/test_video_processing_common.py",
            "description": "This file provides common utility functions for preparing video data and a `VideoProcessingTestMixin` class to facilitate comprehensive testing of video processing functionalities, including serialization, deserialization, and runtime behavior, within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/utils/test_image_processing_utils.py",
            "description": "This file contains unit tests for various utility functions related to image processing within the Hugging Face Transformers library, including caching behavior, loading from subfolders, and pushing image processors to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "tests/utils/test_video_utils.py",
            "description": "This file contains unit tests for the video utility functions and the `BaseVideoProcessor` within the Hugging Face Transformers library. It verifies functionalities such as video batching, resizing, normalization, cropping, color conversion, and grouping of video data.",
            "spof": true
          },
          {
            "path": "tests/models/auto/test_video_processing_auto.py",
            "description": "This file contains unit tests for the `AutoVideoProcessor` class in the Hugging Face Transformers library. It verifies the functionality of loading video processors from pretrained models, local files, handling errors, dynamic module loading, and custom video processor registration.",
            "spof": true
          },
          {
            "path": "tests/models/auto/test_image_processing_auto.py",
            "description": "This file contains unit tests for the `AutoImageProcessor` class in the Hugging Face Transformers library. It verifies the correct loading and instantiation of image processors from various sources, including model shortcuts, local directories, and dynamic modules, ensuring robust auto-detection and error handling.",
            "spof": false
          },
          {
            "path": "tests/models/beit/test_image_processing_beit.py",
            "description": "This file contains unit tests for the `BeitImageProcessor` and `BeitImageProcessorFast` classes in the Hugging Face Transformers library. It verifies their image processing functionalities, including resizing, cropping, normalization, and handling of segmentation maps.",
            "spof": true
          },
          {
            "path": "tests/models/bit/test_image_processing_bit.py",
            "description": "This file contains unit tests for the `BitImageProcessor` and `BitImageProcessorFast` classes, verifying their initialization, properties, and image processing functionalities within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/conditional_detr/test_image_processing_conditional_detr.py",
            "description": "This file contains unit tests for the `ConditionalDetrImageProcessor` and `ConditionalDetrImageProcessorFast` classes in the Hugging Face Transformers library. It verifies their functionality for image preprocessing, including resizing, normalization, and handling of COCO detection and panoptic annotations.",
            "spof": false
          },
          {
            "path": "tests/models/depth_pro/test_image_processing_depth_pro.py",
            "description": "This file contains unit tests for the DepthProImageProcessor and DepthProImageProcessorFast classes, ensuring their image processing functionalities work as expected.",
            "spof": true
          },
          {
            "path": "tests/models/convnext/test_image_processing_convnext.py",
            "description": "This file contains unit tests for the ConvNext image processing classes (`ConvNextImageProcessor` and `ConvNextImageProcessorFast`), verifying their properties and configuration from dictionaries.",
            "spof": false
          },
          {
            "path": "tests/models/dinov3_vit/test_image_processing_dinov3_vit_fast.py",
            "description": "This file contains unit tests for the `DINOv3ViTImageProcessorFast` class, verifying its initialization, properties, and image processing functionalities within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/deformable_detr/test_image_processing_deformable_detr.py",
            "description": "This file contains unit tests for the `DeformableDetrImageProcessor` and `DeformableDetrImageProcessorFast` classes. It verifies their image processing functionalities, including resizing, normalization, padding, and handling of COCO detection and panoptic annotations.",
            "spof": false
          },
          {
            "path": "tests/models/deit/test_image_processing_deit.py",
            "description": "This file contains unit tests for the `DeiTImageProcessor` and `DeiTImageProcessorFast` classes within the Hugging Face Transformers library, ensuring their properties and `from_dict` initialization methods work as expected.",
            "spof": false
          },
          {
            "path": "tests/models/detr/test_image_processing_detr.py",
            "description": "This file contains unit tests for the `DetrImageProcessor` and `DetrImageProcessorFast` classes within the Hugging Face Transformers library. It verifies their image processing functionalities, including resizing, normalization, and handling of COCO detection annotations.",
            "spof": false
          },
          {
            "path": "tests/models/dpt/test_image_processing_dpt.py",
            "description": "This file contains unit tests for the `DPTImageProcessor` and `DPTImageProcessorFast` classes within the Hugging Face Transformers library. It verifies various functionalities such as initialization, image preprocessing (resizing, normalization, padding), and handling of segmentation maps for DPT models.",
            "spof": true
          },
          {
            "path": "tests/models/donut/test_image_processing_donut.py",
            "description": "This file contains unit tests for the `DonutImageProcessor` and `DonutImageProcessorFast` classes, ensuring their image processing functionalities work as expected across different input types and configurations within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
            "description": "This file contains unit tests for the `EfficientLoFTRImageProcessor` and `EfficientLoFTRImageProcessorFast` classes, verifying their image preprocessing functionality and input handling within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/eomt/test_image_processing_eomt.py",
            "description": "This file contains a testing suite for the PyTorch EoMT Image Processor, including `EomtImageProcessingTester` and `EomtImageProcessingTest` classes. It verifies the functionality and slow/fast equivalence of the `EomtImageProcessor` for various input types.",
            "spof": true
          },
          {
            "path": "tests/models/efficientnet/test_image_processing_efficientnet.py",
            "description": "This file contains unit tests for the `EfficientNetImageProcessor` and `EfficientNetImageProcessorFast` classes. It verifies the correct implementation of image processing operations such as resizing, normalization, and rescaling for EfficientNet models.",
            "spof": true
          },
          {
            "path": "tests/fixtures/tests_samples/COCO/coco_panoptic",
            "description": "This directory is designated to store sample data for tests specifically related to COCO Panoptic Segmentation within the Transformers library's test suite. Although currently empty, its purpose is to serve as a fixture location for such test samples, which might be dynamically populated during test execution.",
            "spof": false
          },
          {
            "path": "src/transformers/image_processing_utils.py",
            "description": "This file defines the `BaseImageProcessor` class, which serves as a foundational component for image processing in the Transformers library. It provides common image manipulation methods such as rescaling, normalization, and center-cropping, along with utility functions for handling image dimensions.",
            "spof": false
          },
          {
            "path": "src/transformers/image_processing_base.py",
            "description": "This file defines the base `ImageProcessingMixin` class, providing common functionalities like loading, saving, and configuration management for image processors. It also defines `BatchFeature` for handling the output of image processing methods.",
            "spof": false
          },
          {
            "path": "src/transformers/image_processing_utils_fast.py",
            "description": "This file provides a base class (`BaseImageProcessorFast`) and utility functions for fast, optimized image preprocessing operations (like resize, crop, rescale, normalize) using PyTorch and TorchVision, with a focus on GPU support and batch processing.",
            "spof": false
          },
          {
            "path": "src/transformers/image_transforms.py",
            "description": "This file provides utility functions for common image transformations and manipulations, including channel dimension handling, rescaling, conversion to PIL format, and calculating image resize dimensions.",
            "spof": false
          },
          {
            "path": "src/transformers/image_utils.py",
            "description": "This file provides utility functions for handling images in various formats (PIL, NumPy, PyTorch), including type checking, format conversions, batching, and channel dimension inference, primarily for use within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/video_utils.py",
            "description": "This file provides utility functions for handling video data in various formats, including validation, conversion, batching, and metadata management. It supports different backends like NumPy, PyTorch, PIL, and popular video libraries for processing video inputs within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/video_processing_utils.py",
            "description": "This file defines utility functions and the `BaseVideoProcessor` class for preprocessing videos. It handles operations such as resizing, cropping, normalization, and frame sampling, preparing videos for use with deep learning models within the HuggingFace `transformers` library.",
            "spof": true
          },
          {
            "path": "src/transformers/utils/constants.py",
            "description": "This file defines common normalization constants (mean and standard deviation) for image processing, specifically for datasets like ImageNet and models like OpenAI CLIP.",
            "spof": true
          },
          {
            "path": "src/transformers/models/auto/video_processing_auto.py",
            "description": "This file implements the AutoVideoProcessor class, which provides a mechanism to automatically infer and load video processor configurations and classes based on a given model type or pretrained model identifier within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/auto/image_processing_auto.py",
            "description": "This file implements the `AutoImageProcessor` class, which automatically selects and provides the appropriate image processing class (e.g., `BlipImageProcessor`, `CLIPImageProcessorFast`) based on a given model's configuration. It maps various model architectures to their corresponding slow and fast image processor implementations.",
            "spof": false
          },
          {
            "path": "src/transformers/models/beit/image_processing_beit_fast.py",
            "description": "This file implements a fast image processor for the BEiT model, handling image preprocessing steps like resizing, cropping, normalization, and label reduction. It also provides functionality for post-processing semantic segmentation outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/beit/image_processing_beit.py",
            "description": "This file defines the `BeitImageProcessor` class, which handles the pre-processing of images for the BEiT (Bidirectional Encoder representation from Image Transformers) model, including resizing, cropping, rescaling, normalization, and label reduction.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bit/image_processing_bit_fast.py",
            "description": "This file defines the `BitImageProcessorFast` class, a fast image processor specifically designed for the BiT (Big Transfer) model, inheriting from `BaseImageProcessorFast` and configuring various image processing steps and parameters.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bit/image_processing_bit.py",
            "description": "This file implements the `BitImageProcessor` class, which handles all image preprocessing operations for the BiT model, including resizing, cropping, rescaling, and normalizing images.",
            "spof": false
          },
          {
            "path": "src/transformers/models/chinese_clip/image_processing_chinese_clip_fast.py",
            "description": "This file defines the `ChineseCLIPImageProcessorFast` class, which is a fast image processor for the Chinese-CLIP model. It configures the specific image pre-processing parameters like resizing, cropping, normalization, and color conversion for use with Chinese-CLIP.",
            "spof": true
          },
          {
            "path": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "description": "This file contains fast image processing utilities and annotation preparation functions specifically designed for the Conditional DETR model. It handles tasks such as converting COCO polygon annotations to masks, preparing COCO detection and panoptic annotations, and related image transformations.",
            "spof": false
          },
          {
            "path": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "description": "This file defines the image processing class and associated utility functions for the Conditional DETR model. It includes functionalities like resizing, padding, normalization, and converting COCO annotations for object detection tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convnext/image_processing_convnext.py",
            "description": "This file implements the `ConvNextImageProcessor` class, which handles the preprocessing of images for the ConvNeXT model. It includes functionalities for resizing, cropping, rescaling, and normalizing images to prepare them for model inference.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "description": "This file defines the `ConvNextImageProcessorFast` class, an optimized image processor for ConvNeXT models. It implements fast image preprocessing steps such as resizing, cropping, rescaling, and normalization using PyTorch and torchvision utilities.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "description": "This file implements the image processor for the Deformable DETR model, handling image transformations like resizing, padding, and annotation processing (e.g., bounding boxes and segmentation masks) for model input preparation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "description": "This file implements a fast image processor for the Deformable DETR model, handling pre-processing of images and converting COCO detection and panoptic annotations into the format expected by the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deit/image_processing_deit_fast.py",
            "description": "This file defines the `DeiTImageProcessorFast` class, which is a fast image processor specifically designed for the DeiT (Data-efficient Image Transformers) model. It inherits from `BaseImageProcessorFast` and sets up parameters for image transformations such as resizing, center cropping, normalization, and scaling.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deit/image_processing_deit.py",
            "description": "This file defines the `DeiTImageProcessor` class, responsible for handling image preprocessing steps such as resizing, cropping, rescaling, and normalizing images for the DeiT model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/detr/image_processing_detr_fast.py",
            "description": "This file implements a fast image processor for the DETR model, providing optimized utilities for image manipulation and COCO annotation preparation (detection and panoptic segmentation).",
            "spof": false
          },
          {
            "path": "src/transformers/models/detr/image_processing_detr.py",
            "description": "This file implements the image processor for the DETR model, handling various image transformations, resizing, normalization, and annotation processing (including COCO formats) specific to DETR within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "description": "This file defines the `DepthProImageProcessorFast` class, a fast image processor for the DepthPro model in the Hugging Face Transformers library. It handles preprocessing of input images (rescaling, normalizing, resizing) and post-processing of the model's depth estimation outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/depth_pro/image_processing_depth_pro.py",
            "description": "This file implements the `DepthProImageProcessor` class, which handles image pre-processing steps like resizing, rescaling, and normalization for the DepthPro model. It prepares input images for inference or training with the DepthPro model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py",
            "description": "This file defines the `DINOv3ViTImageProcessorFast` class, a fast image processor for DINOv3 models. It handles image preprocessing steps such as resizing, rescaling, cropping, and normalization in a batched manner, optimized for performance.",
            "spof": false
          },
          {
            "path": "src/transformers/models/donut/image_processing_donut_fast.py",
            "description": "This file implements the `DonutImageProcessorFast` class, providing a fast image processing pipeline for the Donut model, including methods for resizing, padding, aligning, and normalizing images.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/image_processing_dpt.py",
            "description": "This file defines the DPTImageProcessor class, which handles the preprocessing of images for the DPT (Dense Prediction Transformer) model, including resizing, scaling, normalization, and padding.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "description": "This file defines the `DPTImageProcessorFast` class, providing optimized image preprocessing functionalities for the DPT (Depth Prediction Transformer) model, including resizing, normalization, and handling of segmentation maps. It also includes methods for post-processing semantic segmentation outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/modular_dpt.py",
            "description": "This file implements the DPTImageProcessorFast class, a fast image processor for the DPT (Depth Perception Transformer) model, handling image preprocessing like resizing, padding, and normalization, as well as post-processing of depth estimation outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/efficientloftr/modular_efficientloftr.py",
            "description": "This file defines `EfficientLoFTRImageProcessorFast`, a class for post-processing the raw outputs of the EfficientLoFTR model to extract and filter keypoint matches, scores, and descriptors, converting them to absolute image coordinates.",
            "spof": true
          },
          {
            "path": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "description": "This file defines the `EfficientLoFTRImageProcessor` class, which handles all image preprocessing steps for the EfficientLoFTR model in the Hugging Face Transformers library. It includes functionalities like resizing, rescaling, and converting images to grayscale, adapting them for model input.",
            "spof": true
          },
          {
            "path": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "description": "This file provides a fast image processor for the EfficientLoFTR model, handling preprocessing of image pairs for keypoint matching and post-processing of model outputs to extract and visualize keypoints and matches.",
            "spof": true
          },
          {
            "path": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "description": "This file defines the `EfficientNetImageProcessorFast` class, which provides a fast and optimized image processing pipeline for the EfficientNet model, including functionalities like resizing, cropping, rescaling, and normalization of input images.",
            "spof": false
          },
          {
            "path": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "description": "This file defines the `EfficientNetImageProcessor` class, which handles the preprocessing of images for EfficientNet models within the Hugging Face Transformers library. It includes methods for resizing, rescaling, and normalizing images according to EfficientNet's requirements.",
            "spof": false
          },
          {
            "path": "src/transformers/models/eomt/image_processing_eomt.py",
            "description": "This file implements the `EomtImageProcessor` class, which handles image preprocessing steps such as resizing, rescaling, normalization, and padding for the EoMT model. It also includes utility functions for processing segmentation masks, specifically for converting them to binary masks and computing segments from model predictions.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "description": "This file implements a fast image processor for the EoMT model, handling various image preprocessing steps like resizing, scaling, normalization, splitting into patches, padding, and converting segmentation maps to binary masks efficiently.",
            "spof": false
          },
          {
            "path": "src/transformers/models/flava/image_processing_flava.py",
            "description": "This file implements the image processing utilities for the FLAVA model, including defining its specific image processor, masking generation logic, and related parameters for both general image and codebook processing.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Yoni Gozlan",
            "percent": 23
          },
          {
            "name": "Raushan Turganbay",
            "percent": 23
          },
          {
            "name": "Lucain",
            "percent": 10
          }
        ]
      },
      "Technical Documentation and Tutorials": {
        "files": [
          {
            "path": "docs/source/en/backbones.md",
            "description": "This document explains what backbones are in computer vision tasks, how to load and use them with the Hugging Face Transformers library (including `AutoBackbone` and `timm` backbones), and how to extract features from them.",
            "spof": false
          },
          {
            "path": "docs/source/en/image_processors.md",
            "description": "This document explains how image processors in the Hugging Face Transformers library convert images into pixel values for vision models, detailing their types, usage, and the preprocessing steps involved for matching model input requirements.",
            "spof": true
          },
          {
            "path": "docs/source/en/main_classes/image_processor.md",
            "description": "This file provides documentation for the `Image Processor` in the Hugging Face `transformers` library, explaining its role in preparing image features for vision models and detailing the use of both base and 'fast' image processors, including performance benchmarks.",
            "spof": false
          },
          {
            "path": "docs/source/en/main_classes/backbones.md",
            "description": "This documentation file explains what backbones are in the context of the Hugging Face Transformers library, detailing how to use them for feature extraction in computer vision tasks. It covers the `AutoBackbone` class, utility mixins, and lists supported models like BEiT, ResNet, and Swin Transformer.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/image_feature_extraction.md",
            "description": "This documentation file explains how to perform image feature extraction using Hugging Face Transformers, covering both the `image-feature-extraction` pipeline and `AutoModel` for tasks like image similarity and retrieval.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/image_to_image.md",
            "description": "This file is a documentation guide explaining how to perform image-to-image tasks, specifically super-resolution, using the Hugging Face Transformers library. It demonstrates both pipeline usage and direct model/processor interaction.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/image_classification.md",
            "description": "This file provides a guide on how to fine-tune a ViT model for image classification on the Food-101 dataset using the Hugging Face Transformers library, and subsequently use it for inference.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/monocular_depth_estimation.md",
            "description": "This document explains monocular depth estimation, its categories, and provides a guide on performing inference using the Hugging Face Transformers library's pipeline and manual methods.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/video_classification.md",
            "description": "This document provides a guide on performing video classification using the Hugging Face Transformers library, specifically demonstrating how to fine-tune a VideoMAE model on a subset of the UCF101 dataset and use it for inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/keypoint_matching.md",
            "description": "This documentation file provides a tutorial on keypoint matching using the Hugging Face Transformers library. It demonstrates how to use the `EfficientLoFTR` model to extract and visualize keypoint matches between two images, including examples with `AutoModelForKeypointMatching` and the pipeline API.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/zero_shot_image_classification.md",
            "description": "This file provides a guide on zero-shot image classification using the Hugging Face Transformers library. It explains the task and demonstrates how to perform it using both the `pipeline` API and manual model/processor usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/tasks/keypoint_detection.md",
            "description": "This document explains and demonstrates keypoint detection using the Hugging Face Transformers library, specifically with the SuperPoint model. It covers model loading, inference, post-processing, and visualization of keypoints on sample images.",
            "spof": false
          },
          {
            "path": "docs/source/en/tasks/semantic_segmentation.md",
            "description": "This document explains different types of image segmentation (semantic, instance, and panoptic) and provides examples using the Hugging Face Transformers library. It also guides users on how to fine-tune a model for semantic segmentation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/conditional_detr.md",
            "description": "This file provides documentation for the Conditional DETR model within the Hugging Face Transformers library, detailing its overview, architecture, and offering API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/bit.md",
            "description": "This file provides comprehensive documentation for the Big Transfer (BiT) model within the Hugging Face Transformers library. It covers the model's overview, usage tips, and API references for its configuration, image processor, and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/beit.md",
            "description": "This file provides comprehensive documentation for the BEiT (Bidirectional Encoder representation from Image Transformers) model within the Hugging Face Transformers library, covering its overview, usage, and specific functionalities like image processing and scaled dot-product attention.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/convnext.md",
            "description": "This file provides documentation for the ConvNeXT model within the Hugging Face Transformers library, including its overview, architecture, and API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/convnextv2.md",
            "description": "This file provides documentation for the ConvNeXt V2 model, including an overview, its architectural details, and usage examples within the Hugging Face Transformers library. It serves as a comprehensive resource for users interested in understanding and implementing ConvNeXt V2.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/cvt.md",
            "description": "This file provides documentation for the Convolutional Vision Transformer (CvT) model, including its architecture, usage examples for image classification, and references to its specific configuration and model classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/depth_anything_v2.md",
            "description": "This file provides documentation for the Depth Anything V2 model within the Hugging Face Transformers library. It includes an overview of the model, usage examples using the pipeline and direct model API, and links to relevant resources.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/depth_anything.md",
            "description": "This file provides documentation for the Depth Anything model within the Hugging Face Transformers library. It explains the model's architecture, training methodology, and offers code examples for performing monocular depth estimation using the `pipeline` and `AutoModel` classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/dab-detr.md",
            "description": "This file provides documentation for the DAB-DETR model, explaining its overview, functionality, and how to use it with code examples in the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/dinov2_with_registers.md",
            "description": "This documentation file describes the DINOv2 with Registers model, detailing its purpose, the problem it solves (attention map artifacts in ViTs), and its improved performance. It also provides links to the model's configuration and classes within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/dinat.md",
            "description": "This file provides documentation for the Dilated Neighborhood Attention Transformer (DiNAT) model within the Hugging Face Transformers library, detailing its architecture, usage, and available classes like `DinatConfig`, `DinatModel`, and `DinatForImageClassification`.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/dit.md",
            "description": "This file is a documentation page for the DiT (Document Image Transformer) model in the Hugging Face Transformers library. It describes the model, its capabilities for visual document tasks, and provides usage examples for image classification.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/d_fine.md",
            "description": "This file provides documentation for the D-FINE object detection model within the Hugging Face Transformers library. It includes an overview of the model, its architecture, performance metrics, and Python usage examples.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/deformable_detr.md",
            "description": "This file provides documentation for the Deformable DETR model, including an overview, usage examples for object detection with the Hugging Face Transformers library, and API references for its components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/eomt.md",
            "description": "This file provides documentation for the Encoder-only Mask Transformer (EoMT) model, explaining its architecture, capabilities in image segmentation tasks (semantic, instance, panoptic), and offering usage examples with code.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/dpt.md",
            "description": "This file provides documentation for the DPT (Dense Prediction Transformer) model, detailing its architecture, use cases for tasks like depth estimation and semantic segmentation, and usage within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/detr.md",
            "description": "This file provides comprehensive documentation for the DETR (Detection Transformer) model within the Hugging Face Transformers library. It explains the model's architecture, usage with code examples for object detection, and various technical notes regarding its implementation and training.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/eomt_dinov3.md",
            "description": "This file provides documentation for the EoMT-DINOv3 model within the Hugging Face Transformers library. It describes the model's architecture, features, and usage for universal segmentation tasks, including code examples.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/depth_pro.md",
            "description": "This document provides an overview and usage guide for the DepthPro model within the Hugging Face Transformers library. It details its architecture for zero-shot monocular depth estimation, explains how to use it, and covers features like Field-of-View prediction and performance optimization with SDPA.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/edgetam_video.md",
            "description": "This file provides documentation and usage examples for the EdgeTAMVideo model, which is designed for efficient on-device video segmentation and object tracking within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/edgetam.md",
            "description": "This document provides an overview and usage guide for the EdgeTAM model within the Hugging Face Transformers library, detailing its features and offering code examples for various image and video segmentation tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/deit.md",
            "description": "This document provides comprehensive documentation for the DeiT (Data-efficient image Transformers) model within the Hugging Face Transformers library, detailing its overview, usage tips, integration with Scaled Dot Product Attention, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/efficientloftr.md",
            "description": "This file provides documentation for the EfficientLoFTR model in the Hugging Face Transformers library, including its capabilities, usage examples for keypoint matching, and API references for its related classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/efficientnet.md",
            "description": "This file provides comprehensive documentation for the EfficientNet model within the Hugging Face Transformers library. It covers the model's overview, its configuration, image processing functionalities, and various model classes for image classification.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glpn.md",
            "description": "This file provides documentation for the GLPN model within the Hugging Face Transformers library, detailing its purpose for monocular depth estimation, its architecture, and relevant API references. It serves as a guide for understanding and using the GLPN model.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/dinov3.md",
            "description": "This file provides documentation and usage examples for the DINOv3 vision foundation model within the Hugging Face Transformers library. It covers model usage, quantization, and interpretation of its outputs.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/hiera.md",
            "description": "This file provides documentation for the Hiera model within the Hugging Face Transformers library, detailing its architecture, research paper overview, and usage examples for image classification.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/glm_ocr.md",
            "description": "This file is a documentation template or incomplete document for the GlmOcr model within the Hugging Face Transformers library, detailing its overview, usage examples, and API references for its various components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/focalnet.md",
            "description": "This file provides documentation for the FocalNet model within the Hugging Face Transformers library, including an overview of its architecture, performance benchmarks, and API references for its configuration and model implementations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/granitevision.md",
            "description": "This documentation file describes the Granite Vision model, its architecture, and provides usage examples within the Hugging Face Transformers library. It details how to use the model, its processor, and configuration for conditional generation tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/dinov2.md",
            "description": "This file is a documentation page for the DINOv2 vision foundation model within the Hugging Face Transformers library. It provides an overview of DINOv2, its capabilities, and includes code examples for image classification, feature extraction, and quantization.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/imagegpt.md",
            "description": "This file provides documentation for the ImageGPT model within the Hugugging Face Transformers library, detailing its overview, usage tips, and API references for its various components like configuration, image processor, and different model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/lightglue.md",
            "description": "This file is a documentation page for the LightGlue model within the Hugging Face Transformers library. It describes the model's functionality for matching local features across images, provides usage examples with the `pipeline` and `AutoModel` classes, and details related configurations and processors.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/hgnet_v2.md",
            "description": "This file provides documentation for the HGNet-V2 convolutional neural network model within the Hugging Face Transformers library. It describes the model's features and offers code examples for image classification using the `pipeline` and `AutoModel` classes, along with API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/got_ocr2.md",
            "description": "This documentation page provides an overview and usage examples for the GOT-OCR2 model, a unified end-to-end OCR-2.0 model for processing various types of optical characters. It includes Python code snippets demonstrating plain, batched, formatted, multi-page, and cropped-patch inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/levit.md",
            "description": "This file provides documentation for the LeViT (LeViT: Introducing Convolutions to Vision Transformers) model within the Hugging Face Transformers library, detailing its architecture, usage, and available classes for image classification tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/ijepa.md",
            "description": "This file provides documentation for the I-JEPA (Image Joint Embedding Predictive Architecture) model within the Hugging Face Transformers library. It explains the model's self-supervised learning approach for image representations and offers code examples for feature extraction, image similarity, and model quantization.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/lighton_ocr.md",
            "description": "This file documents the LightOnOcr model within the Hugging Face Transformers library, providing an overview, usage examples, and API references for its components.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/mlcd.md",
            "description": "This file is a documentation page for the MLCD vision model within the Hugging Face Transformers library, providing an overview, usage examples, and configuration details.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/mask2former.md",
            "description": "This file provides comprehensive documentation for the Mask2Former model within the Hugging Face Transformers library. It includes an overview, usage tips, and API reference for its configuration, model classes, and image processor.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mobilenet_v2.md",
            "description": "This file provides documentation for the MobileNet V2 model within the Hugging Face Transformers library. It explains the model's architecture, offers usage examples for image classification, and details its associated classes and configurations.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mobilevitv2.md",
            "description": "This file provides documentation for the MobileViTV2 model within the Hugging Face Transformers library, detailing its architecture, history, usage tips, and API references for its configuration and various task-specific models.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mobilenet_v1.md",
            "description": "This document provides a detailed overview and usage guide for the MobileNet V1 model within the Hugging Face Transformers library. It includes its architecture, example code for image classification, and specifics on its implementation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mobilevit.md",
            "description": "This file provides comprehensive documentation for the MobileViT model within the Hugging Face Transformers library. It describes the model's architecture, includes usage examples for image classification, and offers API references for its various components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/omdet-turbo.md",
            "description": "This file provides documentation for the OmDet-Turbo model, detailing its overview, unique features, and usage examples for open-vocabulary object detection. It also includes API references for its configuration, processor, and object detection model.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/oneformer.md",
            "description": "This document provides comprehensive documentation for the OneFormer model within the Hugging Face Transformers library. It details the model's overview, architecture, usage, and API for universal image segmentation tasks.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/maskformer.md",
            "description": "This file provides the official documentation for the MaskFormer model within the Hugging Face Transformers library, detailing its overview, usage tips, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/mgp-str.md",
            "description": "This document provides an overview and usage guide for the MGP-STR (Multi-Granularity Prediction for Scene Text Recognition) model within the Hugging Face Transformers library, including its architecture, training, inference, and API documentation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/owlvit.md",
            "description": "This file is a documentation page for the OWL-ViT model within the Hugging Face Transformers library. It provides an overview of the model, usage examples for object detection, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/poolformer.md",
            "description": "This file provides documentation for the PoolFormer model within the Hugging Face Transformers library. It includes an overview of the model, usage tips, available variants, and links to related resources and code examples.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pvt.md",
            "description": "This file provides documentation for the Pyramid Vision Transformer (PVT) model, detailing its overview, architecture, and available configurations within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/pixio.md",
            "description": "This file provides documentation for the Pixio vision foundation model within the Hugging Face Transformers library, detailing its architecture, capabilities, and offering code examples for its usage.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/resnet.md",
            "description": "This file provides documentation for the ResNet model within the Hugging Face Transformers library, detailing its overview, architecture, historical context, and API references for its implementation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/pvt_v2.md",
            "description": "This file provides comprehensive documentation for the Pyramid Vision Transformer V2 (PVTv2) model, including its architecture, key features, usage examples for various tasks like image classification, and details on its configuration and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/regnet.md",
            "description": "This file provides documentation for the RegNet model within the Hugging Face Transformers library. It describes the model's overview, origin, and provides resources and API references for its implementation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/sam.md",
            "description": "This file provides documentation for the SAM (Segment Anything Model) within the Hugging Face Transformers library, detailing its overview, usage examples, related resources, and information on derived models like SlimSAM and Grounded SAM.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/sam2_video.md",
            "description": "This file provides documentation and usage examples for the SAM2 Video model within the Hugging Face Transformers library, detailing its capabilities for video segmentation and object tracking, including basic, multi-object, and streaming inference scenarios.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/rt_detr.md",
            "description": "This file provides the documentation for the RT-DETR (Real-Time DEtection Transformer) model within the Hugging Face Transformers library, including its overview, usage examples, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/seggpt.md",
            "description": "This file provides documentation for the SegGPT model within the Hugging Face Transformers library. It explains the model's overview, capabilities, usage tips, and includes code examples for one-shot semantic segmentation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/sam2.md",
            "description": "This document provides an overview and usage examples for the SAM2 (Segment Anything Model 2) within the Hugging Face Transformers library, covering its capabilities for image and video segmentation with various input types like points and bounding boxes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/rt_detr_v2.md",
            "description": "This file provides documentation for the RT-DETRv2 model within the Hugging Face Transformers library, including its overview, usage tips with code examples, and references to its configuration and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/swin2sr.md",
            "description": "This file is a documentation page for the Swin2SR model within the Hugging Face Transformers library. It provides an overview of the model, its architecture, resources, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/superglue.md",
            "description": "This file provides documentation for the SuperGlue model within the Hugging Face Transformers library. It explains how to use SuperGlue for keypoint matching between images, including code examples for both pipeline and AutoModel usage, and details its associated configuration, processor, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/sam3_tracker.md",
            "description": "This documentation page describes the SAM3 Tracker model in Hugging Face Transformers, covering its capabilities for promptable visual segmentation and providing detailed usage examples for various segmentation tasks.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/shieldgemma2.md",
            "description": "This documentation file introduces the ShieldGemma 2 model, detailing its purpose as an image safety classifier for identifying harmful content across various categories. It includes usage examples for classifying images with built-in or custom policies within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/sam3_video.md",
            "description": "This file provides documentation for the SAM3 Video model within the Hugging Face Transformers library, detailing its features for promptable concept segmentation and object tracking in videos, along with usage examples for pre-loaded and streaming inference.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/pe_video.md",
            "description": "This file provides documentation for the PE Video (Perception Encoder Video) model, including an overview, usage examples, and details on its components like processors, configurations, and models, within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/sam_hq.md",
            "description": "This documentation file provides an overview, features, usage examples, and API references for the SAM-HQ (High-Quality Segment Anything Model) within the Hugging Face Transformers library. It explains how SAM-HQ improves upon the original SAM model for high-quality segmentation.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/superpoint.md",
            "description": "This file provides documentation for the SuperPoint model in the Hugging Face Transformers library. It explains its capabilities for keypoint detection, provides usage examples, and details its configuration, image processor, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/upernet.md",
            "description": "This file provides documentation for the UPerNet model within the Hugging Face Transformers library, including an overview, usage examples, and related resources. It details how UPerNet can be used for semantic segmentation with various vision backbones.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/timm_wrapper.md",
            "description": "This documentation file describes the `TimmWrapper` helper class within the Hugging Face Transformers library, which enables the integration and use of `timm` models for tasks like image classification. It includes an overview, code examples, and references to related classes and resources.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/videomae.md",
            "description": "This file provides documentation for the VideoMAE model within the Hugging Face Transformers library, including its overview, usage instructions (e.g., with SDPA), related resources, and API documentation for its various components like configuration, processors, and specific model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/segformer.md",
            "description": "This file provides the documentation for the SegFormer model in the Hugging Face Transformers library, detailing its architecture, usage examples for semantic segmentation, key considerations, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/swin.md",
            "description": "This file provides documentation for the Swin Transformer model, detailing its architecture, capabilities, and usage examples for image classification within the Hugging Face Transformers library. It also includes auto-generated documentation for various Swin-related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/swinv2.md",
            "description": "This file provides documentation for the Swin Transformer V2 model within the Hugging Face Transformers library, detailing its features, usage examples, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/timesformer.md",
            "description": "This file provides documentation for the TimeSformer model within the Hugging Face Transformers library. It includes an overview, usage tips, and references to its configuration and model classes for video classification.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/trocr.md",
            "description": "This file provides documentation for the TrOCR model in the Hugging Face Transformers library, explaining its capabilities for optical character recognition (OCR) and offering usage examples, including quantization.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/vitpose.md",
            "description": "This file provides comprehensive documentation for the ViTPose model within the Hugging Face Transformers library. It details its architecture, usage for pose estimation, and includes examples for both standard and quantized inference.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/textnet.md",
            "description": "This file provides documentation for the TextNet model within the Hugging Face Transformers library. It includes an overview of the model for text detection, usage tips, and API references for its configuration, image processors, and various model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vitmatte.md",
            "description": "This document provides an overview and detailed explanation of the ViTMatte model within the Hugging Face Transformers library, covering its architecture, purpose, and usage with associated classes like `VitMatteConfig` and `VitMatteImageProcessor`.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vit.md",
            "description": "This file provides documentation for the Vision Transformer (ViT) model, including its architecture, usage examples for image classification, and API references for its various components within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vivit.md",
            "description": "This file provides documentation for the Video Vision Transformer (ViViT) model within the Hugging Face Transformers library, including its overview, research paper, implementation details, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vjepa2.md",
            "description": "This document provides an overview and usage examples for the V-JEPA 2 self-supervised video encoder model within the Hugging Face Transformers library. It details its capabilities for feature extraction and video classification, alongside auto-generated API documentation.",
            "spof": true
          },
          {
            "path": "docs/source/en/model_doc/table-transformer.md",
            "description": "This file provides documentation for the Table Transformer model within the Hugging Face Transformers library, detailing its overview, capabilities (table detection and structure recognition), and relevant resources.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vitdet.md",
            "description": "This file provides documentation for the ViTDet model within the Hugging Face Transformers library, detailing its overview, research paper, capabilities in object detection using Vision Transformers, and related configuration and model classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/yolos.md",
            "description": "This file provides documentation for the YOLOS (You Only Look at One Scale) object detection model within the Hugging Face Transformers library. It includes an overview of the model, code examples for its usage with `pipeline` and `AutoModel`, and references to its configurations and processors.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/zoedepth.md",
            "description": "This file provides documentation for the ZoeDepth model in the Hugging Face Transformers library, explaining its functionality for depth estimation and demonstrating its usage through code examples for both pipeline and AutoModel interfaces.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vit_mae.md",
            "description": "This file documents the ViTMAE (Vision Transformer Masked Autoencoder) model within the Hugging Face Transformers library, explaining its self-supervised pretraining, usage for vision tasks, and providing code examples for pretraining and fine-tuning. It also includes API references for ViTMAE-related classes.",
            "spof": false
          },
          {
            "path": "docs/source/en/model_doc/vit_msn.md",
            "description": "This file provides documentation for the ViTMSN (Vision Transformer Masked Siamese Networks) model within the Hugging Face Transformers library. It details the model's overview, usage tips, performance with Scaled Dot Product Attention (SDPA), and API references for its configuration and classes.",
            "spof": false
          },
          {
            "path": "docs/source/es/tasks/image_classification.md",
            "description": "This documentation page, written in Spanish, provides a tutorial on image classification. It guides users through fine-tuning a ViT model on the Food-101 dataset, covering data loading, preprocessing, and training steps.",
            "spof": true
          },
          {
            "path": "docs/source/zh/main_classes/image_processor.md",
            "description": "This document provides a Chinese explanation of the `Image Processor` concept in the Hugging Face Transformers library, detailing its role in preparing image inputs for visual models and post-processing their outputs. It also includes documentation references for core image processing classes like `ImageProcessingMixin`, `BatchFeature`, and `BaseImageProcessor`.",
            "spof": false
          },
          {
            "path": "docs/source/ja/main_classes/image_processor.md",
            "description": "This file is a Japanese-language documentation page for the Hugging Face Transformers library, explaining the `ImageProcessor` class and its role in preparing image inputs and post-processing outputs for vision models. It also covers related classes like `ImageProcessingMixin`, `BatchFeature`, and `BaseImageProcessor`.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/image_classification.md",
            "description": "This file is a Japanese language guide that explains how to fine-tune a ViT model for image classification on the Food-101 dataset and then use the fine-tuned model for inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/monocular_depth_estimation.md",
            "description": "This document provides a Japanese guide on monocular depth estimation using the Hugging Face Transformers library, demonstrating both pipeline usage and manual inference for the task.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/semantic_segmentation.md",
            "description": "This document is a Japanese guide on performing semantic segmentation using the Hugging Face Transformers library, detailing the process of fine-tuning a SegFormer model and performing inference.",
            "spof": true
          },
          {
            "path": "docs/source/ja/tasks/zero_shot_image_classification.md",
            "description": "This document provides a Japanese-language guide on performing zero-shot image classification using the Hugging Face Transformers library. It demonstrates both pipeline usage and manual inference for this task.",
            "spof": true
          },
          {
            "path": "docs/source/ja/model_doc/bit.md",
            "description": "This file is a Japanese documentation page for the Big Transfer (BiT) model in the Hugging Face Transformers library. It provides an overview of the BiT model, usage tips, and references to its API components.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/cvt.md",
            "description": "This documentation file in Japanese describes the Convolutional Vision Transformer (CvT) model, including its overview, key features, usage tips, and references to its API classes and related resources within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/convnext.md",
            "description": "This document provides Japanese-language documentation for the ConvNeXT model within the Hugging Face Transformers library, including an overview, architecture details, usage resources, and API references for its components.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/convnextv2.md",
            "description": "This document provides a Japanese-language overview and documentation for the ConvNeXt V2 model within the Hugging Face Transformers library, detailing its architecture, origins, and usage for tasks like image classification.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/detr.md",
            "description": "This document provides an overview and detailed explanation of the DETR model for object detection and segmentation, including its architecture, training, usage tips, and resources, written in Japanese.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/deformable_detr.md",
            "description": "This file provides Japanese documentation for the Deformable DETR model, including an overview, usage tips, resources, and API references within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "docs/source/ja/model_doc/dinat.md",
            "description": "This file provides Japanese documentation for the DiNAT (Dilated Neighborhood Attention Transformer) model, explaining its architecture, usage, and API within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "docs/source/ko/image_processors.md",
            "description": "This document explains image processors in the Transformers library, detailing their role in converting images into suitable pixel value tensors for vision models. It covers processor classes, usage, preprocessing, and optimization techniques like fast processors and padding.",
            "spof": true
          },
          {
            "path": "docs/source/ko/internal/image_processing_utils.md",
            "description": "This documentation page, written in Korean, lists utility functions for image processors within the Hugging Face Transformers library. It primarily covers function-based image transformations and the `ImageProcessingMixin`.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/swinv2.md",
            "description": "This file provides Korean documentation for the Swin Transformer V2 model within the Hugging Face Transformers library. It includes an overview, resources for image classification and masked image modeling, and API documentation for `Swinv2Config`, `Swinv2Model`, `Swinv2ForMaskedImageModeling`, and `Swinv2ForImageClassification`.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/swin.md",
            "description": "This document provides a Korean-language overview and documentation for the Swin Transformer model within the Hugging Face Transformers library, including its architecture, usage tips, and API references.",
            "spof": false
          },
          {
            "path": "docs/source/ko/model_doc/swin2sr.md",
            "description": "This document provides the Korean-language documentation for the Swin2SR model within the Hugging Face Transformers library. It explains the model's architecture, improvements for image super-resolution and restoration, and includes API references for its associated classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/vivit.md",
            "description": "This file is the Korean-language documentation for the ViViT (Video Vision Transformer) model in the Hugging Face Transformers library. It provides an overview of the model, its paper, and references to its configuration, image processor, and model classes.",
            "spof": true
          },
          {
            "path": "docs/source/ko/model_doc/sam_hq.md",
            "description": "This document provides an overview and detailed explanation of the SAM-HQ (High-Quality Segment Anything Model) within the Hugging Face Transformers library. It describes the model's features, improvements over SAM, usage examples, and API references.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/keypoint_detection.md",
            "description": "This document provides a Korean-language guide on performing keypoint detection using the Hugging Face Transformers library, specifically demonstrating how to use the SuperPoint model to extract keypoints from images.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/zero_shot_image_classification.md",
            "description": "This document provides a Korean-language guide to zero-shot image classification using the Hugging Face Transformers library. It explains the concept and demonstrates how to perform zero-shot image classification using both the pipeline API and by manually instantiating models and processors.",
            "spof": false
          },
          {
            "path": "docs/source/ko/tasks/image_classification.md",
            "description": "This document is a Korean-language guide on performing image classification using the Hugging Face Transformers library. It details the process of fine-tuning a ViT model on the Food-101 dataset and then using it for inference.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/semantic_segmentation.md",
            "description": "This document is a Korean-language guide explaining semantic segmentation. It provides instructions on how to fine-tune a SegFormer model for semantic segmentation using the SceneParse150 dataset and perform inference with the trained model.",
            "spof": true
          },
          {
            "path": "docs/source/ko/tasks/video_classification.md",
            "description": "This document is a Korean-language guide on how to perform video classification using the Hugging Face Transformers library. It demonstrates fine-tuning a VideoMAE model on a subset of the UCF101 dataset and using the fine-tuned model for inference.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Yuanyuan Chen",
            "percent": 14
          },
          {
            "name": "Yoni Gozlan",
            "percent": 14
          },
          {
            "name": "Joao Gante",
            "percent": 7
          }
        ]
      },
      "High-Level Vision Task Pipelines": {
        "files": [
          {
            "path": "tests/pipelines/test_pipelines_depth_estimation.py",
            "description": "This file contains unit tests for the DepthEstimationPipeline in the Hugging Face Transformers library. It verifies the pipeline's functionality across various models and input types, including single and batch inferences, and checks output formats.",
            "spof": false
          },
          {
            "path": "tests/pipelines/test_pipelines_image_segmentation.py",
            "description": "This file contains unit tests for the image segmentation pipelines in the Hugging Face Transformers library, covering various models, subtasks, and input formats.",
            "spof": false
          },
          {
            "path": "src/transformers/pipelines/depth_estimation.py",
            "description": "This file defines the `DepthEstimationPipeline` for Hugging Face Transformers, enabling users to perform depth estimation on images using pre-trained models. It handles image input, preprocessing, model inference, and postprocessing to output predicted depth maps.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Joao Gante",
            "percent": 16
          },
          {
            "name": "Yih-Dar",
            "percent": 14
          },
          {
            "name": "Matt",
            "percent": 14
          }
        ]
      },
      "Vision Model Architectures": {
        "files": [
          {
            "path": "tests/models/beit/test_modeling_beit.py",
            "description": "This file contains unit tests for the BEiT (Bidirectional Encoder representations from Image Transformers) model implementations in the Hugging Face Transformers library, covering various model configurations and tasks like image classification and masked image modeling.",
            "spof": false
          },
          {
            "path": "tests/models/bit/test_modeling_bit.py",
            "description": "This file contains unit and integration tests for the PyTorch Bit model within the Hugging Face Transformers library, covering its various components like the model itself, image classification head, and backbone.",
            "spof": false
          },
          {
            "path": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "description": "This file contains unit tests for the Conditional DETR model, object detection, and segmentation implementations within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/convnextv2/test_modeling_convnextv2.py",
            "description": "This file contains unit tests for the PyTorch ConvNextV2 model within the Hugging Face Transformers library, covering its configuration, core functionalities, and integration with common testing utilities.",
            "spof": false
          },
          {
            "path": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "description": "This file contains unit tests for the PyTorch DAB-DETR model and its object detection head within the HuggingFace Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch DepthPro model within the Hugging Face Transformers library. It includes tests for the model's configuration, core functionality, and specific applications like depth estimation and field-of-view prediction.",
            "spof": false
          },
          {
            "path": "tests/models/convnext/test_modeling_convnext.py",
            "description": "This file contains the unit and integration tests for the PyTorch ConvNext model implementations within the Hugging Face Transformers library. It covers testing various ConvNext model configurations, including the base model, image classification head, and backbone, as well as their functionalities and compatibility.",
            "spof": false
          },
          {
            "path": "tests/models/dac/test_modeling_dac.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch Dac model within the Hugging Face Transformers library. It includes tests for model configuration, forward passes, determinism, and output equivalence.",
            "spof": false
          },
          {
            "path": "tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py",
            "description": "This file contains unit tests and an integration test for the PyTorch DINOv3 ConvNext model within the Hugging Face Transformers library, ensuring its correct functionality and API compliance.",
            "spof": true
          },
          {
            "path": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "description": "This file contains unit tests for the PyTorch Data2VecVision model and its various heads (e.g., for image classification and semantic segmentation) within the Hugging Face Transformers library. It extends common testing utilities to ensure correct functionality and behavior of the model.",
            "spof": false
          },
          {
            "path": "tests/models/dinov3_vit/test_modeling_dinov3_vit.py",
            "description": "This file contains unit tests for the DINOv3ViT model and its backbone within the Hugging Face Transformers library. It verifies the model's configuration, core functionality, and integration with common testing utilities.",
            "spof": false
          },
          {
            "path": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "description": "This file contains the testing suite for the PyTorch Deformable DETR model, including its core components and object detection head, within the Hugging Face Transformers library. It defines various test configurations and asserts correct model behavior and output shapes.",
            "spof": false
          },
          {
            "path": "tests/models/deit/test_modeling_deit.py",
            "description": "This file contains unit tests for the PyTorch implementation of the DeiT (Data-efficient Image Transformers) model in the Hugging Face Transformers library. It thoroughly tests various DeiT model configurations and functionalities, including basic model forward passes, masked image modeling, and image classification.",
            "spof": false
          },
          {
            "path": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "description": "This file contains unit tests and integration tests for the Hugging Face Transformers' PyTorch Depth Anything model. It verifies the model's configuration, depth estimation capabilities, and loading from pretrained weights.",
            "spof": false
          },
          {
            "path": "tests/models/dinov2/test_modeling_dinov2.py",
            "description": "This file contains unit tests for the PyTorch Dinov2 model, including its core model, backbone, and image classification capabilities within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "description": "This file contains the PyTorch testing suite for the Dinov2WithRegisters model, including tests for its configuration, base model, backbone, and image classification capabilities.",
            "spof": false
          },
          {
            "path": "tests/models/detr/test_modeling_detr.py",
            "description": "This file contains unit tests for the DETR (DEtection TRansformer) model in the Hugging Face Transformers library. It includes tests for the base model, object detection, and segmentation variants, ensuring their configurations and functionalities work as expected.",
            "spof": false
          },
          {
            "path": "tests/models/cvt/test_modeling_cvt.py",
            "description": "This file contains the PyTorch testing suite for the Convolutional Vision Transformer (CvT) model within the Hugging Face Transformers library. It includes unit tests for the model's configuration, core functionality, image classification, and integration tests for pretrained models.",
            "spof": false
          },
          {
            "path": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "description": "This file contains unit and integration tests for the DPT (Depth Prediction Transformer) model within the Hugging Face Transformers library. It specifically tests DPT's depth estimation capabilities, including configurations with different auto-backbones like Dinov2 and BEiT.",
            "spof": false
          },
          {
            "path": "tests/models/dpt/test_modeling_dpt.py",
            "description": "This file contains the unit tests for the PyTorch DPT (Dense Prediction Transformer) model implementation within the Hugging Face Transformers library. It verifies the model's configuration, forward pass, and specific tasks like depth estimation and semantic segmentation.",
            "spof": false
          },
          {
            "path": "tests/models/dpt/test_modeling_dpt_hybrid.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch DPT (Vision Transformer for Depth Prediction) model, specifically for its hybrid configuration. It includes tests for the base model, depth estimation, and semantic segmentation functionalities.",
            "spof": false
          },
          {
            "path": "tests/models/dinat/test_modeling_dinat.py",
            "description": "This file contains a comprehensive testing suite for the PyTorch implementation of the Dinat model within the Hugging Face Transformers library. It includes tests for the model's configuration, various model functionalities like image classification and backbone operations, and general model behaviors.",
            "spof": false
          },
          {
            "path": "tests/models/efficientloftr/test_modeling_efficientloftr.py",
            "description": "This file contains unit tests for the EfficientLoFTR model and its configuration within the Hugging Face Transformers library. It verifies the model's functionality, output shapes, and adherence to common model interface requirements.",
            "spof": true
          },
          {
            "path": "tests/models/eomt/test_modeling_eomt.py",
            "description": "This file contains the testing suite for the PyTorch EoMT (Encoder-only Masked Transformer) model, particularly focusing on its universal segmentation capabilities. It includes unit tests for the model's configuration, training, and integration tests for inference and post-processing.",
            "spof": false
          },
          {
            "path": "tests/models/dit/test_modeling_dit.py",
            "description": "This file contains integration tests for the DiT (Diffusion Transformer) model in the Hugging Face Transformers library. It specifically verifies the model's functionality for image classification tasks by loading a pre-trained model and asserting its output against expected values.",
            "spof": false
          },
          {
            "path": "tests/models/efficientnet/test_modeling_efficientnet.py",
            "description": "This file contains unit and integration tests for the Hugging Face Transformers EfficientNet model, covering its configuration, core model functionality, and image classification capabilities. It uses `ModelTesterMixin` and `PipelineTesterMixin` for comprehensive testing.",
            "spof": false
          },
          {
            "path": "tests/models/edgetam_video/test_modeling_edgetam_video.py",
            "description": "This file contains integration tests for the PyTorch EdgeTamVideoModel within the Hugging Face Transformers library, specifically verifying its video mask generation and propagation capabilities.",
            "spof": true
          },
          {
            "path": "tests/models/eomt_dinov3/test_modeling_eomt_dinov3.py",
            "description": "This file contains the testing suite for the PyTorch EoMT DiNoV3 model, specifically the `EomtDinov3ForUniversalSegmentation` class within the Transformers library. It includes unit tests for configuration, model functionalities, training, initialization, and an integration test for inference.",
            "spof": true
          },
          {
            "path": "tests/models/focalnet/test_modeling_focalnet.py",
            "description": "This file contains the testing suite for the PyTorch FocalNet model within the Hugging Face Transformers library. It includes unit tests for the model's configuration, various architectures like image classification and masked image modeling, and its integration with pipelines.",
            "spof": false
          },
          {
            "path": "src/transformers/backbone_utils.py",
            "description": "This file contains utility classes and mixins for configuring and initializing backbone models, handling their output features and indices. It supports both `timm` and `transformers` type backbones.",
            "spof": true
          },
          {
            "path": "src/transformers/loss/loss_lw_detr.py",
            "description": "This file implements the Hungarian matcher and loss functions specifically adapted for the Light-Weight DETR (LwDETR) object detection model, including label, box, GIOU, mask, and cardinality losses.",
            "spof": true
          },
          {
            "path": "src/transformers/loss/loss_d_fine.py",
            "description": "This file implements the D-FINE loss function for object detection, including components for distribution-based bounding box regression, Fine-Grained Localization (FGL) Loss, and Decoupled Distillation Focal (DDF) Loss.",
            "spof": true
          },
          {
            "path": "src/transformers/loss/loss_rt_detr.py",
            "description": "This file implements the `RTDetrHungarianMatcher` for assigning ground truth to predictions, and the `RTDetrLoss` class, which computes various loss functions (VFL, bounding box, and GIOU losses) for the RT-DETR object detection model.",
            "spof": true
          },
          {
            "path": "src/transformers/loss/loss_deformable_detr.py",
            "description": "This file implements the loss functions and Hungarian matching algorithm specific to the Deformable DETR model, supporting both object detection and segmentation tasks. It computes classification, bounding box, and GIOU losses, incorporating focal loss for classification.",
            "spof": true
          },
          {
            "path": "src/transformers/models/beit/convert_beit_unilm_to_pytorch.py",
            "description": "This script converts BEiT model checkpoints from the original unilm repository to the Hugging Face Transformers format. It supports various BEiT configurations, including masked image modeling, image classification, and semantic segmentation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/beit/__init__.py",
            "description": "This `__init__.py` file defines the `beit` model package structure, enabling lazy loading of its submodules like configuration, feature extraction, image processing, and modeling components within the `transformers` library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/beit/configuration_beit.py",
            "description": "This file defines the `BeitConfig` class, which is used to store and manage the configuration parameters for the BEiT model architecture within the Hugging Face Transformers library. It specifies various architectural details and hyperparameters for instantiating a BEiT model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/beit/modeling_beit.py",
            "description": "This file implements the PyTorch architecture for the Bidirectional Encoder representation from Image Transformers (BEiT) model. It defines core components such as embeddings, attention mechanisms, and utilities for the BEiT model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bit/configuration_bit.py",
            "description": "This file defines the `BitConfig` class, which is used to store and manage the configuration parameters for the BiT (Big Transfer) model architecture in the Hugging Face Transformers library. It specifies various architectural details such as channel counts, hidden sizes, depths, and activation functions.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bit/__init__.py",
            "description": "This file serves as the `__init__.py` for the `bit` model within the `transformers` library, using lazy loading to import its components (configuration, image processing, and modeling classes) only when needed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/bit/modeling_bit.py",
            "description": "This file implements the PyTorch modules and architecture for the BiT (Big Transfer) model. It includes custom convolutional layers, pooling, normalization, and embedding components, also supporting its use as a backbone for hybrid Vision Transformers.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bit/convert_bit_to_pytorch.py",
            "description": "This script converts Big Transfer (BiT) model checkpoints from the timm library to the Hugging Face Transformers format. It includes renaming keys, transferring weights, and verifying outputs, with options to save locally or push to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/bros/__init__.py",
            "description": "This `__init__.py` file defines the `bros` model package within the Hugging Face Transformers library. It uses lazy loading to import the model's configuration, modeling, and processing components only when they are first accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "description": "This file defines the `ConditionalDetrConfig` class, which is used to store and manage the configuration parameters for instantiating a Conditional DETR model, specifying its architecture and hyperparameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/conditional_detr/__init__.py",
            "description": "This `__init__.py` file defines the import structure and enables lazy loading for the Conditional DETR model components within the Hugging Face Transformers library, including its configuration, feature extraction, image processing, and modeling classes.",
            "spof": true
          },
          {
            "path": "src/transformers/models/conditional_detr/modular_conditional_detr.py",
            "description": "This file implements modular components for the Conditional DETR model within the Hugging Face Transformers library, including its specific image processor and extended output structures for object detection and semantic segmentation.",
            "spof": true
          },
          {
            "path": "src/transformers/models/conditional_detr/convert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts original PyTorch checkpoints of the Conditional DETR model into a format compatible with the Hugging Face Transformers library. It renames various keys in the checkpoint state dictionary to match the Transformers model's expected structure.",
            "spof": true
          },
          {
            "path": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "description": "This file defines the output data structures for the Conditional DETR model, including specific output classes for object detection and segmentation tasks within the HuggingFace Transformers library. It is an auto-generated file from a modular source.",
            "spof": true
          },
          {
            "path": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "description": "This file implements the PyTorch ConvNextV2 model architecture, including its core components like layers, embeddings, and stages, for use within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convnextv2/configuration_convnextv2.py",
            "description": "This file defines the `ConvNextV2Config` class, which is used to store and manage the configuration parameters for the ConvNeXTV2 model architecture. It specifies various architectural details like the number of channels, patch size, hidden sizes, and depths for different stages of the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convnextv2/__init__.py",
            "description": "This `__init__.py` file defines the `convnextv2` model module structure for lazy loading. It sets up the import mechanism for its configuration and modeling components using `_LazyModule`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/convnextv2/convert_convnextv2_to_pytorch.py",
            "description": "This script converts pre-trained ConvNeXTV2 model checkpoints from their original format to the Hugging Face Transformers library format. It renames keys, adjusts configurations, and verifies outputs.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convnext/configuration_convnext.py",
            "description": "This file defines the `ConvNextConfig` class, which is used to store and manage the configuration parameters for instantiating a ConvNeXT model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convnext/modeling_convnext.py",
            "description": "This file implements the core ConvNeXT model architecture in PyTorch, including its various layers, embeddings, stages, and encoder, for the Hugging Face Transformers library. It provides the foundational classes for building and using ConvNeXT models.",
            "spof": false
          },
          {
            "path": "src/transformers/models/convnext/__init__.py",
            "description": "This `__init__.py` file sets up lazy loading for the ConvNeXT model within the Hugging Face Transformers library. It defines the module's public API, enabling on-demand import of its components like configuration, feature extraction, and modeling.",
            "spof": true
          },
          {
            "path": "src/transformers/models/convnext/convert_convnext_to_pytorch.py",
            "description": "This script converts original ConvNeXt model checkpoints from Facebook AI into the Hugging Face Transformers format, handling state dictionary key renaming, configuration, verification, and saving or pushing to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cvt/__init__.py",
            "description": "This `__init__.py` file sets up the `cvt` model package within the `transformers` library. It uses a lazy loading mechanism to efficiently import model configurations and implementations.",
            "spof": true
          },
          {
            "path": "src/transformers/models/cvt/modeling_cvt.py",
            "description": "This file implements the PyTorch model architecture for the Convolutional Vision Transformer (CvT), including its embeddings, self-attention mechanisms, and core building blocks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cvt/configuration_cvt.py",
            "description": "This file defines the `CvtConfig` class, which is used to store and manage the configuration parameters for the Convolutional Vision Transformer (CvT) model. It allows users to instantiate a CvT model with specified architectural settings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/cvt/convert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This file provides a script to convert original Convolutional Vision Transformer (CvT) PyTorch checkpoints into a format compatible with the Hugging Face Transformers library. It renames model weights to match the Hugging Face CvT model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "description": "This file defines the `DabDetrConfig` class, which is used to store and manage the configuration parameters for the DAB-DETR object detection model architecture in the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dab_detr/convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts original PyTorch DAB-DETR model checkpoints into a format compatible with the Hugging Face Transformers library, including renaming state dictionary keys and setting up the image processor and model configuration.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dab_detr/__init__.py",
            "description": "This `__init__.py` file sets up the `dab_detr` model package for lazy loading within the Transformers library, importing its configuration and modeling components on demand to optimize import times.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "description": "This file implements the PyTorch model architecture, output structures, and utility components for the DAB-DETR (DAB-DETR) object detection model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/d_fine/modular_d_fine.py",
            "description": "This file defines the `DFineConfig` class, which is the configuration for the D-FINE model, an object detection model, within the Transformers library. It specifies various architectural and training parameters for the D-FINE model's encoder, decoder, and other components.",
            "spof": false
          },
          {
            "path": "src/transformers/models/d_fine/configuration_d_fine.py",
            "description": "This file defines the `DFineConfig` class, which stores the configuration for a D-FINE model, specifying its architecture and various hyperparameters. It is automatically generated from `modular_d_fine.py`.",
            "spof": false
          },
          {
            "path": "src/transformers/models/d_fine/__init__.py",
            "description": "This file defines the D-Fine model package within the Transformers library, enabling lazy loading of its configuration and modeling components to improve import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/d_fine/modeling_d_fine.py",
            "description": "This file defines the core modeling components and architecture for the DFine model within the Hugging Face Transformers library. It includes definitions for layers, output structures, and attention mechanisms specific to the DFine model, generated from a modular source.",
            "spof": false
          },
          {
            "path": "src/transformers/models/d_fine/convert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "description": "This script converts original PyTorch checkpoints of the D-Fine object detection model to the Hugging Face Transformers format. It includes functions to configure the model and map state dictionary keys between the original and Hugging Face architectures.",
            "spof": true
          },
          {
            "path": "src/transformers/models/data2vec/configuration_data2vec_vision.py",
            "description": "This file defines the `Data2VecVisionConfig` class, which is used to store and manage the configuration for a Data2VecVision model, specifying its architecture and various hyperparameters.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "description": "This file implements the PyTorch Data2VecVision model architecture, including its embeddings, patch embeddings, and self-attention mechanism. Many components are adapted or copied from the BEiT model implementation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/data2vec/convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts original PyTorch checkpoints of Data2VecVision and BEiT models into a format compatible with Hugging Face Transformers, handling key renaming and state dictionary adjustments for different model architectures and tasks like image classification or pretraining.",
            "spof": false
          },
          {
            "path": "src/transformers/models/depth_anything/configuration_depth_anything.py",
            "description": "This file defines the `DepthAnythingConfig` class, which is used to store and manage the configuration parameters for the DepthAnything model architecture. It specifies settings for the backbone, reassemble layers, neck, fusion, and depth estimation head.",
            "spof": false
          },
          {
            "path": "src/transformers/models/depth_anything/convert_depth_anything_to_hf.py",
            "description": "This file provides utilities to convert original Depth Anything model checkpoints to the Hugging Face Transformers format, including functions for configuration, key renaming, and state dictionary manipulation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "description": "This file implements the core PyTorch architectural components for the Depth Anything model, including reassembly, feature fusion, neck, and depth estimation head, designed for integration with the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/depth_anything/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `Depth Anything` model within the HuggingFace Transformers library. It uses lazy loading to import the model's configuration and modeling components, improving startup performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deformable_detr/configuration_deformable_detr.py",
            "description": "This file defines the `DeformableDetrConfig` class, which is used to store and manage the configuration parameters for instantiating a Deformable DETR model, specifying its architecture and hyperparameters.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deformable_detr/__init__.py",
            "description": "This file serves as the `__init__.py` for the `deformable_detr` model package within the Transformers library, defining its import structure and enabling lazy loading of its various components like configuration, feature extraction, and modeling.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "description": "This file defines the core modeling components and output structures for the Deformable DETR model within the Hugging Face Transformers library. It is automatically generated from a modular source file.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py",
            "description": "This script converts official Deformable DETR checkpoints into the Hugging Face Transformers format, handling model architecture variations and saving the converted model and image processor.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deformable_detr/modular_deformable_detr.py",
            "description": "This file implements modular components, data structures, and post-processing utilities specifically for the Deformable DETR model within the Hugging Face Transformers library. It includes an image processor for object detection and various model output classes.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deit/configuration_deit.py",
            "description": "This file defines the `DeiTConfig` class, which is used to store and manage the configuration parameters for the DeiT model architecture within the Hugging Face Transformers library. It allows for the instantiation of a DeiT model with specified architectural settings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deit/modeling_deit.py",
            "description": "This file defines the PyTorch model architecture for the DeiT (Data-efficient Image Transformer), including its embeddings, patch embeddings, and self-attention mechanism.",
            "spof": false
          },
          {
            "path": "src/transformers/models/deit/convert_deit_timm_to_pytorch.py",
            "description": "This script converts pre-trained DeiT (Data-efficient Image Transformers) models from the timm library to the Hugging Face Transformers format, adapting their weights and configurations for use with `DeiTForImageClassificationWithTeacher`.",
            "spof": true
          },
          {
            "path": "src/transformers/models/deit/__init__.py",
            "description": "This `__init__.py` file defines the `DeiT` model package structure, enabling lazy loading of its configuration, feature extraction, image processing, and modeling components to improve import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov2/configuration_dinov2.py",
            "description": "This file defines the `Dinov2Config` class, which is used to store and manage the configuration parameters for the DINOv2 model architecture within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov2/convert_dinov2_to_hf.py",
            "description": "This script converts DINOv2 model checkpoints from the original Facebook Research repository to the Hugging Face Transformers format, handling different DINOv2 architectures and verifying the conversion accuracy.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov2/__init__.py",
            "description": "This file serves as the package initializer for the `dinov2` model within the `transformers` library. It implements lazy loading for its submodules, `configuration_dinov2` and `modeling_dinov2`, to improve import performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov2/modeling_dinov2.py",
            "description": "This file implements the core PyTorch modules for the DINOv2 model within the Hugging Face Transformers library, including its patch and position embeddings, and self-attention mechanism.",
            "spof": false
          },
          {
            "path": "src/transformers/models/detr/convert_detr_to_pytorch.py",
            "description": "This script converts official DETR model checkpoints (using a native Transformers backbone) to the Hugging Face Transformers format, handling model configuration and renaming keys for compatibility.",
            "spof": false
          },
          {
            "path": "src/transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py",
            "description": "This script converts original PyTorch DETR (Detection Transformer) model checkpoints, including those with timm backbones, into the format expected by the Hugging Face Transformers library. It renames and restructures the model weights to match the Hugging Face implementation.",
            "spof": true
          },
          {
            "path": "src/transformers/models/detr/configuration_detr.py",
            "description": "This file defines the `DetrConfig` class, which is used to store the configuration and hyperparameters for the DETR (Detection Transformer) model. It allows instantiating a DETR model with specific architectural settings.",
            "spof": false
          },
          {
            "path": "src/transformers/models/detr/__init__.py",
            "description": "This `__init__.py` file defines the package structure for the DETR model within the Transformers library. It manages the lazy import of various DETR-related components like configuration, feature extraction, image processing, and modeling.",
            "spof": true
          },
          {
            "path": "src/transformers/models/detr/modeling_detr.py",
            "description": "This file defines the PyTorch model architecture, including various output classes and foundational modules, for the DETR (DEtection TRansformer) model within the Hugging Face Transformers library, specifically tailored for object detection and segmentation tasks.",
            "spof": true
          },
          {
            "path": "src/transformers/models/depth_pro/configuration_depth_pro.py",
            "description": "This file defines the `DepthProConfig` class, which is used to store and manage the configuration parameters for the DepthPro model architecture. It specifies various settings and sub-configurations for different components of the DepthPro model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/depth_pro/__init__.py",
            "description": "This `__init__.py` file defines the DepthPro model's public API and lazily imports its core components, including configuration, image processing, and modeling modules, for efficient loading within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/depth_pro/convert_depth_pro_weights_to_hf.py",
            "description": "This script converts the weights of an official DepthPro model from Apple to the Hugging Face Transformers format, mapping original layer names to Hugging Face compatible names and saving the model and image processor.",
            "spof": true
          },
          {
            "path": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "description": "This file implements the PyTorch DepthPro model, including utilities for splitting images into patches, merging features, and reconstructing feature maps for depth estimation tasks within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinat/__init__.py",
            "description": "This `__init__.py` file acts as the entry point for the `dinat` model within the `transformers` library, using lazy loading to defer imports of its configuration and modeling components until they are explicitly accessed.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinat/configuration_dinat.py",
            "description": "This file defines the `DinatConfig` class, which is used to store and manage the configuration parameters for the Dilated Neighborhood Attention Transformer (Dinat) model architecture within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinat/modeling_dinat.py",
            "description": "This file implements the core PyTorch architectural components for the Dilated Neighborhood Attention Transformer (DinaT) model, including its embeddings, downsampling layers, and the specialized Neighborhood Attention mechanism, for use within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov3_vit/configuration_dinov3_vit.py",
            "description": "This file defines the `DINOv3ViTConfig` class, which is used to configure the architecture and hyperparameters for the DINOv3 Vision Transformer (ViT) model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "description": "This file implements the core modeling components for the DINOv3 Vision Transformer, including patch, CLS, mask, and register token embeddings, as well as rotary position embeddings and attention mechanisms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov3_vit/modular_dinov3_vit.py",
            "description": "This file implements the PyTorch DINOv3 Vision Transformer model's modular components, including embeddings, attention mechanisms, and Rotary Position Embeddings, with utility functions for patch coordinate generation and augmentation.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov3_vit/convert_dinov3_vit_to_hf.py",
            "description": "This script converts DINOv3 Vision Transformer checkpoints from their original format to the Hugging Face Transformers format and includes testing to verify the conversion.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov3_vit/__init__.py",
            "description": "This `__init__.py` file serves as the package initializer for the DINOv3 ViT model within the Transformers library, implementing lazy loading for its configuration, image processing, and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "description": "This file implements the PyTorch DINOv3 ConvNext model architecture, including its various layers, stages, and the full model, for use within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov3_convnext/__init__.py",
            "description": "This `__init__.py` file defines the DINOv3 ConvNext model's API within the `transformers` library, using lazy loading for its configuration and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov3_convnext/convert_dinov3_convnext_to_hf.py",
            "description": "This script converts pre-trained DINOv3 ConvNext model checkpoints from their original format to the Hugging Face Transformers format, including state dict key mapping, inference testing, and optional uploading to the Hugging Face Hub.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov3_convnext/configuration_dinov3_convnext.py",
            "description": "This file defines the `DINOv3ConvNextConfig` class, which is used to store and manage the configuration parameters for the DINOv3ConvNext model architecture within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "description": "This file defines the PyTorch model architecture for the DINOv2 vision transformer, including its patch embeddings, positional embeddings with registers, and self-attention mechanism.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov2_with_registers/convert_dinov2_with_registers_to_hf.py",
            "description": "This script converts DINOv2 with Registers model checkpoints from the original repository to the Hugging Face Transformers format. It handles renaming keys and transferring weights to make the model compatible with the Hugging Face ecosystem.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov2_with_registers/__init__.py",
            "description": "This `__init__.py` file defines the import structure and enables lazy loading for the `dinov2_with_registers` model within the Hugging Face Transformers library. It primarily facilitates the dynamic import of the model's configuration and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "description": "This file defines the configuration and core embedding components for a DINOv2 vision transformer model that includes register tokens, extending the base DINOv2 implementation within the Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dinov2_with_registers/configuration_dinov2_with_registers.py",
            "description": "This file defines the `Dinov2WithRegistersConfig` class, which specifies the architectural configuration and parameters for the DINOv2 with Registers model. It serves to configure and instantiate the model with various settings.",
            "spof": true
          },
          {
            "path": "src/transformers/models/edgetam/modeling_edgetam.py",
            "description": "This file implements the core modeling components for the EdgeTam (Edge Transformer Attention Module) model, including custom attention layers, layer normalization, and a two-way attention block. It defines the architecture for the EdgeTam vision encoder and mask decoder within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/edgetam/modular_edgetam.py",
            "description": "This file defines the configuration classes and model components for the EdgeTAM model, which is an extension of the SAM 2 model, providing modular vision, prompt encoder, and mask decoder configurations.",
            "spof": false
          },
          {
            "path": "src/transformers/models/edgetam/convert_edgetam_to_hf.py",
            "description": "This script converts Segment Anything Model (SAM) checkpoints from their original repository format to the Hugging Face EdgeTam model format. It handles key remapping, model configuration, and optional sanity checks.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dpt/__init__.py",
            "description": "This `__init__.py` file defines the DPT model package structure within the `transformers` library. It uses lazy loading to manage imports of DPT-related configurations, feature extractors, image processors, and models.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dpt/configuration_dpt.py",
            "description": "This file defines the DPTConfig class, which is used to store and manage the configuration parameters for instantiating a DPT (Dense Prediction Transformer) model within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/dpt/convert_dinov2_depth_to_hf.py",
            "description": "This script converts DINOv2 and DPT (Depth Prediction Transformer) model checkpoints from their original repository format to the Hugging Face Transformers format. It includes functions for configuring models, renaming keys, and processing attention weights for various DINOv2 sizes.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/convert_dpt_swinv2_to_hf.py",
            "description": "This script converts DPT-SwinV2 3.1 checkpoints from the MiDaS repository to the Hugging Face Transformers format, handling model configuration and weight mapping.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/convert_dpt_beit_to_hf.py",
            "description": "This script converts DPT-BEiT model checkpoints from the MiDaS repository into a format compatible with the Hugging Face Transformers library, adjusting configurations and renaming weights.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/convert_dpt_to_pytorch.py",
            "description": "This file provides utilities to convert DPT (Dense Prediction Transformer) model checkpoints from their original repository format into the Hugging Face Transformers library format. It handles model configuration, state dictionary key renaming, and verification for both depth estimation and semantic segmentation tasks.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/convert_dpt_hybrid_to_pytorch.py",
            "description": "This script converts DPT model checkpoints from their original repository format to the Hugging Face Transformers library's DPT model format, including renaming keys and adjusting configurations.",
            "spof": false
          },
          {
            "path": "src/transformers/models/dpt/modeling_dpt.py",
            "description": "This file implements the PyTorch DPT (Dense Prediction Transformers) model, including its embedding layers and custom output structures, for use within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/efficientloftr/__init__.py",
            "description": "This `__init__.py` file defines the package structure for the `efficientloftr` model within the Transformers library. It uses lazy loading to manage imports of its configuration, image processing, and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "description": "This file implements the core modeling components and output structures for the EfficientLoFTR model within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/efficientloftr/configuration_efficientloftr.py",
            "description": "This file defines the `EfficientLoFTRConfig` class, which is used to store and manage the configuration parameters for the EfficientLoFTR model architecture. It specifies various settings like stage blocks, feature dimensions, attention parameters, and matching thresholds for the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/efficientloftr/convert_efficientloftr_to_hf.py",
            "description": "This script converts a pre-trained EfficientLoFTR model from an original checkpoint format to the Hugging Face Transformers library format. It renames model weights, loads them into a Hugging Face model, and includes verification steps.",
            "spof": true
          },
          {
            "path": "src/transformers/models/edgetam_video/__init__.py",
            "description": "This `__init__.py` file defines the `edgetam_video` package within the transformers library. It uses lazy loading to import its configuration and modeling components only when they are accessed, improving startup performance.",
            "spof": true
          },
          {
            "path": "src/transformers/models/edgetam_video/configuration_edgetam_video.py",
            "description": "This file defines configuration classes for the EdgeTamVideo model and its sub-components, such as the prompt encoder and mask decoder, used to set up the model architecture and hyperparameters.",
            "spof": true
          },
          {
            "path": "src/transformers/models/edgetam_video/convert_edgetam_video_to_hf.py",
            "description": "This script converts EdgeTamVideo model checkpoints from their original repository format to a format compatible with Hugging Face Transformers, including adapting the model configuration and state dictionary keys.",
            "spof": true
          },
          {
            "path": "src/transformers/models/edgetam_video/modular_edgetam_video.py",
            "description": "This file defines the configuration classes for the EDGETAM video model, including its prompt encoder, mask decoder, and overall model architecture, specifying parameters for components like memory attention and encoders.",
            "spof": true
          },
          {
            "path": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "description": "This file defines the core modeling components, including layers, attention mechanisms, and output structures, for the EdgeTamVideo model within the HuggingFace Transformers library. It is an auto-generated file from a modular source.",
            "spof": true
          },
          {
            "path": "src/transformers/models/efficientnet/__init__.py",
            "description": "This `__init__.py` file defines the EfficientNet model package within the transformers library, managing its module structure and enabling lazy loading of its components like configuration, image processing, and modeling files.",
            "spof": true
          },
          {
            "path": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "description": "This file implements the PyTorch EfficientNet model architecture, defining various layers and blocks such as embeddings, depthwise convolutions, expansion, squeeze-excitation, and final block layers, used within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/efficientnet/configuration_efficientnet.py",
            "description": "This file defines the `EfficientNetConfig` class, which is used to store and manage the architectural configuration parameters for the EfficientNet model within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/efficientnet/convert_efficientnet_to_pytorch.py",
            "description": "This script converts pre-trained EfficientNet model checkpoints from the original Keras/TensorFlow repository to the Hugging Face Transformers PyTorch format. It includes functions for mapping model configurations, renaming keys, and transferring weights.",
            "spof": false
          },
          {
            "path": "src/transformers/models/eomt/convert_eomt_to_hf.py",
            "description": "This script converts EOMT (Efficient Open-Mouth Transformer) model weights, configuration, and image processor from their original format to the Hugging Face Transformers format, enabling their use within the Hugging Face ecosystem.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt/configuration_eomt.py",
            "description": "This file defines the `EomtConfig` class, which is used to store and manage the configuration parameters for the EoMT model, specifically for universal segmentation. It specifies various architectural details and hyperparameters for the model.",
            "spof": false
          },
          {
            "path": "src/transformers/models/eomt/__init__.py",
            "description": "This `__init__.py` file defines the `eomt` model package for the Hugging Face Transformers library, setting up lazy loading for its configuration, image processing, and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt/modular_eomt.py",
            "description": "This file defines the configuration and output structure for the modular EoMT (Encoder-only Mask Transformer) model, specifically tailored for universal segmentation tasks within the Hugging Face Transformers library.",
            "spof": false
          },
          {
            "path": "src/transformers/models/eomt/modeling_eomt.py",
            "description": "This file defines core components for the Eomt universal segmentation model within the Hugging Face Transformers library, including its output structure, utility functions for point sampling and loss calculation, and a Hungarian matcher for prediction-to-label assignment.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt_dinov3/__init__.py",
            "description": "This `__init__.py` file serves as the entry point for the `eomt_dinov3` model, configuring lazy imports for its modeling and configuration components within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt_dinov3/configuration_eomt_dinov3.py",
            "description": "This file defines the `EomtDinov3Config` class, which is used to store and manage the configuration and hyperparameters for the EoMT-DINOv3 model, defining its architecture for universal segmentation tasks. It was automatically generated from a modular source file.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt_dinov3/convert_eomt_dinov3_to_hf.py",
            "description": "This script converts official EoMT-DINOv3 model checkpoints into a format compatible with Hugging Face Transformers. It handles state dictionary renaming, merging backbone weights, and building the appropriate configuration.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt_dinov3/modeling_eomt_dinov3.py",
            "description": "This file defines the core architectural components (like attention, embeddings, and utility functions) for the EomtDinov3 vision transformer model within the Hugging Face Transformers library. It is an auto-generated version from a modular source file.",
            "spof": true
          },
          {
            "path": "src/transformers/models/eomt_dinov3/modular_eomt_dinov3.py",
            "description": "This file defines the configuration and core architectural components for an EoMT (Encoder-only Masked Transformer) model that integrates the DINOv3 vision transformer backbone. It is designed for tasks like universal segmentation within the Hugging Face Transformers library.",
            "spof": true
          },
          {
            "path": "src/transformers/models/focalnet/configuration_focalnet.py",
            "description": "This file defines the `FocalNetConfig` class, which is used to store and manage the configuration parameters for instantiating a FocalNet model architecture.",
            "spof": false
          },
          {
            "path": "src/transformers/models/focalnet/__init__.py",
            "description": "This file is the `__init__.py` for the FocalNet model within the `transformers` library, implementing lazy loading for its configuration and modeling components.",
            "spof": true
          },
          {
            "path": "src/transformers/models/focalnet/convert_focalnet_to_hf_format.py",
            "description": "This script converts pre-trained FocalNet model checkpoints from the original repository to the Hugging Face Transformers format, verifies the conversion, and optionally saves or pushes them to the Hugging Face Hub.",
            "spof": false
          },
          {
            "path": "src/transformers/models/focalnet/modeling_focalnet.py",
            "description": "This file implements the PyTorch model architecture components and output dataclasses for the FocalNet model within the Hugging Face Transformers library.",
            "spof": false
          }
        ],
        "contributors": [
          {
            "name": "Yoni Gozlan",
            "percent": 36
          },
          {
            "name": "NielsRogge",
            "percent": 10
          },
          {
            "name": "Cyril Vallez",
            "percent": 10
          }
        ]
      },
      "Structural Biology Modeling Utilities": {
        "files": [
          {
            "path": "src/transformers/models/esm/openfold_utils/rigid_utils.py",
            "description": "This file provides utility functions and a `Rotation` class for handling 3D rotations using both rotation matrices and quaternions. It includes operations like conversions, multiplications, and inversions, with a focus on numerical stability and batch processing.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/data_transforms.py",
            "description": "This file provides utility functions for processing protein atom data, specifically for converting between 14-atom and 37-atom representations and generating existence masks for atoms.",
            "spof": false
          },
          {
            "path": "src/transformers/models/esm/openfold_utils/residue_constants.py",
            "description": "This file defines various constants related to amino acid residues and protein structure, including chi angles, their masks, and atom positions relative to rigid groups, primarily used in protein folding models like AlphaFold.",
            "spof": true
          }
        ],
        "contributors": [
          {
            "name": "Matt",
            "percent": 60
          },
          {
            "name": "Yuanyuan Chen",
            "percent": 22
          },
          {
            "name": "Matthew Hoffman",
            "percent": 10
          }
        ]
      }
    },
    "stats": {
      "totalFiles": 356,
      "spofCount": 147
    },
    "busFactor": 10,
    "authorCount": 74
  }
}